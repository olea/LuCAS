\documentclass[a4paper,11pt,spanish]{article}
\usepackage[T1]{fontenc}
\usepackage{babel}
\usepackage[latin1]{inputenc}
\usepackage{a4wide}


\usepackage{color}


\begin{document}

%% Traducido por Jorge Nonius 12/6/02 %%
%% Revisado por Daniel Pérez Alcázar 9/9/02 %%




%% Esto crea el comando "FIXME", para resaltar lo que 
%% se quiere corregir o está mal.
%%		Ej.: \FIXME{Esto estará en rojo} 



\newcommand{\FIXME}[1]{\textcolor{red}{\textbf{\large{#1}}}}
  
  
\begin{flushright}

\Large{El límite de Linux}

\large{\emph{Linus Torvalds}}

\end{flushright}


  
    
\section*{}
    Linux tiene hoy millones de usuarios, miles de desarrolladores y un mercado creciente. Es utilizado 
	en sistemas empotrados, para controlar dispositivos robóticos, ha volado en una lanzadera espacial.
	Me gustaría decir que sabía que esto iba a ocurrir, que todo ello es parte del plan de dominación del mundo. 
	Pero francamente me ha cogido un poco por sorpresa. Fui mucho más consciente de la transición 
	desde un usuario Linux a cien que desde cien de ellos a un millón.

    Linux ha triunfado no porque el objetivo original fuera hacerlo altamente portable y disponible, sino porque
	se basó en buenos principios de diseño y en un buen modelo de desarrollo. Este robusto fundamento facilitó
	la portabilidad y la disponibilidad.

    Compárese por un momento Linux con proyectos que han tenido fuerte respaldo comercial, como Java o Windows NT.
	El entusiasmo por Java ha convencido a muchos de que ``escribirlo una vez, ejecutarlo en cualquier sitio'' es un
	objetivo que vale la pena. Nos dirigimos hacia una época en informática en que se usa una gama más y más amplia de \emph{hardware}, 
	así que ciertamente ése es un valor importante. Sin embargo, Sun no inventó el ``escribirlo una vez, ejecutarlo en 
	cualquier sitio''. La portabilidad ha sido desde siempre un santo grial de la industria informática. Microsoft, 
	por ejemplo, esperaba originalmente que Windows NT fuera un sistema operativo portable, que pudiera funcionar 
	sobre máquinas Intel, pero también sobre máquinas RISC comunes en el ámbito de las estaciones de trabajo. Linux 
	nunca tuvo un objetivo original tan ambicioso. Resulta irónico que Linux sea tan portable.

    Originalmente Linux fue proyectado para una única arquitectura: Intel 386. Hoy día Linux se ejecuta en cualquier
	cosa: desde Palm-Pilots hasta estaciones Alpha; es el sistema operativo para PCs más ampliamente portado. Si 
	se escribe un programa para ejecutarse sobre Linux, entonces, ese programa puede ser ``escrito una vez y
	ejecutado en cualquier sitio'' para una variedad muy grande de máquinas. Es interesante fijarse en las
	decisiones que llevaron hasta el diseño de Linux, y en cómo evolucionó el esfuerzo en su desarrollo, para 
	ver cómo Linux se las arregló para ser algo que en absoluto se contemplaba en la versión original. 

  

  
    
\section*{El \emph{port} a Amiga y Motorola}

    Linux es un sistema operativo como Unix,
	pero no es una versión de Unix. Esto da a Linux una 
	herencia distinta a la de, por ejemplo, FreeBSD. Lo que quiero decir es lo siguiente: los creadores de FreeBSD 
	partieron del código fuente del Unix de Berkeley y su \emph{kernel} desciende directamente de ese código fuente. 
	Por ello FreeBSD es una versión de Unix; forma parte del árbol genealógico de Unix. Linux, por contra, 
	buscaba proveer un interfaz que fuera compatible con Unix, pero el \emph{kernel} fue escrito desde cero, 
	sin línea alguna del código fuente de Unix. Así que Linux mismo no es un \emph{port} de Unix. Es un sistema 
	operativo nuevo.

    En realidad, al principio yo no tenía en mente portar este nuevo sistema operativo a otras plataformas. Yo 
	sólo quería algo que se ejecutara en mi 386.

    El esfuerzo serio para hacer portable el código del \emph{kernel} Linux comenzó con el portado a la máquina 
	Alpha de DEC. Pero este \emph{port} no fue sin embargo el primero.

    El primer \emph{port} vino de un equipo que llevó el \emph{kernel} Linux a la serie Motorola 68K, que era 
	el chip de los primeros ordenadores Sun, Apple y Amiga. Los programadores que se encontraban detrás
	del portado a Motorola querían en realidad hacer algo de bajo nivel, y ya había en Europa gente de 
	la comunidad Amiga que se encontraba desencantada con la perspectiva de usar DOS o Windows.

    Aunque la gente de Amiga obtuvo un sistema funcional para el 68K, a mí no me pareció que fuera 
	un \emph{port} de Linux con éxito. Ellos tenían el mismo tipo de enfoque que tuve yo con Linux en un 
	primer momento: escribir código desde cero dirigido al soporte de cierto tipo de interfaz. De este 
	modo, el primer \emph{port} a 68K podría considerarse como un sistema operativo tipo Linux, una bifurcación 
	del código base original.

    En cierto sentido, este primer Linux 68K no ayudó a la creación de un Linux portable, pero en otro 
	sentido sí lo hizo. Cuando empecé a pensar en el \emph{port} a Alpha tuve que pensar también en la experiencia 68K.
	Si adoptábamos el mismo enfoque con Alpha íbamos a encontrarnos con la necesidad de apoyar tres bases de código
	distintas para mantener Linux. Incluso si esto fuera factible en términos de código, 
	no lo era en términos de gestión. Yo no podría gestionar el desarrollo de Linux si significaba seguir la pista 
	de una base enteramente nueva de código cada vez que alguien quisiera Linux en una arquitectura nueva. En 
	su lugar, yo deseaba hacer un sistema para el que tuviera un árbol Alpha específico, un árbol 68K específico 
	y un árbol x86 específico, pero todo con una base de código común.

    Así que el \emph{kernel} experimentó en ese momento un reescritura muy importante. Pero esta reescritura fue 
	motivada por la necesidad de trabajar con una comunidad de desarrolladores creciente.

  

  
    
\section*{Microkernels}

    Cuando comencé a escribir el \emph{kernel} Linux existía una reconocida corriente de pensamiento acerca de cómo 
	escribir un sistema portable. El sentir común era que se tenía que utilizar una arquitectura estilo \emph{microkernel}.

    Con un \emph{kernel} monolítico como el de Linux, la memoria se divide entre espacio de usuario y espacio del \emph{kernel}.
	El espacio del \emph{kernel} es donde en realidad se carga el código del \emph{kernel}, y donde se asigna la memoria para las 
	operaciones del nivel del \emph{kernel}. Las operaciones del \emph{kernel} comprenden la planificación y control de procesos, 
	señales, dispositivos de E/S, paginación y \emph{swapping}: las operaciones básicas de las que  dependen los 
	demás programas. Dado que el código del \emph{kernel} incluye  interacción de bajo nivel con el \emph{hardware}, los \emph{kernel}s 
	monolíticos resultan ser específicos para cada arquitectura.

    Un \emph{microkernel} realiza un conjunto de operaciones mucho menor, y de forma más limitada: comunicación entre 
	procesos, control y  planificación de procesos limitado, y algo de E/S de bajo nivel. Los \emph{microkernels} 
	resultan ser menos específicos de un  determinado \emph{hardware} porque mucho código específico del sistema  
	es ubicado en el espacio de usuario. Una arquitectura \emph{microkernel} es básicamente un medio de 
	abstracción de los detalles del control de procesos, asignación de memoria y de recursos, de modo que el
	\emph{port} a otro \emph{chipset} requiera mínimos cambios.

    Así que en el momento en que comencé a trabajar en Linux en 1991, se asumía que la portabilidad provendría
	de un enfoque \emph{microkernel}. Los \emph{microkernels} eran entonces el tema de investigación preferido entre los informáticos. 
	Sin embargo, soy una persona pragmática, y en esa época yo veía que los \emph{microkernels} (a) eran experimentales,
	(b) eran obviamente más complicados que los \emph{kernel}s monolíticos, y (c) funcionaban notablemente más despacio
	que los \emph{kernel}s monolíticos. La velocidad importa mucho en un sistema operativo real, por ello una buena 
	cantidad de fondos destinados a investigación se gastaba por entonces en estudiar la optimización de los 
	\emph{microkernels}, para que pudieran correr tan rápido como un \emph{kernel} normal. Lo divertido es que si uno lee hoy 
	aquellos documentos, se da cuenta de que, mientras los investigadores aplicaban sus recetas optimizadoras
	a un \emph{microkernel}, de hecho las mismas recetas podrían aplicarse fácilmente de igual forma a los \emph{kernels}
	tradicionales para acelerar su ejecución.

    Esto me hizo pensar que el enfoque \emph{microkernel} era esencialmente un enfoque deshonesto en cuanto 
	estaba dirigido a obtener más fondos para investigación. Esto no significa que yo pensara en los investigadores
	como personas intencionadamente deshonestas. Quizá eran simplemente estúpidos. O unos ilusos. Dicho en sentido 
	literal. La deshonestidad era el resultado de la obsesión de la comunidad de investigadores en el asunto 
	``\emph{microkernel}''. En un laboratorio de investigación informática, o se estudiaban los
	\emph{microkernels} o no se estudiaban los \emph{kernels} en absoluto. Así que todos eran empujados hacia esa deshonestidad, 
	incluso la gente que diseñaba Windows NT. Aunque el equipo de NT supo que el resultado final no se parecería a un 
	\emph{microkernel}, tuvo que pagar tributo a aquellas ideas.

    Afortunadamente yo nunca sufrí mucha presión para centrarme en los \emph{microkernels}. La Universidad de Helsinki
	había estado investigando sobre sistemas operativos desde finales de los 60, y allí no se veía ya el \emph{kernel} 
	de un sistema operativo como tema de investigación. En cierto modo tenían razón: los fundamentos de los
	sistemas operativos, y por extensión el \emph{kernel} Linux, se comprendían del todo desde principios de los 70; 
	todo lo que ha venido después ha sido un cierto ejercicio de autogratificación.

    Si se quiere que el código sea portable, no necesariamente tiene que crearse una capa de abstracción
	para obtener la portabilidad. En su lugar, es mejor programar inteligentemente. En esencia, intentar 
	hacer portables los \emph{microkernels} es una pérdida de tiempo. Es como construir un coche excepcionalmente
	rápido y ponerle ruedas cuadradas. La idea de abstraer la única cosa que ha de ser rápida como el rayo 
	-el \emph{kernel}- es inherentemente contraproducente.

    Por supuesto, hay algo más que éso en las investigaciones sobre \emph{microkernels}. Pero una gran parte del
	problema reside en la diferencia de objetivos. La finalidad de muchas investigaciones sobre \emph{microkernels}
	era el diseñar para un ideal teórico, llegar a un diseño que fuese lo más portable posible a cualquier 
	arquitectura concebible. Con Linux yo no pretendía un objetivo tan grande. Estaba interesado en la
	portabilidad entre sistemas reales, no en sistemas teóricos.

  

  
    
\section*{De Alpha a la portabilidad}

    El \emph{port} a Alpha comenzó en 1993, y completarlo llevó cerca de un año. El \emph{port} no estaba completamente
	realizado después de un año, pero ya teníamos los fundamentos. Aunque este primer \emph{port} fue difícil, estableció
	ciertos principios de diseño que Linux ha seguido desde entonces, y que han hecho más fáciles otros \emph{ports}.

    El \emph{kernel} Linux no está escrito para ser portable a cualquier arquitectura. Decidí que si una arquitectura de
	destino es suficientemente sólida, y sigue unas reglas básicas, entonces Linux 
	apoyará esa clase de modelo. Por ejemplo, la gestión de memoria puede ser muy distinta de una máquina a otra.
	Después de leerme la documentación sobre la gestión de memoria del 68K, el Sparc, el Alpha y el PowerPC, encontré 
	que aunque había diferencias en los detalles, había también mucho en común en el uso de paginación, \emph{caching}, etc. 
	La gestión de memoria del \emph{kernel} Linux podría ser escrita bajo un denominador común a estas arquitecturas y entonces
	no sería tan difícil modificar el código de control de memoria para los detalles de cada arquitectura en particular.

    Unas cuantas premisas simplifican mucho el problema del \emph{port} a otras arquitecturas. Por ejemplo, si se dice 
	que una CPU tiene que disponer de paginación, entonces por extensión debe tener alguna clase de TLB (translation lookup buffer), 
	que dice a la CPU cómo mapear la memoria virtual para uso de 
	la misma CPU. Naturalmente, uno no está seguro de cómo es el TLB. Pero realmente lo único que se necesita saber
	es cómo llenarlo y cómo vaciarlo cuando se decide qué debe borrase. Entonces, en esta arquitectura ''bien concebida'' 
	se sabe que se necesitan en el \emph{kernel} unas pocas piezas específicas para la máquina, pero la mayor parte del código
	está basada en mecanismos genéricos gracias a los cuales algo como el TLB trabaja.

    Otra regla práctica que sigo es que siempre es mejor una constante en tiempo de compilación que una variable, 
	y a menudo --siguiendo esta regla-- el compilador optimizará mejor el código. Esto es obviamente astuto, porque 
	se podrá tener listo un código de definición flexible y a la vez  de fácil optimización.

    Lo que interesa de este enfoque --el enfoque de intentar definir una sólida arquitectura común-- es 
	que procediendo así se puede ofrecer al SO una arquitectura mejor que la disponible en realidad en
	la plataforma de \emph{hardware} en cuestión. No parece intuitivo, pero es importante. Las generalizaciones
	que se buscan cuando se examinan sistemas se corresponden con las optimizaciones que a uno le gustaría hacer
	para mejorar el rendimiento del \emph{kernel}.

    Si se hacen inspecciones profundas suficientes en cosas como la implementación de la tabla de páginas, y se 
	toma una decisión basada en las observaciones --digamos, que el árbol de páginas debería tener sólo tres niveles
	de profundidad-- se comprueba más adelante que sólo podría haberse hecho así de haber interesado únicamente 
	la obtención de un alto rendimiento. En otras palabras, si no se ha pensado en la portabilidad como un objetivo de diseño, 
	sino en la optimización del \emph{kernel} para una arquitectura determinada, frecuentemente se alcanza la misma 
	conclusión --que la profundidad óptima en el \emph{kernel} para representar el árbol de páginas es de tres.

    Esto no es sólo cuestión de suerte. A menudo cuando una arquitectura se desvía del sólido diseño general
	en alguno de sus detalles es porque es un mal diseño. Así, los mismos principios que nos llevaron a las
	especificaciones de diseño para obtener la portabilidad también nos llevan por un lado a malos diseños y 
	por otro a atenernos a un diseño general más optimizado. Fundamentalmente yo intenté alcanzar una
	base mixta, mezclando lo mejor de la teoría con los hechos reales de las arquitecturas de los ordenadores actuales.

  

  
    
\section*{Espacio de \emph{kernel} y espacio de usuario}
    Con un \emph{kernel} monolítico como el \emph{kernel} Linux, es importante ser muy cauto al introducir en el \emph{kernel} código
	nuevo o nuevas prestaciones. Estas decisiones pueden afectar después a muchos aspectos del ciclo de desarrollo
	más allá del trabajo central en el \emph{kernel}.

    La primera regla básica es evitar los interfaces. Si se desea añadir algo que implique un nuevo interfaz 
	del sistema es preciso ser extraordinariamente cauteloso. Una vez se da a los usuarios un interfaz, 
	comenzarán a escribir código para él y una vez alguien comienza a escribir código, hay que atenerse a él.
	¿Quieres mantener exactamente el mismo interfaz para el resto de vida del sistema?

    El otro tipo de código no es tan problemático. Si no lleva interfaz, digamos un driver de disco, no hay
	mucho que pensar: puede añadirse un nuevo driver de disco con poco riesgo. Si Linux no tenía antes ese driver,
	añadirlo ahora no daña a nadie que ya use Linux, y Linux se abre a nuevos usuarios.

    Cuando te ofrecen código nuevo, es preciso ponderar. ¿Es una buena implementación? ¿Se está añadiendo de veras 
	una característica buena? A veces, incluso cuando la característica es buena, se da que o el interfaz es
	malo o la implementación de ese tipo de características implica que algún otro tipo de función deje de 
	efectuarse, ahora o en el futuro.

    Por ejemplo -- piense que se trata de un problema de interfaces --, suponga que alguien tiene una implementación estúpida
	de un sistema de archivos en el que los nombres no pueden tener más de 14 caracteres. Lo típico que 
	uno quiere evitar siempre porque es difícil de modificar. Por otro lado, cuando quieres extender
	el sistema de archivos, te quedas con un palmo de narices porque hay que encontrar la forma de ajustar este
	interfaz peor que antes ya estaba bloqueado. Peor aún, todo programa que solicite un nombre de archivo puede
	tener espacio sólo para una variable de, digamos, 13 caracteres, así que si se le pasa un nombre de archivo
	más largo, lo hará pedazos.

    Actualmente, el único fabricante que hace cosas así de estúpidas es Microsoft. En esencia, para leer 
	archivos DOS/Windows se disponía de ese interfaz ridículo en el que todos los archivos tenían once 
	caracteres, ocho más tres. Con NT, que permitía nombres de archivo largos, tuvieron que  añadir un conjunto 
	completo de nuevas rutinas para hacer lo mismo que hacían las otras rutinas, excepto que ese conjunto 
	podía manejar también nombres de archivo más largos. Éste es un ejemplo de un mal interfaz enturbiando 
	el futuro trabajo.
	
    Otro ejemplo es lo que ocurrió con el sistema operativo Plan 9. Tenía una muy buena llamada al 
	sistema para hacer mejor los \emph{forks} -- una forma sencilla para un programa de autodividirse
	en dos y continuar procesándose ambos conjuntamente. Esta nueva llamada \emph{fork}, que Plan 9 llamaba R-Fork 
	(y SGC llamó después S-Proc) esencialmente crea dos espacios de proceso separados que comparten el mismo
	espacio de direcciones. Esto es especialmente útil en el \emph{threading}.
	
    Linux hace esto con la llamada al sistema ''\emph{clone}'', pero implementada correctamente. Sin embargo, con las 
	rutinas SGI y Plan 9 se decidió que los programas con dos ramas pudieran compartir el mismo espacio de
	direcciones pero usar pilas separadas. Normalmente, cuando se usa el mismo espacio de direcciones en 
	ambos hilos, se obtiene la misma asignación de memoria. Pero se tiene un segmento de pila que es
	específico, así que si se usan direcciones de memoria basadas en pila en realidad se obtienen dos 
	asignaciones de memoria distintas que pueden compartir un puntero de pila sin sobrescribir la otra pila.

    Aunque esto es un inteligente resultado, la contra es que el coste necesario para mantener 
	las pilas hace que resulte estúpido ponerlo en práctica. Se dieron cuenta demasiado tarde de que 
	el rendimiento se iba al infierno. Dado que tenían programas que usaban el interfaz, no pudo 
	corregirse. En su lugar, tuvieron que introducir un interfaz adicional escrito apropiadamente de
	forma que pudieran hacer un uso sensato del espacio de pila.

    Mientras un fabricante puede a veces presionar para cambiar el diseño de la arquitectura, en el caso de
	Linux no tenemos libertad para hacer eso.

    Éste es otro caso en el que la gestión del desarrollo de Linux y la toma de decisiones sobre su diseño
	dictaron el mismo enfoque. Desde un punto de vistan práctico, yo no podía entendérmelas con un montón 
	de desarrolladores contribuyendo al \emph{kernel} con interfaces. No hubiese sido capaz de mantener el control
	sobre el \emph{kernel}. Pero desde un punto de vista de diseño ésta es también la opción correcta: mantener 
	el \emph{kernel} relativamente pequeño, y mantener al mínimo el número de interfaces y otras limitaciones
	sobre el desarrollo futuro.


    Desde luego, a este respecto Linux no está completamente limpio. Linux ha heredado cierto número 
	de interfaces horribles procedentes de implementaciones anteriores de Unix. Por eso, en algunos casos
	habría sido más feliz si no tuviese que mantener el mismo interfaz que Unix. Pero Linux es todo lo
	limpio que un sistema operativo no creado desde cero pueda ser. Y si se busca el beneficio del uso de 
	aplicaciones Unix, hay que llevar como equipaje algunas características Unix. Poder utilizar estas 
	aplicaciones ha sido vital para la popularidad de Linux, así que ha merecido la pena este equilibrio.


  

  
    
\section*{GCC}
    El propio Unix es la historia de un gran éxito en términos de portabilidad. El \emph{kernel} Unix, 
	como muchos \emph{kernels}, cuenta con la existencia de C para proporcionarle la mayor parte de la 
	portabilidad que necesita. Lo mismo que Linux. Para Unix, la amplia disponibilidad de compiladores
	de C en muchas arquitecturas hizo posible portarlo a esas arquitecturas.

    Así que Unix subraya la importancia de los compiladores. La importancia de los compiladores fue
	una de las razones por las que opté por licenciar Linux bajo la GNU Public License (GNU-GPL). 
	La GPL era la licencia del compilador GCC. Creo que todos los demás proyectos del grupo GNU son
	para Linux insignificantes en comparación. GCC es el único del que me preocupo. Algunos de ellos
	los odio con pasión: el editor Emacs es horrible, por ejemplo. Aunque Linux es más grande que Emacs,
	al menos Linux tiene la excusa de que necesita serlo.

    Pero los compiladores son básicamente una verdadera necesidad primaria.

    Ahora que el \emph{kernel} Linux sigue un diseño generalmente portable, al menos para arquitecturas 
	razonablemente sólidas, la portabilidad debería ser posible en tanto en cuanto se disponga de un compilador
	razonablemente bueno. Desde el punto de vista del \emph{kernel}, no me preocupa demasiado la portabilidad 
	de las arquitecturas de los próximos chips: me preocupo por los compiladores. El chip de Intel de 64 bits, el Merced, es
	un ejemplo obvio, porque Merced es muy diferente para un compilador.

    Así que la portabilidad de Linux está muy ligada al hecho de que GCC haya sido portado a las 
	arquitecturas de los chips importantes.


  

  
    
\section*{Los módulos del \emph{kernel}}

    Con el \emph{kernel} Linux quedó claro muy pronto que buscábamos un sistema tan modular como fuera posible.
	En realidad, el modelo de desarrollo open-source lo requiere así, porque de lo contrario no es fácil 
	para la gente trabajar en paralelo. Es penoso verles trabajar en la misma parte del \emph{kernel} de forma 
	incompatible.

    Sin modularidad yo tendría que comprobar cada archivo que se modificara, lo que sería un trabajo enorme,
	para asegurarme de que ninguna modificación afectara a cualquier otra parte. Con modularidad, si 
	alguien me envía parches para un nuevo sistema de ficheros y no confío necesariamente en los
	parches ``per se'', puedo confiar todavía en el hecho de que si nadie usa ese sistema de ficheros,
	no habrá conflictos.

    Por ejemplo, Hans Reiser está trabajando en un nuevo sistema de ficheros, que apenas ha puesto en
	funcionamiento. No creo que en este momento merezca la pena intentar incluirlo en el \emph{kernel} 2.2. 
	Pero gracias a la modularidad del \emph{kernel} yo podría hacerlo si realmente quisiera, no sería demasiado
	difícil. La clave es mantener la gente sin pisarse unos a otros.

    Con el \emph{kernel} 2.0, Linux ciertamente creció mucho. Éste fue el punto en que incluimos módulos del \emph{kernel}
	cargables. Obviamente esto mejoró la modularidad, creándose una estructura explícita para escribir
	módulos. Los programadores podían trabajar en módulos diferentes sin riesgo de interferencias. Yo 
	podía mantener adecuadamente el control sobre lo que era escrito en el \emph{kernel}. Así que, de nuevo,
	manejar el personal y manejar el código llevaron a la misma decisión de diseño. Para mantener coordinada
	a la gente que trabajaba en Linux, necesitábamos algo como los módulos de \emph{kernel}. Pero desde el punto de 
	vista del diseño era también la forma correcta de proceder.

    El otro aspecto de la modularidad es menos obvio, y más problemático. Se trata de la parte de carga en
	tiempo de ejecución, que todo el mundo está de acuerdo es algo bueno, pero que trae consigo
	nuevos problemas. El primer problema es técnico, pero los problemas técnicos son (casi) siempre los más 
	fáciles de resolver. El problema más importante reside en los aspectos no técnicos. Por ejemplo, ¿hasta qué
	punto un módulo es obra derivada de Linux, y por tanto protegido por la GPL?

    Cuando se terminó el primer módulo, había gente que ya tenía escritos drivers para SCO, y no 
	querían liberar las fuentes, como exige la GPL, pero sí querían recompilar para proporcionar binarios 
	Linux. En ese punto, por razones morales, decidí que no podía aplicar la GPL a este tipo de situaciones.

    La GPL exige que la obra ``derivada de'' una obra licenciada bajo GPL sea también licenciada bajo GPL.
	Desafortunadamente, es difícil determinar qué es una ''obra derivada''. Tan pronto
	se intenta trazar la línea divisoria de las obras derivadas, surge el problema de ¿por dónde se traza?

    Nosotros acabamos decidiendo (o tal vez yo acabé decretando) que las llamadas al sistema no serían consideradas
	enlazadas con el \emph{kernel}. Es decir, cualquier programa ejecutándose sobre Linux no sería considerado 
	GPL. Esta decisión se tomó muy pronto, e incluso añadí un archivo especial ''read-me'' (ver
	Apéndice B) para asegurarme de que todo el mundo la conocía. Por esta razón, los fabricantes comerciales 
	pueden escribir programas para Linux sin tener que preocuparse por la GPL.

    El resultado para los creadores de módulos fue que podía escribirse un módulo propietario 
	si para cargarlo sólo se usaba el interfaz normal. Pero ésta es todavía un área gris del \emph{kernel}. 
	Estas áreas grises suponen huecos para quienes quieren aprovecharse del estado de las cosas, quizás, 
	y es en parte a causa de que la GPL no es clara en cuestiones tales como interfaces de módulos. 
	Si alguien fuera a abusar de las líneas básicas mediante el uso de símbolos exportados en modo tal
	que burlara la GPL, me parece que estaríamos ante un caso punible. Pero no creo que nadie vaya 
	a abusar del \emph{kernel}: aquellos que han mostrado interés comercial en el \emph{kernel} lo han hecho porque 
	están interesados en los beneficios del modelo de desarrollo.

    El poder de Linux deriva en buena parte de la comunidad cooperativa que está detrás tanto como del
	código mismo. Si Linux fuera raptado -- si alguien tratara de hacer y distribuir una versión cerrada --
	, el atractivo de Linux, que es esencialmente el modelo de desarrollo open-source, se perdería 
	para esa versión cerrada.

  

  
    
\section*{La portabilidad hoy}

    Hoy Linux ha alcanzado muchos de los objetivos de diseño que originalmente se asumía que sólo
	podrían alcanzarse con una arquitectura \emph{microkernel}.

    Al construirse un modelo general del \emph{kernel} extraído de los elementos comunes de las arquitecturas
	típicas, el \emph{kernel} Linux se beneficia de su portabilidad de tal manera que, de otro modo, requeriría una capa 
	de abstracción, y sin la penalización en el rendimiento que pagan los \emph{microkernels}.

    Al permitirse los módulos del \emph{kernel}, el código específico para una arquitectura puede quedar 
	confinado en un módulo, manteniendo el corazón del \emph{kernel} altamente portable. Los \emph{drivers} de 
	dispositivos son un buen ejemplo del uso efectivo de módulos del \emph{kernel} para mantener las 
	partes específicas del \emph{hardware} en los módulos. Éste es un buen término medio entre poner todo 
	lo específico del \emph{hardware} en el corazón del \emph{kernel} -- que lo hace más rápido y menos portable --
	y ponerlo en el espacio de usuario -- de lo que resulta un sistema lento o inestable, o ambas cosas.

    Pero el enfoque de Linux sobre la portabilidad ha sido beneficioso también para la comunidad de 
	desarrolladores del entorno de Linux. Las decisiones que motivan la portabilidad permiten asímismo a 
	grandes grupos de personas trabajar simultáneamente en partes de Linux sin dejar el \emph{kernel} fuera de mi
	control. Las generalizaciones de arquitecturas sobre las que se basa Linux me proporcionan un marco 
	de referencia para comprobar las modificaciones en el \emph{kernel}, y me facilitan suficiente abstracción 
	para no tener que mantener bifurcaciones del código completamente separadas para arquitecturas 
	separadas. Así que incluso aunque mucha gente trabaje en Linux, el corazón del \emph{kernel} sigue siendo 
	algo a lo que puedo seguir la pista. Y los módulos del \emph{kernel} proveen un medio obvio a los 
	programadores de trabajar independientemente en partes del sistema que efectivamente deberían 
	ser independientes.

  

  
    
\section*{El futuro de Linux}

    Estoy seguro de que tomamos la decisión acertada con Linux al operar lo menos posible en el espacio
	del \emph{kernel}. En este momento, la verdad es que no auguro grandes actualizaciones en el \emph{kernel}. Un proyecto
	de \emph{software} que tenga éxito debe madurar a partir de cierto punto, y descender entonces el ritmo de los 
	cambios. No hay muchas innovaciones mayores que añadir al \emph{kernel}. Ahora se trata más bien de apoyar 
	un rango más amplio de sistemas: aprovechar la ventaja de la portabilidad de Linux para llevarlo hacia 
	nuevos sistemas.

    Aparecerán nuevos interfaces, pero creo que llegarán en parte por el apoyo a un rango mayor de sistemas. 
	Por ejemplo, cuando empiezas a hacer \emph{clustering}, entonces de repente es necesario indicar al planificador 
	que organice ciertos grupos de procesos mediante planificación anidada (\emph{gang scheduling}), y cosas así. Pero
	al mismo tiempo, no quiero a todo el mundo centrado en el \emph{clustering} y en la supercomputación, porque
	buena parte del futuro puede estar en los portátiles o en tarjetas que se enchufan a cualquier sitio donde
	vayas y cosas similares, así que preferiría que Linux fuese también en esas direcciones.

    Y después están los sistemas embebidos, en los que en realidad no hay interfaz de usuario en absoluto. 
	Sólo se accede al sistema para, quizá, mejorar el \emph{kernel}, pero poco más. Luego ésta es 
	otra dirección para Linux. No creo que Java o Inferno (el sistema operativo embebido de Lucent) vayan a
	tener éxito en sistemas embebidos. Se han olvidado del significado de la Ley de Moore. De primeras 
	suena bien eso de diseñar una especificación de sistema optimizada para un dispositivo embebido particular,
	pero en cuanto se obtenga un diseño funcional, la Ley de Moore habrá bajado el precio del \emph{hardware} más
	potente, minando el valor del diseño para un dispositivo específico. Se encuentra todo tan barato que es 
	mejor tener el mismo sistema en tu escritorio y en tu dispositivo embebido. Hará a todos la vida más fácil.

    El multiprocesamiento simétrico (SMP) es un área cuyo desarrollo aumentará. El \emph{kernel} Linux 2.2 manejará 4 
	procesadores más que bien, y desarrollaremos hasta ocho o dieciséis procesadores. El soporte para más 
	de cuatro procesadores ya está ahí, aunque no es usable. Tener ahora más de cuatro procesadores es como
	tirar el dinero. Así que esto ciertamente va a ser mejorado.

    Pero quienes quieran sesenta y cuatro procesadores, tendrán que usar una versión especial del \emph{kernel},
	porque poner tal soporte en el \emph{kernel} común causaría caídas de rendimiento a los usuarios normales.

    Ciertas áreas de aplicaciones particulares dirigirán el  desarrollo del \emph{kernel}. Los servidores web
	han sido siempre un problema interesante, porque es la única aplicación real que verdaderamente exige
	mucho del \emph{kernel}. En cierto modo, el servicio web ha sido peligroso para mí, porque tengo tantos encuentros
	con la comunidad de quienes usan Linux como plataforma de servicios web, que podría acabar optimizando
	sólo para servicio web. Debo tener en mente siempre que el servicio web es una aplicación importante, 
	pero no lo es todo.

    Por supuesto, Linux no está siendo usado en todo su potencial ni siquiera por los servidores web actuales.
	El mismo Apache no trabaja bien con \emph{threads}, por ejemplo.

    Este tipo de optimización se ha ralentizado por los límites del ancho de banda. En estos momentos\footnote{1999 (N. del T.)} 
	(1999), se saturan las redes de diez megabits tan fácilmente que no hay razón para optimizar más. El 
	único modo de no saturar las redes de diez megabits es tener una infinidad de CGIs a pleno 
	rendimiento. Pero en eso el \emph{kernel} no puede ayudar. Lo que el \emph{kernel} podría potencialmente hacer es
	responder directamente a las peticiones de páginas estáticas, y pasar las peticiones más complicadas a Apache.
	Una vez sean habituales redes más rápidas, el asunto se hará más interesante. Pero por el momento 
	no tenemos la masa crítica de \emph{hardware} para comprobaciones y desarrollo.

    La lección de todas estas posibles direcciones futuras es que quiero a Linux justo al límite, e
	incluso un poco más allá, porque lo que está más allá hoy es lo que estará en tu escritorio mañana.

    Pero los desarrollos más excitantes para Linux ocurrirán en el espacio de usuario, no en el espacio
	del \emph{kernel}. Los cambios en el \emph{kernel} parecerán pequeños comparados con lo que sucederán en el 
	sistema exterior. Desde esta perspectiva, dónde estará el \emph{kernel} Linux no es una pregunta tan
	interesante como qué prestaciones habrá en Red Hat 17.5 o adónde se dirigirá Wine (el emulador
	Windows) en unos pocos años.

    En quince años espero ver llegar a alguien diciendo: eh, yo hago todo cuanto Linux hace, pero soy
	más ligero e impetuoso porque mi sistema no tiene veinte años a sus espaldas. Dirá que Linux fue 
	diseñado para el 386 y las nuevas CPU hacen de forma muy distinta las cosas verdaderamente interesantes.
	Expulsemos este vejestorio de Linux. Esto es esencialmente lo que yo hice cuando creé Linux. Y en 
	el futuro, podrán mirar nuestro código, usar nuestros interfaces y proporcionar compatibilidad 
	binaria, y si todo esto ocurre seré feliz.

  


\end{document}

