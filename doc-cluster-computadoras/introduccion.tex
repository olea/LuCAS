\chapter{INTRODUCCIÓN.}
\minitoc
\section{Resumen proyecto.}
Debido al espectacular auge de la tecnología y de la informática en la actualidad los equipos informáticos
se quedan anticuados en un plazo corto de tiempo. Para aprovechar estos ``viejos'' equipos se puede construir un
cluster\footnote{conjunto de computadoras interconectadas con dispositivos de alta velocidad que actúan en
conjunto usando el poder cómputo de varios CPU en combinación para resolver ciertos problemas dados}, de
forma que la capacidad de computación que obtengamos pueda llegar a compensarnos ante la inversión a realizar
en equipos más potentes. Programas con gran carga de computación tardarían bastante tiempo en realizarse
en uno de estos ``viejos'' ordenadores. Se pretende usar estos ordenadores para construir un cluster y con él
reducir este tiempo de computación mediante el reparto de la carga entre sus nodos.
	
Este proyecto se basa en el desarrollo de un cluster de computadoras heterogéneo formado por un
front-end\footnote{ordenador central o servidor encargado de gestionar los nodos y operaciones que se
realizan en el cluster} y veinte y tres nodos.

La realización de este cluster de computadoras heterogéneo estará compuesta de dos fases, una de configuración
hardware y otra de desarrollo software. La configuración hardware estará a su vez dividida en tres partes, configuración
del equipo central o front-end, configuración de los respectivos nodos que formarán parte del cluster y configuración
de la red de interconexión.

La configuración software consistirá en la realización de programas para control de latencia de red y
ancho de banda de la misma, además de un programa para obtener el incremento de rendimiento o ganancia de velocidad
que se consigue añadiendo nuevos nodos a la resolución del problema.

Además, se instalarán programas que nos permitirán observar los detalles de utilización de cada nodo:
números de usuarios conectados al nodo, memoria libre, memoria swap disponible, etc.

\section{Memoria descriptiva.}
Instalación mínima del sistema operativo en cada uno de los nodos, es decir, se instalará los servicios
mínimos para realizar un procesamiento paralelo y servicios de acceso para que el sistema funcione correctamente.
Además se recompilará el núcleo del sistema operativo.

Configuración de la red de interconexión entre los nodos y el ordenador central o front-end a través del
protocolo DHCP (Dinamic Host Configure Protocol) de forma que el ordenador central sea el encargado
de asignar la dirección IP correspondiente a cada nodo, en función de la dirección MAC (Medium Access Control)
de la tarjeta de red Ethernet.

Configuración de un firewall en la máquina central o front-end que permita únicamente el acceso desde
el exterior a éste a través del protocolo SSH Secure SHell.

Los nodos añadidos se configurarán de forma que puedan arrancar vía NFS (Network File System) a
través del protocolo DHCP, obteniendo su dirección IP mediante el protocolo DHCP cuando el nodo arranque de
forma local.

Instalación del software PVM (Parallel Virutal Machine) y XPVM (A Graphical Console and Monitor for
PVM). Configuración de dicho software, de forma que permita la ejecución de programas paralelos basados en el
reparto de la carga computacional mediante paso de mensajes entre los distintos nodos del cluster.

Instalación y configuración del software LAM/MPI (Local Area Multicomputer/Message Passing Interface)
con el mismo propósito que PVM. Se realizarán comparativas entre ambos paquetes a través de la implementación
de programas.

Instalación de herramientas software que nos permitan observar el estado del cluster.

\section{Planificación.}
Se realizará un estudio sobre qué distribución Linux se ajusta mejor a nuestras necesidades en función
de los ordenadores que disponemos para realizar el cluster.

Instalación mínima distribución Linux en el front-end y recompilación del núcleo.
	
Configuración protocolo DHCP.
	
Configuración de un firewall en la máquina central o front-end.
	
Instalación y configuración de PVM y XPVM. Desarrollo de programas en PVM.
	
Instalación y configuración de LAM/MPI. Desarrollo de programas en LAM/MPI.
	
Instalación de herramientas software para conocer el estado del ordenador central y los restantes nodos.

\section{Introducción a los cluster de computadoras.}
\subsection{¿Que es un cluster de computadoras?}
Un cluster es un grupo de equipos independientes que ejecutan una serie de aplicaciones de forma
conjunta y aparecen ante clientes y aplicaciones como un solo sistema. Los clusters permiten aumentar la
escalabilidad, disponibilidad y fiabilidad de múltiples niveles de red.

La escalabilidad es la capacidad de un equipo para hacer frente a volúmenes de trabajo cada vez mayores
sin, por ello, dejar de prestar un nivel de rendimiento aceptable. Existen dos tipos de escalabilidad:
\begin{itemize}
\item Escalabilidad del hardware (también denominada «escalamiento vertical»). Se basa en la
utilización de un gran equipo cuya capacidad se aumenta a medida que lo exige la carga de trabajo existente.
\item Escalabilidad del software (también denominada «escalamiento horizontal»). Se basa, en
cambio, en la utilización de un cluster compuesto de varios equipos de mediana potencia que funcionan en tándem de
forma muy parecida a como lo hacen las unidades de un RAID (Redundant Array of Inexpensive Disks o Array
redundante de discos de bajo coste). Se utilizan el término RAC (Redundant Array of Computers o Array redundante
de equipos) para referirse a los clusters de escalamiento horizontal. Del mismo modo que se añaden discos a
un array RAID para aumentar su rendimiento, se pueden añadir nodos a un cluster para aumentar también su
rendimiento.
\end{itemize}

La disponibilidad y la fiabilidad son dos conceptos que, si bien se encuentran íntimamente relacionados,
difieren ligeramente. La disponibilidad es la calidad de estar presente, listo para su uso, a mano, accesible;
mientras que la fiabilidad es la probabilidad de un funcionamiento correcto.

Pero hasta el más fiable de los equipos acaba fallando. Los fabricantes de hardware intentan anticiparse
a los fallos aplicando la redundancia en áreas clave como son las unidades de disco, las fuentes de alimentación,
las controladoras de red y los ventiladores, pero dicha redundancia no protege a los usuarios de los fallos
de las aplicaciones. De poco servirá, por lo tanto, que un servidor sea fiable si el software de base de datos
que se ejecuta en dicho servidor falla, ya que el resultado no será otro que la ausencia de disponibilidad.
Ésa es la razón de que un solo equipo no pueda ofrecer los niveles de escalabilidad, disponibilidad y
fiabilidad necesarios que sí ofrece un cluster.	

Vemos cómo los clusters imitan a los arrays RAID al aumentar el nivel de disponibilidad y fiabilidad.
En las configuraciones de discos tolerantes a fallos, como RAID 1 o RAID 5, todos los discos funcionan
conjuntamente formando un array redundante de modo que cuando uno de ellos falla, sólo hay que reemplazarlo
por otro; el resto del array sigue funcionando sin problemas, sin necesidad de que se efectúen tareas de
configuración y, lo que es más importante, sin que se produzcan tiempos muertos. En efecto, el sistema RAID
reconstruye automáticamente la unidad nueva para que funcione conjuntamente con las restantes. De igual modo,
cuando falla un equipo que forma parte de un cluster, sólo hay que sustituirlo por otro. Algunos programas de
cluster incluso configuran e integran el servidor de forma automática en el cluster, y todo ello sin que el cluster
deje de estar disponible ni un solo instante.
	
En definitiva, un cluster es un conjunto de computadoras interconectadas con dispositivos de alta
velocidad que actúan en conjunto usando el poder cómputo de varios CPU en combinación para resolver ciertos
problemas dados.
	
Se usa un cluster con varios computadores para crear un supercomputador.

Hoy día los supercomputadores son equipos excesivamente costosos que están fuera del alcance de
empresas o instituciones pequeñas. Un cluster, siendo una combinación de equipos microcomputadores
(IBM PC Compatibles), puede ser instalado inclusive por particulares y puede ofrecer rendimiento muy cercano
a un SuperComputador en cuanto a poder de cómputo.

En pocas palabras imagínate unos 20 PCs Pentium II ó III de 500 Mhz que actúan en conjunto como
si fuese un sólo CPU de 10.000 Mhz!!! (Si bien no es tan fácil como eso, sirve para ilustrar algo aproximado
a lo que se obtendrá).	

El surgimientos de plataformas computacionales de comunicación y procesamiento estándares de bajo costo,
les ha brindado la oportunidad a los programadores académicos de crear herramientas computacionales del dominio
público o de costo razonable. Estas realidades permiten la implantación de códigos paralelizados sobre este tipo
de plataformas obteniendo un rendimiento competitivo en relación a equipos paralelos especializados cuyos costos
de operación y mantenimiento son elevados.

Una de las herramientas de más auge en la actualidad son los llamados cluster Beowulf, los cuales presentan
diversas capacidades para el cómputo paralelo con un relativo alto rendimiento.
\subsection{Conceptos generales.}

Cluster Beowulf no es un paquete software especial, ni una nueva topología de red, ni un núcleo
modificado. Beowulf es una tecnología para agrupar computadores basados en el sistema operativo Linux para
formar un supercomputador virtual paralelo. En 1994 bajo el patrocinio del proyecto ESS del Centro
de la Excelencia en Ciencias de los Datos y de la Información del Espacio (CESDIS), Thomas Sterling
y Don Becker crearon el primer cluster Beowulf con fines de investigación.

Beowulf posee una arquitectura basada en multicomputadores el cual puede ser utilizado para la
computación paralela. Este sistema consiste de un nodo maestro y uno o más nodos esclavos conectados a través
de una red Ethernet u otra topología de red. Esta construido con componentes hardware comunes en el mercado,
similar a cualquier PC capaz de ejecutar Linux, adaptadores de Ethernet y switches estándares. Como no contiene
elementos especiales, es totalmente reproducible. Una de las diferencias principales entre Beowulf y un cluster
de estaciones de trabajo (COW, Cluster Of Workstations) es el hecho de que Beowulf se comporta más como
una sola máquina que como muchas estaciones de trabajo conectadas. En la mayoría de los casos los nodos esclavos
no tienen monitores o teclados y son accedidos solamente vía remota o por terminal serie. El nodo maestro
controla el cluster entero y presta servicios de sistemas de archivos a los nodos esclavos. Es también la consola
del cluster y la conexión hacia el exterior. Las máquinas grandes de Beowulf pueden tener más de un nodo maestro,
y otros nodos dedicados a diversas tareas específicas, como por ejemplo, consolas o estaciones de supervisión.
En la mayoría de los casos los nodos esclavos de un sistema Beowulf son estaciones simples. Los nodos son
configurados y controlados por el nodo maestro, y hacen solamente lo que éste le indique. En una configuración
de esclavos sin disco duro, estos incluso no saben su dirección IP hasta que el maestro les dice cuál es.	

\begin{figure}[h!]
\begin{center}
\epsfig{file=imagenes/intro/arquitectura.eps, width=4.25in}
\caption{Arquitectura genérica de un cluster Beowulf}
\end{center}
\end{figure}

La topología de red recomendada es un Bus, debido a la facilidad para proporcionar escalabilidad a la
hora de agregar nuevos nodos al cluster. Protocolos como Ethernet, Fast Ethernet, GigaEthernet, 10/100 Mbps Switched Ethernet,
etc, son tecnologías apropiadas para ser utilizadas en Beowulf.

Beowulf utiliza como sistema operativo cualquier distribución Linux. Además usa bibliotecas de paso
de mensajes como PVM y MPI.
\newpage
Sin lugar a duda los cluster presenta una alternativa importante para varios problemas particulares, no
solo por su economía, sino también porque pueden ser diseñados y ajustados para aplicaciones específicas.

\subsection{Clasificación.}
Para establecer las diferencias entre los distintos tipos de sistemas Beowulf se presenta la siguiente
clasificación.
\begin{itemize}
\item Clase I. Son sistemas compuestos por máquinas cuyos componentes cumplen con la prueba
de certificación ``Computer Shopper'' lo que significa que sus elementos son de uso común, y pueden ser adquiridos
muy fácilmente en cualquier tienda distribuidora. De esta manera, estos clusters no están diseñados para ningún
uso ni requerimientos en particular.
\item Clase II. Son sistemas compuestos por máquinas cuyos componentes no pasan la prueba de
certificación ``Computer Shopper'' lo que significa que sus componentes no son de uso común y por tanto no pueden
encontrarse con la misma facilidad que los componentes de sistemas de la clase anterior. De tal manera, pueden
estar diseñados para algún uso o requerimiento en particular. Las máquinas ubicadas en esta categoría pueden
presentar un nivel de prestaciones superior a las de la clase I.
\end{itemize}
