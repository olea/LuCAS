<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"><html><head><title>Why Open Source Software / Free Software (OSS/FS)? Look at the Numbers!</title>

<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="keywords" content="open source, open source software, free software, libre software, libre, Linux, freedom, software, free, software free, software libre, OSS, FS, OSS/FS, GNU, GNU/Linux, Unix, Windows, NT, Windows 2000, Apache, fuzz, performance, scaleability, reliability, market, market share, references, cost, total cost of ownership, quantitative, study, numbers, analysis, analyses, proprietary, GPL, General Public License, TCO, Microsoft">
<meta name="description" content="This paper provides quantitative data that, in many cases, open source software / free software is equal to or superior to their proprietary competition.  The paper examines market share, reliability, performance, scaleability, security, and total cost of ownership; it also comments on non-quantitative issues and unnecessary fears.">

<link rev="made" href="mailto:dwheeler@dwheeler.com"></head>

<body bgcolor="#ffffff">
<center>
<h1><font color="#339900">Why Open Source Software / Free Software (OSS/FS)? Look at the Numbers!</font></h1>
</center>
<center>David A. Wheeler</center>
<center>dwheeler@dwheeler.com</center>
<center>Revised as of April 26, 2002</center>
<p>
<i>This paper provides quantitative data that, in many cases, using
<a href="http://www.dwheeler.com/oss_fs_refs.html">open source software / free software</a>
is a reasonable or even superior approach
to using their proprietary competition according to various measures.
This paper examines
<a href="#market_share">market share</a>,
<a href="#reliability">reliability</a>,
<a href="#performance">performance</a>,
<a href="#scaleability">scaleability</a>,
<a href="#security">security</a>, and
<a href="#tco">total cost of ownership</a>.
It also has sections on
<a href="#non_quantitative">non-quantitative issues</a>,
<a href="#fears">unnecessary fears</a>, and
<a href="#other">other sites providing related information</a>,
and ends with some
<a href="#conclusions">conclusions</a>.
</i>
</p><p>
</p><h1><a name="introduction">Introduction</a></h1>
<p>
<a href="http://www.dwheeler.com/oss_fs_refs.html">Open Source Software / Free Software (OSS/FS)</a>
has risen to great prominence.
Briefly, OSS/FS programs are programs whose licenses give
users the freedom to run the program for any purpose,
to modify the program, and to freely redistribute either the
original or modified program (e.g.,
without further limitations or royalty payments).
</p><p>
Some sites provide a few anecdotes on why you should use OSS/FS, but
for many that's not enough information to justify using OSS/FS.
Instead, this paper emphasizes <i>quantitative</i> measures
(such as experiments and market studies) on why
using OSS/FS products is, in a number of circumstances,
a reasonable or even superior approach.
I should note that while I find much to like about OSS/FS, I'm not a
rabid advocate; I use both proprietary and OSS/FS products myself.
Vendors of proprietary products often work hard to find numbers to support
their claims; this page provides a useful antidote of hard figures to
aid in comparing these proprietary products to OSS/FS.
</p><p>
Note that this paper's goal is <i>not</i> to show that all OSS/FS is
better than all proprietary software.
There are those who believe this is true from ethical, moral, or
social grounds, but no numbers could prove such a broad statement.
Instead, I'll simply compare commonly-used OSS/FS software with
commonly-used proprietary software, to show that at least in certain
situations and by certain measures, OSS/FS is at least as good or better
than its proprietary competition.
</p><p>
I'll emphasize the GNU/Linux operating system (which some abbreviate as
<a href="http://www.dwheeler.com/oss_fs_refs.html#linux-vs-gnu-linux">``Linux''</a>)
and the Apache web server,
since these are some of the most visible OSS/FS projects.
I'll also primarily compare OSS/FS software to Microsoft's
products (such as Windows and IIS), since Windows has a
significant market share and Microsoft is one of proprietary software's
strongest proponents.
I'll mention Unix systems in passing as well, though the situation with
Unix is more complex; many Unix systems include a number of OSS/FS
components or software primarily derived from OSS/FS components.
Thus, comparing proprietary Unix systems to OSS/FS systems
(when examined as entire systems) is often not as clear-cut.
I use the term ``Unix-like'' to mean systems intentionally similar to Unix;
both Unix and GNU/Linux are ``Unix-like'' systems.
The most recent Apple Macintosh operating system (MacOS OS X)
presents the same kind of complications;
older versions of MacOS were entirely proprietary, but
Apple's operating system has been redesigned so that it's now
based on a Unix system with a substantial contribution from OSS/FS programs.
Indeed, <a href="http://www.opensource.apple.com/">Apple is now
openly encouraging collaboration with OSS/FS developers</a>.
I include data over a series of years, not just the past year;
I believe that all relevant data should be considered when making a
decision, instead of arbitrarily ignoring older data, and the older data
shows that OSS/FS has a history of many positive traits.
</p><p>
You can get a more detailed explanation of the terms ``open source software''
and ``Free Software'', as well as related information, from my
<a href="http://www.dwheeler.com/oss_fs_refs.html">list of
Open Source Software / Free Software (OSS/FS) references at
http://www.dwheeler.com/oss_fs_refs.html</a>.
Note that those who use the term
<a href="http://www.opensource.org/docs/definition.html"> ``open source software''</a> tend to emphasize
technical advantages of such software
(such as better reliability and security), while those who use the term
<a href="http://www.gnu.org/philosophy/free-sw.html">``Free Software''</a> tend to emphasize freedom from control by another and/or ethical issues.
The opposite of OSS/FS is ``closed'' or ``proprietary'' software.
Note that many OSS/FS programs are commercial programs, so don't make the
mistake of calling OSS/FS software ``non-commercial.''
Almost no OSS/FS programs are in the ``public domain''
(which has a specific legal meaning), so avoid that term as well.
Some call OSS/FS software ``public service software'', since often these
software projects are designed to serve the public at large, or
``libre software'' (where libre means free as in freedom).

</p><p>
Below is data discussing
<a href="#market_share">market share</a>,
<a href="#reliability">reliability</a>,
<a href="#performance">performance</a>,
<a href="#scaleability">scaleability</a>,
<a href="#security">security</a>, and
<a href="#tco">total cost of ownership</a>.
I close with a brief discussion of
<a href="#non_quantitative">non-quantitative issues</a>,
<a href="#fears">unnecessary fears</a>,
<a href="#other">other sites providing related information</a>, and
<a href="#conclusions">conclusions</a>.

</p><h1><a name="market_share">Market Share</a></h1>
<p>
Many people believe that a product is only a winner if it has significant
market share.
This is lemming-like, but there's some rationale for this: products
with big market shares get
applications, trained users, and momentum that reduces future risk.
Some writers argue against OSS/FS or GNU/Linux as ``not being mainstream'',
but if their use is widespread then such statements reflect the past,
not the present.
<!-- E.G., Computerworld, July 2, 2001, Vol. 35, #27, page 23,
    "Linux is Full of Fanatics, Potential" by Bill Laberis -->
There's excellent evidence that
OSS/FS has significant market share in numerous markets:
</p><ol>
<li>
<b>The most popular web server has always been OSS/FS
since such data have been collected, for example, Apache is currently
the #1 web server.</b>

<a href="http://www.netcraft.com/survey">Netcraft's statistics on webservers</a>
have consistently shown Apache (an OSS/FS web server) dominating
the public Internet web server market ever since Apache
became the #1 web server in April 1996.
Before that time, the NCSA web server (Apache's ancestor)
dominated the web from August 1995 through March 1996 - and it is also OSS/FS.
For example, in March 2002, Netcraft polled all the web sites they could
find (totaling 38,118,962 sites), and found that of
all the sites they could find, Apache had 53.76% of the market,
Microsoft had 34.02% of the market, iPlanet (aka Netscape) had 2.33%,
and Zeus had 2.24%.
<!--
 Originally, I placed here the original Netcraft graphics for
 "Market Share for Top Servers Across All Domains August 1995 - *".
 However, when I was slashdotted on April 2002, this graphic put
 a HEAVY burden on the server.

 Also, the original graphic was too large to correctly view
 or print for many.
 My thanks to Mark Bucciarelli, mark, at easymailings dot com,
 for figuring out how to use "mogrify" to create an easy-to-read result.
 His approach was to get the gif, then do the following:

 $ identify netcraft.gif
 netcraft.gif 806x250+0+0 PseudoClass 8c 8867b GIF 2s

  [250 * 640 / 806 = 198.5]

 $ cp netcraft.gif netcraft640.gif
 $ mogrify -geometry 640x198 netcraft640.gif
 $ mogrify -format png netcraft640.gif

  [touch up legend with different font]

 However, this process for updating the chart is time-consuming, and
 combined with the heavy server burden it causes, I decided to
 abandon the chart.
<p>
<center>
Market Share for Top Web Servers, August 1995 - November 2001<br>
<a href="frozen/netcraft2001.png">
<img align="middle" src="frozen/netcraft2001.png" width="638" height="198" alt="Thumbnail graph of market share for top servers across all domains, August 1995 - November 2001"></a>
</center>
-->
<p>
More recently, Netcraft has been trying to separately count only
``active'' sites.
The problem is that many web sites
have been created that are simply ``placeholder'' sites
(i.e., their domain names have been reserved but they are not being used).
Netcraft's count of active sites is probably a far more relevant figure,
since this shows
the web server selected by those who choose to develop a web site.
When counting only active sites, Apache does even better; by March 2002,
Apache had 64.37% of the web server market,
Microsoft had 26.81%, iPlanet 1.80%, and Zeus 1.15%.
</p><p>
<!-- Original file is at
http://www.netcraft.com/survey/Reports/200203/overalld.gif
-->
</p><center>
Market Share for Active Web Servers, June 2000 - March 2002<br>
<a href="http://www.dwheeler.com/frozen/netcraft-200203-overalld-active.gif">
<img align="middle" src="oss_fs_why_20020426.en_files/netcraft-200203-over.gif" width="482" height="250" alt="Active servers across all domains, June 2000 - March 2002"></a>
</center>
<p>
The same overall result has been determined independently by
<a href="http://www.securityspace.com/s_survey">E-soft</a> - their report
published April 1, 2002 surveyed 5,940,265 web servers in March 2002 and
found that Apache was #1 (65.80%), with Microsoft IIS being #2 (25.30%).
Obviously these figures fluctuate monthly; see
<a href="http://www.netcraft.com/survey">Netcraft</a> and
<a href="http://www.securityspace.com/s_survey">E-soft</a> for the
latest survey figures.

</p><p>
</p></li><li>
<b>GNU/Linux is the #2 web serving operating system on the public Internet
(counting by physical machine), according to a study by Netcraft
surveying March and June 2001.</b>
Some of <a href="http://www.netcraft.com/">Netcraft's</a>
surveys have also included data on operating systems;
two 2001 surveys
(their 
<a href="http://www.netcraft.com/Survey/index-200106.html#computers">June 2001</a> and
<a href="http://www.netcraft.com/Survey/index-200109.html#computers">September 2001</a>
surveys) found that GNU/Linux is the #2 operating system
for web servers when counting physical machines (and has been consistently
gaining market share since February 1999).
As Netcraft themselves point out,
the usual Netcraft web server survey (discussed above)
counts web server hostnames rather than physical computers,
and so it doesn't measure such things as the installed hardware base.
Companies can run several thousand web sites on a single computer,
and most of the world's web sites are located at hosting
and co-location companies.
<p>
Therefore, Netcraft developed a technique that
indicates the number of actual computers being used as Web servers,
together with the operating system and web server software used.
The technique is based on arranging a number of IP addresses to
send packets to Netcraft nearly simultaneously;
low level TCP/IP characteristics can be used to work out
if those packets originate from the same computer
by checking for similarities in a number of TCP/IP protocol header fields.
This is a statistical approach, so
many visits to the site are used over a month to build up sufficient certainty.
This technique has its weaknesses;
Round robin DNS, reverse web proxies,
some load balancing/failover products like Cisco LocalDirector and BIG-IP,
and some connection level firewalls hide
a number of web servers behind a hostname.
Only a single ``front'' web server will be counted, and
with some of these products the operating system detected
is that of the ``front'' device rather than the web server behind.
Still, Netcraft believes that the
error margins world-wide are well within the order of plus or minus 10%,
and this is the best available survey of such data.
</p><p>
Before presenting the data, it's important to explain Netcraft's
system for dating the data.
Netcraft dates their information based on the web server
surveys (not the publication date), and they only report operating system
summaries from an earlier month.
Thus, the survey dated ``June 2001'' was published in July and
covers operating system survey results of March 2001, while the survey
dated ``September 2001'' was published in October and covers the operating
system survey results of June 2001.
</p><p>
Here's a summary of Netcraft's study results:
</p><p>
<!-- <font size="-2"> -->
<table align="center" border="1" cellpadding="2" summary="GNU/Linux is Number Two for Web Server Operating Systems">
<tbody><tr bgcolor="#bac0ff"><th align="left">OS group</th><th align="right">Percentage (March)</th><th align="right">Percentage (June)</th><th align="left">Composition</th></tr>
<tr bgcolor="#ccccfe"><td align="left"><b>Windows</b></td><td align="right">49.2%</td><td align="right">49.6%</td><td align="left">Windows 2000, NT4, NT3, Windows 95, Windows 98</td></tr>
<tr bgcolor="#ccccfe"><td align="left"><b>[GNU/]Linux</b></td><td align="right">28.5%</td><td align="right">29.6%</td><td align="left">[GNU/]Linux</td></tr>
<tr bgcolor="#ccccfe"><td align="left"><b>Solaris</b></td><td align="right">7.6%</td><td align="right">7.1%</td><td align="left">Solaris 2, Solaris 7, Solaris 8</td></tr>
<tr bgcolor="#ccccfe"><td align="left"><b>BSD</b></td><td align="right">6.3%</td><td align="right">6.1%</td><td align="left">BSDI BSD/OS, FreeBSD, NetBSD, OpenBSD</td></tr>
<tr bgcolor="#ccccfe"><td align="left"><b>Other Unix</b></td><td align="right">2.4%</td><td align="right">2.2%</td><td align="left">AIX, Compaq Tru64, HP-UX, IRIX, SCO Unix, SunOS 4 and others</td></tr>
<tr bgcolor="#ccccfe"><td align="left"><b>Other non-Unix</b></td><td align="right">2.5%</td><td align="right">2.4%</td><td align="left">MacOS, NetWare, proprietary IBM OSs</td></tr>
<tr bgcolor="#ccccfe"><td align="left"><b>Unknown</b></td><td align="right">3.6%</td><td align="right">3.0%</td><td align="left">not identified by Netcraft operating system detector</td></tr>
</tbody></table>
<!-- </font> -->
</p><p>
Much depends on what you want to measure.
Several of the BSDs
(FreeBSD, NetBSD, and OpenBSD) are OSS/FS as well;
so at least a portion of the 6.1% for BSD should be added to GNU/Linux's
29.6% to determine the percentage of OSS/FS operating systems being used
as web servers.
Thus, it's likely that approximately one-third of web serving computers use
OSS/FS operating systems.
There are also regional differences, for example,
GNU/Linux leads Windows in Germany, Hungary, the Czech Republic, and
Poland.
</p><p>
<!-- I checked these as of November 30, 2001: -->
Well-known web sites using OSS/FS include
<a href="http://uptime.netcraft.com/up/graph/?mode_u=on&amp;mode_w=on&amp;site=www.google.com">Google</a> (GNU/Linux) and
<a href="http://uptime.netcraft.com/up/graph/?mode_u=on&amp;mode_w=on&amp;site=www.yahoo.com">Yahoo</a> (FreeBSD).
</p><p>
If you really want to know about the web server market breakdown of
``Unix vs. Windows,'' you can find that also in this study.
All of the various Windows operating systems are rolled into a single number
(even Windows 95/98 and Windows 2000/NT4/NT3 are merged together,
although they are fundamentally very different systems).
Merging all the Unix-like systems in a similar way
produces a total of 44.8% for Unix-like systems (compared
to Windows' 49.2%) in March 2001.
</p><p>
Note that these figures would probably be quite different if they were
based on web addresses instead of physical computers; in such a case,
the clear majority of web sites are hosted by Unix-like systems.
As stated by Netcraft, 
``Although Apache running on various Unix systems runs more sites than Windows,
Apache is heavily deployed at hosting companies and ISPs who strive
to run as many sites as possible on a single computer to save costs.''
</p><p>
</p></li><li>
<b>GNU/Linux is the #1 server operating system on the public Internet (counting
by domain name), according to a 1999 survey of primarily European and educational
sites.</b>
The first study that I've found that examined GNU/Linux's market penetration
is a survey by
<a href="http://www.leb.net/hzo/ioscount">Zoebelein in April 1999</a>.
This survey
found that, of the total number of servers deployed on the Internet in 1999
(running at least ftp, news, or http (WWW)) in a database of names they used,
the #1 operating system was
GNU/Linux (at 28.5%), with others trailing.
It's important to note that this survey, which is the first one that
I've found to try to answer questions of market share,
used existing databases of servers
from the .edu (educational domain) and the RIPE database
(which covers Europe , the Middle East, parts of Asia, and parts of Africa),
so this isn't really a survey of ``the entire Internet'' (e.g., it
omits ``.com'' and ``.net'').
This is a count by domain <i>name</i> (e.g., the text name you would type
into a web browser for a location) instead of by physical
computer, so what it's
counting is different than the Netcraft June 2001 operating system study.
Also, this study counted servers providing ftp and news services
(not just web servers).
<p>
Here's how the various operating systems fared in the study:

<!-- with others trailing (24.4% for all Windows 95/98/NT
combined, 17.7% for Solaris or SunOS,
15% for the BSD family, and 5.3% for IRIX). -->

</p><p>
</p><center>
<table border="1" cellpadding="2" summary="GNU/Linux is Number 1">
<tbody><tr align="left" bgcolor="#bac0ff"><th>Market Share</th><th>Operating System</th><th>Composition</th></tr>
<tr bgcolor="#ccccfe"><td><b>GNU/Linux</b></td><td align="right">28.5%</td><td>GNU/Linux</td></tr>
<tr bgcolor="#ccccfe"><td><b>Windows</b></td><td align="right">24.4%</td><td>All Windows combined (including 95, 98, NT)</td></tr>
<tr bgcolor="#ccccfe"><td><b>Sun</b></td><td align="right">17.7%</td><td>Sun Solaris or SunOS</td></tr>
<tr bgcolor="#ccccfe"><td><b>BSD</b></td><td align="right">15.0%</td><td>BSD Family (FreeBSD, NetBSD, OpenBSD, BSDI, ...)</td></tr>
<tr bgcolor="#ccccfe"><td><b>IRIX</b></td><td align="right">5.3%</td><td>SGI IRIX</td></tr>
</tbody></table>
</center>

<p>
A portion of the BSD family is also OSS/FS, so the OSS/FS operating system
total is even higher; if over 2/3 of the BSDs are OSS/FS, then the total
share of OSS/FS would be about 40%.
Advocates of Unix-like systems
will notice that the majority (around 66%) were running Unix-like
systems, while only around 24% ran a Microsoft Windows variant.
</p><p>
</p></li><li>
<b>GNU/Linux was the #2 server operating system sold in 1999 and 2000,
and is the fastest-growing.</b>
According to
<a href="http://www.idc.com/itforecaster/itf20000808.stm">a June 2000 IDC
survey</a> of 1999 licenses,
24% of all servers (counting both Internet and intranet servers)
installed in 1999 ran GNU/Linux.
Windows NT came in first with 36%;
all Unixes combined totaled 15%.
Again,
since some of the Unixes are OSS/FS systems (e.g., FreeBSD, OpenBSD,
and NetBSD), the number of OSS/FS systems is actually larger than the
GNU/Linux figures.
Note that it all depends on what you want to count;
39% of all servers installed from this survey
were Unix-like (that's 24%+15%), so
``Unix-like'' servers were actually #1 in installed market share
once you count GNU/Linux and Unix together.
<p>
IDC released a similar study on January 17, 2001 titled
<a href="http://www.computer.org/computer/homepage/june/ind_trends/index.htm">
``Server Operating Environments: 2000 Year in Review''</a>.
<!-- Neal Leavitt, "Linux: At a Turning Point?", IEEE Computer. -->
On the server, Windows accounted for 41% of new server operating system sales
in 2000, growing by 20% - but GNU/Linux accounted for 27%
and grew even faster, by 24%.
Other major Unixes had 13%.
<!-- Linux figure from "Why Microsoft Is Wary of Open Source" CNet (06/18/01);
      Wilcox, Joe; Shankland, Stephen -->
<!-- The rest of the figures are from:
 http://www.nwfusion.com/news/2001/0228growth.html John Fontana's
 article quotes interesting information from it.
 I don't have the original study it's quite expensive.

 According to InfoWorld, November 5, 2001, "The Penguin Swoops into the
 OS fray" by P.J. Connolly, IDC reported in September 2001 that the
 market shares for servers in 2000 were: Linux 27%, Windows 41%,
 NetWare 13.8%, major Unix 13.9%, Other 4.3% (includes BSD).
 Note the difference in Novell numbers; the January report claimed
 (to 17%); this isn't important to the discussion.
 The other numbers look like trivial corrections, and since I can't
 reference the September report, I'm using the numbers that everyone
 can obtain (they're almost the same anyway).
-->
<!-- An interview with IDC is at:
     http://slashdot.org/interviews/01/06/21/154203.shtml
    Note in particular that IDC does NOT do sponsored market surveys,
    which is ethically far cleaner than Gartner doing a market survey
    sponsored by Microsoft.  IDC, at least, doesn't have a conflict
    of interest.
-->
</p><p>
Data such as these (and the TCO data shown later)
have inspired statements such as
this one from IT-Director on November 12, 2001:
<a href="http://www.it-director.com/article.php?id=2332">
``Linux on the desktop is still too early to call, but
on the server it now looks to be unstoppable.''</a>
<!-- 
"Linux: The Penguin Marches On" Monday 12th November 2001.
-->
</p><p>
These measures do <i>not</i> measure all server systems installed that year;
some Windows systems are not paid for (they're illegally pirated), and
OSS/FS operating systems such as GNU/Linux and the BSDs
are often downloaded and installed on multiple systems
(since it's legal and free to do so).

</p><p>
</p></li><li>
<b>An Evans Data survey published in November 2001
found that 48.1% of international developers and 39.6% of North Americans
plan to target most of their applications to GNU/Linux.</b>
The
<a href="http://www.businesswire.com/cgi-bin/f_headline.cgi?bw.111301/213170209">
November 2001 edition of the
<i>Evans Data International Developer Survey Series</i></a>
reported on in-depth interviews with more than 400 developers
representing over 70 countries,
and found that when asked
which operating system they plan to target with
most of their applications next year,
48.1% of international developers and 39.6% of North Americans
stated that they plan to target most of their applications to GNU/Linux.
This is particularly surprising since
only a year earlier less than a third of
the international development community was writing GNU/Linux applications.
The survey also found that
37.8% of the international development community
and 33.7% of North American developers
have already written applications for GNU/Linux, and that
more than half of those surveyed have enough confidence in GNU/Linux
to use it for mission-critical applications.


<!-- No longer working: http://news.cnet.com/news/0-1003-200-1549312.html -->
<p>
</p></li><li>
<b>Microsoft sponsored its own research to ``prove'' that
GNU/Linux isn't as widely used, but this research has been shown to
be seriously flawed.</b>
Microsoft sponsored a
<a href="http://www.zdnet.com/zdnn/stories/news/0,4586,2772060,00.html">
Gartner Dataquest report</a> claiming only 8.6% of servers shipped in
the U.S. during the third quarter of 2000 were Linux-based.
However, it's worth noting that Microsoft (as the research sponsor)
has every incentive to create low numbers, and these numbers are quite
different from IDC's research in the same subject.
IDC's Kusnetzky
commented that the likely explanation is that Gartner used a very
narrow definition of "shipped"; he thought the number was
"quite reasonable" if it only surveyed new servers with Linux
,
"But our research is that this is not how most users get their Linux.
We found that just 10 to 15 percent of Linux adoption comes from
pre-installed machines... for every paid copy
of Linux, there is a free copy that can be replicated 15 times."
Note that it's quite difficult to buy a new x86 computer without a
Microsoft operating system (Microsoft's contracts with computer makers
ensure this), but that doesn't mean that these operating systems are used.
Gartner claimed that it used interviews to counter this problem, but
its final research results (when compared to known facts) suggest that
Gartner did not really counter this effect.
For example, Gartner states that Linux shipments in the supercomputer field
were 'zero'.
In fact, Linux is widely used on commodity parallel clusters
at many scientific sites, including a number of high-profile sites.
Many of these systems were assembled in-house, showing that
Gartner's method of defining a ``shipment'' does not appear to
correlate to working installations.
The Register's article,
<a href="http://www.theregister.co.uk/content/4/19662.html">``No one's
using Linux''</a>
(with its companion article
<a href="http://www.theregister.co.uk/content/4/19661.html">``90% Windows..''</a>)
discusses this further.
In short, Microsoft-sponsored research reported low numbers, but
these numbers are quite suspect.
<p>
</p></li><li>
<b>GNU/Linux had 80% as many client shipments in 1999 as Apple's MacOS.</b>
According to <a href="http://www.idc.com/itforecaster/itf20000808.stm">the
June 2000 IDC survey</a> of 1999 licenses (5.0% for Mac OS, 4.1% for GNU/Linux),
there were almost as many client shipments of GNU/Linux as there were of MacOS -
and no one doubts that MacOS is a client system.
<p>
Currently,
GNU/Linux is a relatively new contender in the client
operating system (OS) market, and has relatively little market penetration.
This should not be surprising;
unlike the wealth of server and developer applications,
GNU/Linux has relatively few OSS/FS client applications and many of those
client applications were immature.
There are commercial client applications (e.g. Corel's Word Perfect), but
they aren't OSS/FS and are available on other platforms as well.
Without strong OSS/FS client applications (in particular an office suite),
GNU/Linux has no strong advantage over other client platforms.
After all, GNU/Linux combined with a proprietary office suite
still lacks the freedoms and low cost of purely OSS/FS
systems, and it has to compete with established proprietary systems which have
more applications available to them.
This doesn't mean that GNU/Linux can't support proprietary programs, but
if the office suite is proprietary, I believe few people will find that
a GNU/Linux based desktop is significantly better that Microsoft's.
Microsoft Office is essentially a monopoly controlling the office suite arena,
and it isn't available on GNU/Linux (the second-largest player in
office suites is Corel; since Corel is
partly owned by Microsoft, Corel cannot really be considered a competitor
to Microsoft).
Various OSS/FS office application programs are only now maturing, such as
StarOffice / <a href="http://www.openoffice.org/">OpenOffice</a>),
and alternatives include AbiWord, Gnumeric, and the KOffice suite.
In addition,
the cost of switching all those desktops in the face of compatibility
issues with an entrenched monopoly makes it difficult to use
anything other than Windows and Office on clients.
</p><p>
Nevertheless, in spite of weaker OSS/FS client applications in
several key areas at the time,
<a href="http://news.cnet.com/news/0-1003-200-1546430.html">an IDC study</a>
determined that GNU/Linux captured 4% of the market in 1999.
More generally, IDC
determined that there were 98.6 million shipments of client operating
systems in 1999, of which
Windows 3.x, 95 and 98 accounted for 66%,
Windows NT had 21%,
MacOS had 5%, and
GNU/Linux 4%.
</p><p>
There are some interesting hints that this may change in the future.
Some organizations, such as
<a href="http://desktoplinux.com/articles/AT9664091996.html">TrustCommerce</a>
and the
<a href="http://www.consultingtimes.com/Largo.html">city of Largo, Florida</a>,
report that they've successfully transitioned to using Linux on the desktop.
Back in 1997 I predicted that GNU/Linux would be ready for the desktop
in 2002-2003; at the time it needed more a more mature graphical desktop
and the key GUI-based user applications (in particular a
web browser, email reader, word processor,
spreadsheet, and presentation program).
Obviously, there is no real ``competition'' until the applications are
ready for use by ordinary users.
I think my prediction is on target; OSS/FS client applications
are finally maturing to the point where they are competitive at
the desktop.
Mozilla is about to release their version 1.0 product; it includes
a web browser, email reader, and other tools.
Evolution is also an excellent mail reader.
Open Office (an office suite) is actually quite capable now; running it
currently requires the use of a few non-OSS/FS components, but those
components are free, and work is ongoing to remove this dependency.
Abiword (word processor) has released its version 1.0 product in 2002;
it lacks support for tables, but that is likely to come in the near future.
Gnumeric (spreadsheet) is extremely capable; its interaction with
graphics is weak currently, but that is likely to be improved quickly.
The KOffice (office suite) has a lot of functionality, and has finally begun
to significantly support interoperability with Microsoft's formats
(an important capability that KOffice's competitors
have historically been better at).
In short, it looks like there are now several OSS/FS products that are
beginning to rival their proprietary competitors in both useability and in the
functionality that people need.
And the underlying graphical desktops, GNOME and KDE,
have already matured to be sufficiently usable for most users.
</p><p .="" there's="" already="" some="" evidence="" that="" others="" anticipate="" thi=""><a href="http://www.silicon.com/bin/bladerunner?30REQEVENT=&amp;REQAUTH=21046&amp;14001REQSUB=REQINT1=45449">Richard Thwaite, director of IT for Ford Europe,
stated in 2001 that an open source
desktop is their goal, and that they expect the industry to
eventually go there</a>
(he controls 33,000 desktops, so this would not be a trivial move).
It could be argued that this is just a ploy for negotiation with
Microsoft - but such ploys only work if they're credible.
<a href="http://desktoplinux.com/">Desktoplinux.com</a>
is a web site devoted to the use of GNU/Linux on the desktop; they
state that ``We believe Linux is ready <i>now</i>
for widespread use as a desktop operating system,
and we have created this website to help spread the word
and accelerate the transition to a more open desktop,
one that offers greater freedom and choice
for both personal and business users.''

</p><p>
</p></li><li>
<b>Businesses plan to increase their use of GNU/Linux</b>.
A Zona Research study
found that more than half of the large enterprise respondents expected
increases of up to 25% in the number of GNU/Linux users in their firm, while
nearly 20% expected increases of more than 50%.
In small companies, more than one third felt that GNU/Linux usage would
expand by 50%.
The most important factors identified that drove these
decisions were reliability, lower price,
speed of applications, and scaleability.
Here are the numbers:
<center>
<table border="1" cellpadding="2" summary="GNU/Linux is Number 1">
<tbody><tr bgcolor="#bac0ff"><th>Expected GNU/Linux Use</th><th>Small Business</th><th>Midsize Business</th><th>Large Business</th><th>Total</th></tr>
<tr bgcolor="#ccccfe"><th>50% increase</th><td>21.0%</td><td>16%</td><td>19.0%</td><td>19%</td></tr>
<tr bgcolor="#ccccfe"><th>10-25% increase</th><td>30.5%</td><td>42%</td><td>56.5%</td><td>44%</td></tr>
<tr bgcolor="#ccccfe"><th>No growth</th><td>45.5%</td><td>42%</td><td>24.5%</td><td>36%</td></tr>
<tr bgcolor="#ccccfe"><th>Reduction</th><td>3.0%</td><td>0%</td><td>0%</td><td>1%</td></tr>
<!--
(tr)(th)Sample Size(br)(# businesses)(/th)(td)33(/td)(td)31(/td)(td)44(/td)(td)108(/td)(/tr)
-->
</tbody></table>
</center>
You can see more about this study in
<a href="http://www.zdnet.com/eweek/stories/general/0,11011,2651826,00.html">``The New Religion: Linux and Open Source''</a> (ZDNet)
and in InfoWorld's February 5, 2001 article 
``Linux lights up enterprise: But concerns loom about OS vendor profitability.''
<!-- "Linux lights up enterprise: But concerns loom about OS vendor
profitability."  by Ed Scannell and Dan Neel.
InfoWorld, February 5, 2001. Page 8. Published by IDG, San Mateo, CA. -->
<p>
</p></li><li>
<b>The global top 1000 Internet Service Providers expect GNU/Linux use
to increase by 154%, according to Idaya's survey conducted
January through March 2001.</b>
<!-- Expired from http://biz.yahoo.com/prnews/010403/lntu009.html -->
A
<a href="http://www.idaya.co.uk/news/newsdesk/press_releases.phtml?">survey</a>
conducted by <a href="http://www.idaya.co.uk/">Idaya</a>
of the global top 1000 ISPs
found that they expected GNU/Linux to grow a further 154% in 2001.
Also, almost two thirds (64%) of ISPs consider the leading open source
software meets the standard required for enterprise level applications,
comparable with proprietary software.
Idaya produces OSS/FS software, so keep that in mind as a potential bias.
<p>
</p></li><li>
<b>A 2002 European survey found that 49% of CIOs in financial services,
retail, and the public sector expect to be using OSS/FS</b>.
OpenForum Europe published in February 2002 a survey titled
<a href="http://www.openforumeurope.org/research.php">
Market Opportunity Analysis For Open Source Software</a>.
Over three months CIOs and financial directors
in financial services, retail and public sector were interviewed for this
survey.
In this survey,
37% of the CIOs stated that they were already using OSS/FS,
and 49% expected to be using OSS/FS in the future.
It is quite likely that even more companies are using OSS/FS but
their CIOs are not aware of it.
Perceived benefits cited included decreased costs in general (54%),
lower software license cost (24%), better control over development (22%),
and improved security (22%).
<p>
</p></li><li>
<b>IBM found a 30% growth in the number of enterprise-level applications
for GNU/Linux in the six month period ending June 2001.</b>
At one time, it was common to claim that
``Not enough applications run under GNU/Linux''
for enterprise-level use.
However,
<a href="http://www.businesswire.com/cgi-bin/f_headline.cgi?bw.062701/211782585&amp;ticker=IBM">IBM found there are more than 2,300
GNU/Linux applications (an increase in 30% over 6 months)</a>
available from IBM and the industry's top
independent software vendors (ISVs).
<!-- "IBM And Business Partners Celebrate Milestone in Growth of Linux Applications; Linux Applications Increase More Than 30 Percent", June 27, 2001,
BusinessWire.com. -->
A
<a href="http://www.networkcomputing.com/1224/1224f1.html">Special report by Network Computing on Linux for the Enterprise</a>
discusses some of the strengths and weaknesses of GNU/Linux, and
found many positive things to say about GNU/Linux for enterprise-class
applications.
<p>
</p></li><li>
<b>A 2001 survey found that 46.6% of IT professionals were confident
that their organizations could support GNU/Linux, a figure larger
than any OS except Windows.</b>
A
<a href="http://www.zdnet.com.au/newstech/os/story/0,2000024997,20261699,00.htm">TechRepublic Research survey titled
<i>Benchmarks, Trends, and Forecasts: Linux Report</i></a>
found that ``support for Linux runs surprisingly deep''
when it surveyed IT professionals and asked them how confidently their
organizations could support various operating systems.
Given Windows' market dominance on the desktop, it's not surprising
that most were confident that their organizations could support various
versions of Windows
(for Windows NT the figure was 90.6%; for Windows 2000, 81.6%).
However, GNU/Linux came in third, at 46.4%; about half of
those surveyed responded that their organizations were
already confident in their ability to support GNU/Linux!
This is especially shocking because GNU/Linux beat other well-known
products with longer histories including Unix (42.1%),
Novell Netware (39.5%), Sun Solaris (25.7%), and Apple (13.6%).
TechRepublic suggested that there are several possible reasons for this
surprisingly large result:
<ul>
<li>GNU/Linux is considered to be a rising technology;
many IT professionals are already studying it
and learning how to use it, assuming that it will be
a marketable skill in the near future.
</li><li>Many IT professionals already use GNU/Linux at home,
giving GNU/Linux an entree into professional organizations.
</li><li>Since GNU/Linux is similar to Unix,
IT professionals who are proficient in Unix can easily pick up GNU/Linux.
</li></ul>
TechRepublic suggests that IT executives should inventory their staff's
skill sets, because they may discover that their organization can already
support GNU/Linux if they aren't currently using it.

<p>
</p></li><li>
<b>Sendmail, an OSS/FS program, is the leading email server.</b>
A <a href="http://cr.yp.to/surveys/smtpsoftware6.txt">survey
between 2001-09-27 and 2001-10-03
by D.J. Bernstein of one million random IP addresses</a>
successfully connected to 958 SMTP (email) servers
(such servers are also called mail transport agents, or MTAs).
Bernstein found
that Unix Sendmail had the largest market share (42% of
all email servers), followed by
Windows Microsoft Exchange (18%),
Unix qmail (17%), Windows Ipswitch IMail (6%),
Unix smap (2%), UNIX Postfix (formerly VMailer, 2%)
and Unix Exim (1%).
Note that Bernstein implements one of Sendmail's competitors (qmail),
so he has a disincentive to identify Sendmail's large market share.
Qmail is not OSS/FS, because
<a href="http://openacs.org/about/licensing/open-source-licensing">derivatives
of Qmail cannot be freely redistributed</a>; Qmail is ``source viewable,''
so some people are confused into believing that Qmail is OSS/FS.
However, Sendmail, Postfix, and Exim <i>are</i> all OSS/FS.
Indeed, not only is the leading program (Sendmail) OSS/FS, but that OSS/FS
program has more than twice the installations of its nearest competition.
<!--
 Qmail's redistribution license is given in:
       http://qmail.goof.com/top.html
 it's discussed in:
      http://openacs.org/about/licensing/open-source-licensing
 Qmail License noted:
             http://zgp.org/pipermail/linux-elitists/2000-March/000298.html
 Example of confusion:
             http://www.isp-planet.com/equipment/qmail-a.html
-->
<!-- Clearly sendmail is common, but there seems to be variance on how
     common it is, and the surveys available don't give enough data to
     be confident in their results (how was the survey conducted, etc.)?
     Lots of people say sendmail is "80%" or "75%",
     but without referencing someone
     who's actually done a survey (including giving a survey date).
     I suspect some of these figures were guessed.
  The Radicati Group performed a survey and found that in 1999,
  {a href="http://www8.techmall.com/techdocs/TS991203-4.html"}
  sendmail had 64% market share of messaging software for service providers{/a}.
  {a href="http://www.activemedia-guide.com/messoft_mrkt.htm"}Another
  survey of ISPs claims that sendmail had 75% of the market in 1999{/a},
  though the source of this data isn't clear.
  Other papers report 80% market shares
  http://release1.edventure.com/Issues/1198.pdf
  and/or that 90% of all email traffic passes through sendmail
  http://www.vita.com/vso/vso200005/MITRE/opensw.pdf

  It APPEARS that many of these are quoting old surveys done by Bernstein;
  see his web pages for more.  An old survey of his showed 80% of all
  SMTP servers running Sendmail, but that's an old survey, and I agree
  with Bernstein that it's unfair to quote REALLY old surveys when more
  recent data is available.
-->

<p>
</p></li><li>
<b>A survey in the second quarter of 2000 found that 95% of all
reverse-lookup domain name servers (DNS) used bind, an OSS/FS product.</b>
The Internet is built from many mostly-invisible infrastructure components.
This includes domain name servers (DNSs), which take human-readable machine
names (like ``yahoo.com'') and translate them into numeric addresses.
Publicly accessible machines also generally support ``reverse lookups'', which
convert the numbers back to names; for historical reasons, this is implemented
using the hidden ``in-addr.arpa'' domain.
By surveying the in-addr domain, you can gain insight into
how the entire Internet is supported.
<a href="http://www.isi.edu/%7Ebmanning/in-addr-versions.html">Bill Manning
has surveyed the in-addr domain</a> and found that
95% of all name servers (in 2q2000)
performing this important Internet infrastructure task are
some version of ``bind.''
Bind is an OSS/FS program.
</li></ol>

<h1><a name="reliability">Reliability</a></h1>
<p>
There are a lot of anecdotal stories that OSS/FS is more reliable,
but finally there is quantitative data confirming that mature OSS/FS
programs are more reliable:
</p><ol>
<li>
<b>Equivalent OSS/FS applications are more reliable, according to
a 1995 study.</b>
The 1995
<a href="http://www.cs.wisc.edu/%7Ebart/fuzz/fuzz.html">``Fuzz Revisited''</a>
paper measured reliability by feeding programs random characters and
determining which ones resisted crashing and freeze-ups.
Some researchers scoff at this measure, since this approach is unlikely to
find subtle failures, but the study authors note that their approach
still manages to find many errors in production software and is a
useful tool for finding software flaws.
<p>
OSS/FS had higher reliability by this measure.
It states in section 2.3.1 that:
</p><blockquote>
It is also interesting to compare results of testing the
commercial systems to the results from testing "freeware" GNU and Linux.
The seven commercial systems in the 1995 study have an
average failure rate of 23%,
while Linux has a failure rate of 9% and the GNU utilities have a failure
rate of only 6%.
It is reasonable to ask why a globally scattered group of programmers, with
no formal testing support or software engineering standards can
produce code that is more reliable
(at least, by our measure) than commercially produced code.
Even if you consider only the utilities
that were available from GNU or Linux,
the failure rates for these two systems are better than the other systems.
</blockquote>
<p>
There is evidence that Windows applications have similar reliability to
the proprietary Unix software 
(e.g., less reliable than the OSS/FS software).
A later paper,
<a href="http://www.cs.wisc.edu/%7Ebart/fuzz/fuzz.html">``An Empirical
Study of the Robustness of Windows NT Applications Using Random Testing''</a>,
found that with Windows NT GUI applications, they could crash 21% of the
applications they tested, hang an additional 24% of the applications,
and could crash or hang <i>all</i> the tested applications when subjecting
them to random Win32 messages.
Thus, there's no evidence that proprietary Windows software is more reliable
than OSS/FS by this measure.
Yes, Windows has progressed since that time - but so have the OSS/FS programs.
</p><p>
Although this experiment was done in 1995, nothing that's happened since
suggests that proprietary software has become much better than
OSS/FS programs since then.
Indeed, since 1995 there's been an increased interest and
participation in OSS/FS, resulting
in far more ``eyeballs'' examining and improving the reliability of OSS/FS
programs.
</p><p>
Now be careful: OSS/FS is not magic pixie dust; beta software of
any kind is still buggy!
However, the 1995 experiment measured mature OSS/FS to mature proprietary
software, and the OSS/FS software was more reliable under this measure.
</p><p>
</p></li><li>
<b>GNU/Linux is more reliable than Windows NT, according to a 10-month
ZDnet experiment.</b>
<a href="http://www.zdnet.com/sp/stories/issue/0,4537,2387282,00.html">ZDnet
ran a 10-month test for reliability</a> to compare
Caldera Systems OpenLinux, Red Hat Linux, and
Microsoft's Windows NT Server 4.0 with Service Pack 3.
All three used identical (single-CPU) hardware, and
network requests were sent to each server in parallel 
for standard Internet, file, and print services.
The result:
NT crashed an average of once every six weeks, each taking about 30
minutes to fix; that's not bad, but
neither GNU/Linux server <i>ever</i> went down.
This ZDnet article also does a good job of identifying GNU/Linux weaknesses
(e.g., desktop applications and massive SMP).
Hopefully Windows has made improvements since this study - but
the OSS/FS have certainly made improvements as well.
<p>
</p></li><li>
<b>GNU/Linux is more reliable than Windows NT, according to a one-year
Bloor Research experiment.</b>
<a href="http://gnet.dhs.org/stories/bloor.php3">Bloor Research</a>
had both operating systems running on relatively old
Pentium machines. In the space of one year, GNU/Linux crashed once
because of a hardware fault (disk problems), which took 4 hours to
fix, giving it a measured availability of 99.95 percent. Windows NT
crashed 68 times, caused by hardware problems (disk), memory (26
times), file management (8 times), and a number of odd problems (33 times).
All this took 65 hours to fix, giving an availability of 99.26 percent.
It's intriguing that the only GNU/Linux problem and a number of the Windows
problems were hardware-related;
it could be argued that the Windows hardware was worse, or it could
be argued that GNU/Linux did a better job of avoiding and
containing hardware failures.
In any case, file management failure can be blamed on Windows,
and the ``odd problems'' appear to be caused by Windows as well,
indicating that GNU/Linux is far more reliable than Windows.
Gnet summarized this as saying ``the winner here is clearly Linux.''
<p>
</p></li><li>
<b>Sites using Microsoft's IIS web serving software have more than double
the time offline (on average) than sites using the Apache software,
according to a 3-month Swiss evaluation.</b>
These are the results of
<a href="http://www.syscontrol.ch/e/news/Serversoftware.html">Syscontrol AG's
analysis of website uptime (announced February 7, 2000)</a>
They measured over 100 popular Swiss web sites over a three-month period,
checking from 4 different locations every 5 minutes
(it'd be interesting to see what a larger sample would find!).
You can
<a href="http://www.syscontrol.ch/e/SWePIX/SWePIXe.html">see their report
(in German)</a>, or a
<a href="http://babelfish.altavista.com/translate.dyn?doit=done&amp;lp=de_en&amp;bbltype=urltext&amp;url=http://www.syscontrol.ch/e/SWePIX/SWePIXe.html">Babelfish
(machine) translation of the report</a>.
<!-- 
Now, reliability depends on an entire system, not just the web server
program, but it's worth noting that Microsoft IIS only works on Windows
(so you know what you have!).
In contrast, while Apache can run on Windows,
until recently this was experimental and thus Apache is far more likely to
be running on a Unix-like box.
Besides, if you buy the necessary expensive licenses from Microsoft to
run a commercial web site, IIS is ``free''; this suggests that
an Apache site is extremely likely to be a Unix-like site.
-->
<!-- the German is ``Durchschnittliche Ausfalldauer in Std. pro Servertyp'' -->
<!-- I downloaded this 15-Sep-2000; they may have new numbers now -->
Here's their entire set of published
data on ``average down-time per hour per type of server'',
plus a 3-month average that I've computed:
<!-- Andere means "Other" in German -->

<center>
<table border="1" cellpadding="2" summary="GNU/Linux is Number 1">
<tbody><tr bgcolor="#bac0ff">
<td><b>Downtime</b></td>

<td><div align="right"><b>Apache</b></div></td>
<td><div align="right"><b>Microsoft</b></div></td>
<td><div align="right"><b>Netscape</b></div></td>
<td><div align="right"><b>Other</b></div></td>
</tr>

<tr bgcolor="#ccccfe">
<td><b>September</b></td>
<td><div align="right">5.21</div></td>
<td><div align="right">10.41</div></td>
<td><div align="right">3.85</div></td>
<td><div align="right">8.72</div></td>
</tr>

<tr bgcolor="#ccccfe">
<td><b>October</b></td>
<td><div align="right">2.66</div></td>
<td><div align="right">8.39</div></td>
<td><div align="right">2.80</div></td>
<td><div align="right">12.05</div></td>
</tr>

<tr bgcolor="#ccccfe">
<td><b>November</b></td>
<td><div align="right">1.83</div></td>
<td><div align="right">14.28</div></td>
<td><div align="right">3.39</div></td>
<td><div align="right">6.85</div></td>
</tr>

<tr bgcolor="#ccccfe">
<td><b>Average</b></td>
<td><div align="right">3.23</div></td>
<td><div align="right">11.03</div></td>
<td><div align="right">3.35</div></td>
<td><div align="right">9.21</div></td>
</tr>
</tbody></table>
</center>
<p>
It's hard not to notice that Apache (the OSS web server)
had the best results over the three-month average (and with better results
over time, too).
Indeed, Apache's worst month was better than Microsoft's best month.
I believe the difference between Netscape and Apache is statistically
insignificant - but this still shows that the freely-available
OSS/FS solution (Apache)
has a reliability at least as good as the most reliable proprietary solution.
The report does note that this might not be solely the fault of the software's
quality, since there were several Microsoft IIS sites that had short
interruptions at the same time each day (suggesting regular restarts).
However, this still begs the question -- why did the IIS sites require
so many more regular restarts than the Apache sites?
Every outage, even if preplanned, results in a service loss
(and for e-commerce sites, a potential loss of sales).

</p><p>
</p></li><li>
<b>According to a separate uptime study by Netcraft, OSS/FS does very well;
as of August 3, 2001, of the 50 sites with the highest uptimes,
92% use Apache and 50% run on OSS/FS operating systems.</b>
Netcraft keeps a track of the 50 often-requested
sites with the longest uptimes at
<a href="http://uptime.netcraft.com/">http://uptime.netcraft.com</a>.
Looking at
<a href="http://www.dwheeler.com/frozen/top.avg.2001aug3.html">the August 3, 2001 uptime report</a>,
I found that 92% (46/50) of the sites use Apache;
one site's web server was unknown, and three others were not Apache.
Of those three, only one reported to be Microsoft IIS,
and that one instance is suspicious because
its reported operating system is BSD/OS
(this apparent inconsistency can be explained in many ways, e.g.,
perhaps there is a front-end BSD/OS system that ``masks'' the IIS web site,
or perhaps the web server is lying about its type to confuse attackers).
In this snapshot, 50% (25/50) ran on an open source operating system,
and only Unix-like operating systems had these large uptimes
(no Windows systems were reported as having the best uptimes).
<!--
Richard Wendland, richard, at codeburst  ,co  ,uk, gave me
more information about Netcraft's approach (he wrote the code):

... our remote uptime
detection from any ordinary TCP connection doesn't have access to the
proper kernel counter.  We "decode" the 32-bit RFC1323 TCP timestamp
option on OSs where this is implemented uptime related, so we have a
worse wrap-around problem.

...

Do you mean that by
periodic checks we could tell if a site has "wrapped" the counter,
distinct from a reboot?  Hence where we detect a proper "wrap" we could
carry on incrementing our record of uptime.  Ie maintain and use per-site
state, rather than using the pure function

	(TCP timestamp option value, OS) => uptime

we currently use[?].

We have discussed the possibility of doing this, but haven't.
Four reasons:

1) additional software complexity;

2) where load balancers are used (multiple computers per IP) it's
   difficult/impossible to monitor the same computer through a wrap,
   because we don't know which computer we connect to each time;

3) when we first start monitoring a new site we don't know how
   many "wraps" have previously taken place, so couldn't always
   be accurate;

4) because we add/drop sites due to user request load, this complicates
   matters somewhat - we might not have monitored enough around the wrap
   point when it's subsequently added again.
-->
<p>
As with all surveys, this one has weaknesses, as discussed in
<a href="http://uptime.netcraft.com/up/accuracy.html">
Netcraft's Uptime FAQ</a>.
Their techniques for identifying web server and operating systems can be
fooled.
Only systems for which Netcraft was sent many requests were included
in the survey (so it's not ``every site in the world'').
Any site that is requested through the ``what's that site running''
query form at Netcraft.com is added to the set of sites
that are routinely sampled; Netcraft doesn't routinely monitor all
22 million sites it knows of for performance reasons.
Many operating systems don't provide uptime information and thus
can't be included; this includes
AIX, AS/400, Compaq Tru64, DG/UX, MacOS, NetWare, NT3/Windows 95,
NT4/Windows 98, OS/2, OS/390, SCO UNIX, Sony NEWS-OS, SunOS 4, and VM.
Thus, this uptime counter can only include systems running on
BSD/OS, FreeBSD (but not the default configuration in versions 3 and later),
recent versions of HP-UX, IRIX,
GNU/Linux 2.1 kernel and later (except on Alpha processor based systems),
MacOS X, recent versions of NetBSD/OpenBSD, Solaris 2.6 and later, and
Windows 2000.
Note that Windows NT systems cannot be included
in this survey (because their uptimes couldn't be counted).
Windows 2000 systems's data are included in the source source for this survey,
but they have a different problem.
Windows 2000 had little hope to be included in the August 2001
list, because the 50th system in the list had an uptime of 661 days,
and Windows 2000 had only been launched about 17 months (about 510 days)
earlier.
Note that HP-UX, GNU/Linux (usually), Solaris
and recent releases of FreeBSD cycle back to zero after 497 days,
exactly as if the machine had been rebooted at that precise point.
Thus it is not possible to see an HP-UX, GNU/Linux (usually),
or Solaris system
with an uptime measurement above 497 days, and in fact their uptimes
can be misleading (they may be up for a long time, yet not show it).
There is yet one other weakness: if a computer switches operating
systems later, the long uptime is credited to the new operating system.
Still, this survey does compare Windows 2000, GNU/Linux
(up to 497 days usually),
FreeBSD, and several other operating systems,
and OSS/FS does quite well.
<!--
 April 4, 2002:

 Even now Windows 2000 has no hope, as 50th now has an uptime of 760 days,
 and it's about 2 years (~730 days) since Windows 2000 launch.
-->
</p><p>
It could be argued that perhaps systems on the Internet
that haven't been rebooted for such a long time might be insignificant,
half-forgotten, systems.
For example, it's possible that security patches aren't being
regularly applied, so such long uptimes are not necessarily good things.
However, a counter-argument is that Unix and Linux systems don't need to
be rebooted as often for a security update, and this is a valuable
attribute for a system to have.
Even if you accepted that unproven
claim, it's certainly true that there are half-forgotten Windows systems, too,
and they didn't do so well.
Also, only systems someone specifically asked for information about were
included in the uptime survey, which would limit the number of
insignificant or half-forgotten systems.
</p><p>
At the very least, Unix and Linux are able to quantitatively demonstrate
longer uptimes than their Windows competitors can, so Unix and Linux have
significantly better evidence of their reliability than Windows.
</p></li></ol>
<p>
Of course, there are many anecdotes about Windows reliability vs. Unix.
For example, the
<a href="http://www.info-sec.com/OSsec/OSsec_080498g_j.shtml">
Navy's ``Smart Ship'' program caused a complete failure of the
entire USS Yorktown ship in September 1997</a>.
Anthony DiGiorgio (a whistle-blower) stated that Windows is
``the source of the Yorktown's computer problems.''
Ron Redman, deputy technical director of the Fleet Introduction Division
of the Aegis Program Executive Office, said
``there have been numerous software failures associated with [Windows]
NT aboard the Yorktown.''
Redman also said
``Because of politics, some things are being forced on us that
without political pressure we might not do, like Windows NT...
If it were up to me I probably would not have used Windows NT in this
particular application.
If we used Unix, we would have a system that has less
of a tendency to go down.''
</p><p>
One problem with reliability measures is that it takes a long time to
gather data on reliability in real-life circumstances.
Thus, there's more data comparing older Windows editions to older GNU/Linux
editions.
The key is that these tests contemporary versions of both
OSS/FS and proprietary systems; both have moved forward since, but
it's a fair test.
Nevertheless, the available evidence suggests that OSS/FS has
a significant edge in reliability.




</p><h1><a name="performance">Performance</a></h1>
<p>
Comparing GNU/Linux and Microsoft Windows performance on equivalent
hardware has a
history of contentious claims and different results based on
different assumptions.
I think that OSS/FS has at least shown that it's often competitive, and
in many circumstances it beats the competition.
</p><p>
Performance benchmarks are very sensitive to the assumptions and environment,
so the best benchmark is one you set up yourself to model your intended
environment.
Failing that, you should use unbiased measures, because it's so easy to
create biased measures.
</p><p>
First, here are a few recent studies
suggesting that some OSS/FS systems beat their proprietary competition
in at least some circumstances:

</p><ol>
<li>
<b>PC Magazine's November 2001 performance tests for file servers found
that Linux with Samba significantly outperformed Windows 2000.</b>
PC Magazine's article
<a href="http://www.pcmag.com/article/0,2997,s%253D25068%2526a%253D16554,00.asp">Performance Tests: File Server Throughput and Response Times</a>
<!-- Nov. 13, 2001 -->
found that Linux with Samba significantly outperformed Windows 2000 Server
when used as a file server for Microsoft's own network file protocols.
This was true regardless of the number of
simultaneous clients (they tested a range up to 30 clients), and it was true
on the entire range on computers they used
(Pentium II/233MHz with 128MiB RAM, Pentium III/550MHz with 256MiB RAM, and
Pentium III/1GHz with 512MiB RAM, where
<a href="http://physics.nist.gov/cuu/Units/binary.html">MiB is 2^20 bytes</a>).
Indeed, as the machines became more capable the absolute difference became
more pronounced.
On the fastest hardware while handling
largest number of clients, GNU/Linux's throughput was about
130 MB/sec vs. Windows' 78 MB/sec (GNU/Linux was 78% faster).

<p>
</p></li><li>
<b>PC Magazine file server performance tests again in April 2001;
again Linux with Samba beat Windows 2000, only now Samba surpasses
Windows 2000 by about 100% and can handle 4 times as many clients.</b>
PC Magazine published another comparison of Samba and Windows; a
summary is available electronically as
<a href="http://www.vnunet.com/News/1131114">``Samba runs rings around Win2000.''</a>
<!--
"Samba runs rings around Win2000"
By Roger Howorth and Alan Stevens [22-04-2002]
The Samba open source file and print server outperforms Windows 2000 by a wide margin
-->
They noted that the latest Samba software now surpasses
the performance of Windows 2000 by about 100 percent under benchmark tests,
and found that Linux and Samba can handle four times as many client systems
as Windows 2000 before performance begins to drop off.
Jay White, IT manager at electronics firm BF Group, said that
Samba is one of the most useful pieces of server software
available for a mixed Windows and Linux environment.
"Our Samba server has been online for 394 days so far.
The total cost is the hardware plus 30 minutes of my time each year," he said.
Mark Twells, IT coordinator at a large education facility, said,
"We run six Samba servers on a variety of hardware [and] we have
around 1,000 users.

<p>
</p></li><li>
<b>In performance tests by Sys Admin magazine, GNU/Linux beat
Solaris (on Intel), Windows 2000, and FreeBSD.</b>
The article
<a href="http://www.sysadminmag.com/articles/2001/0107/0107a/0107a.htm">
``Which OS is Fastest for High-Performance Network Applications?''</a>
in the July 2001 edition of
<a href="http://www.sysadminmag.com/"><i>Sys Admin</i></a> magazine
examined high-performance architectures and found that GNU/Linux
beat its competition when compared with Solaris (on Intel), FreeBSD
(an OSS/FS system), and Windows 2000.
They intentionally ran the systems ``out of the box'' (untuned),
except for increasing the number of simultaneous TCP/IP connections
(which is necessary for testing multi-threaded and asynchronous applications).
They used the latest versions of operating systems and the exact same machine.
They reported (by operating system) the results of two different
performance tests.
<p>
The FreeBSD developers complained about these tests, noting
that FreeBSD by default emphasizes reliability (not speed) and that
they expected anyone with a significant performance need would do some
tuning first.
Thus,
<a href="http://www.sysadminmag.com/articles/2001/0108/0108q/0108q.htm">
Sys Admin's re-did the tests for FreeBSD after tuning FreeBSD</a>.
One change they made was
switching to ``asynchronous'' mounting, which makes a system faster
(though it increases the risk of data loss in a power failure) -
this is the GNU/Linux default
and easy to change in FreeBSD, so this was a very small and
reasonable modification.
However, they also made many other changes, for example,
they found and compiled in 17 FreeBSD kernel patches and used various
tuning commands.
The other operating systems weren't given the chance to ``tune'' like this,
so comparing untuned operating systems to a tuned FreeBSD isn't really fair.
</p><p>
In any case, here are their two performance tests:
</p><ol>
<li>
Their ``real-world'' test
measured how quickly large quantities of email
could be sent using their email delivery server (MailEngine).
Up to 100 simultaneous sends there was no difference, but as the number
increased the systems began showing significant differences in their
hourly email delivery speed.
By 500 simultaneous sends GNU/Linux was clearly faster than all except
FreeBSD-tuned, and GNU/Linux remained at the top.
FreeBSD-tuned had similar performance to GNU/Linux
when running 1000 or less simultaneous sends, but FreeBSD-tuned peaked around
1000-1500 simultaneous connections with a steady decline not suffered
by GNU/Linux, and FreeBSD-tuned
had trouble going beyond 3000 simultaneous connections.
By 1500 simultaneous sends, GNU/Linux was sending 1.3 million emails/hour,
while Solaris managed approximately 1 million,  and Windows 2000 and
FreeBSD-untuned were around 0.9 million.
</li><li>
Their ``disk I/O test'' created, wrote, and read back
10,000 identically-sized files in a single directory,
varying the size of the file instances.
Here Solaris was the slowest, with FreeBSD-untuned the second-slowest.
FreeBSD-tuned, Windows 2000, and GNU/Linux
had similar speeds at the smaller file sizes
(in some cases FreeBSD-tuned was faster, e.g., 8k and 16k file size),
but when the file sizes got to 64k to 128k the operating systems began
to show significant performance differences; GNU/Linux was the fastest,
then Windows 2000, then FreeBSD.
At 128k, FreeBSD was 16% worse than Windows 2000, and 39% worse than GNU/Linux;
all were faster than FreeBSD-untuned and Solaris.
When totaling these times across file sizes, the results were
GNU/Linux: 542 seconds, Windows 2000: 613 seconds, FreeBSD-tuned: 630 seconds,
FreeBSD-untuned: 2398 seconds, and Solaris: 3990 seconds.
</li></ol>
<p>
</p></li><li>
<b>GNU/Linux with TUX has produced better SPEC values than Windows/IIS in
several cases, even when given inferior drive configurations.</b>
One organization that tries to develop unbiased benchmarks
is the <a href="http://www.spec.org/">SPEC Consortium</a>,
which develops and maintains a whole series of benchmarks.
We can compare Microsoft Windows versus GNU/Linux
by comparing SPECweb99 results (which measure web server performance)
on identical hardware if both have undergone the same amount of performance
optimization effort.
Alas, things are not so simple; rarely are the same basic hardware platforms
tested with both operating systems, and even when that occurs, as of
July 13, 2001 no exactly identical configurations have been tested
(they differ in ways such as using a different number of hard drives, or
including some faster hard drives).
Using all results available by July 13, 2001,
there were three hardware configurations, all from Dell,
which ran both GNU/Linux (using the TUX web server/accelerator)
and Windows (using IIS) on exactly the same underlying hardware.
Here are the SPECweb99 results as of July 13, 2001 (larger is better),
noting configuration differences:
<center>
<table border="1" cellpadding="2" summary="GNU/Linux is Number 1">
<tbody><tr bgcolor="#bac0ff"><th>System</th><th>Windows SPEC Result</th><th>Linux SPEC Result</th></tr>
<tr bgcolor="#ccccfe"><td>Dell PowerEdge 4400/800, 2 800MHz Pentium III Xeon</td><td>1060 (IIS 5.0, 1 network controller)</td><td>2200 (TUX 1.0, 2 network controllers)</td></tr>
<tr bgcolor="#ccccfe"><td>Dell PowerEdge 6400/700, 4 700MHz Pentium III Xeon</td><td>1598 (IIS 5.0, 7 9GB 10KRPM drives)</td><td>4200 (TUX 1.0, 5 9GB 10KRPM drives)</td></tr>
<tr bgcolor="#ccccfe"><td>Dell PowerEdge 8450/700, 8 700MHz Pentium III Xeon</td>
<td>7300/NC (IIS 5.0, 1 9Gb 10KRPM and 8 16Gb 15KRPM drives) then 8001
(IIS 5.0, 7 9Gb 10KRPM and 1 18Gb 15KRPM drive)</td>
<td>7500 (TUX 2.0, 5 9Gb 10KRPM drives)</td></tr>
</tbody></table>
</center>
<!-- on the 8450/700, TUX 1.0 had 6387; only the most recent figures
 (TUX 2.0) are shown, since there's no point in comparing obsolete systems
 when a newer system is available. -->
<!-- Dell PowerEdge 2400/667 are available, and the numbers for Linux/TUX
     are higher, but the processors used aren't identical;
     667MHz Pentium IIIEB and 667MHz Pentium III for IIS. Since they aren't
     identical processors, it's would not be fair to mention them.
     There are many other SPECweb results, but very few compare
     apples to apples. Even as of July 13, these are the only ones. -->
<p>
The first row (the PowerEdge 4400/800) doesn't really prove anything.
The IIS system has lower performance, but it only had one network
controller and the TUX system has two - so while the TUX system had better
performance, that could simply be because it had two network
connections it could use.
</p><p>
The second entry (the PowerEdge 6400/700) certainly suggests that
GNU/Linux plus TUX really is much better -
the IIS system had two more disk drives available to it (which should increase
performance), but the TUX system had more than
twice the IIS system's performance.
</p><p>
The last entry for the PowerEdge 8450/700 is even more complex.
First, the drives are different - the IIS systems had at least one drive
that revolved more quickly than the TUX systems
(which should give IIS higher performance overall, since the transfer
speed is almost certainly higher).
Also, there were more disk drives (which again should give IIS still higher
performance).
When I originally put this table together showing all data
publicly available in April 2001
(covering the third quarter of 1999 through the first quarter of 2001),
IIS 5.0 (on an 8-processor Dell PowerEdge 8450/700)
had a SPECweb99 value of 7300.
Since that time, Microsoft changed the availability of Microsoft SWC 3.0,
and by SPECweb99 rules, this means that those test results are
``not compliant'' (NC).
This is subtle; it's not that the test itself was invalid, it's that
Microsoft changed what was available and used the
SPEC Consortium's own rules to invalidate a test
(possibly because the test results were undesirable to Microsoft).
A retest then occurred, with yet another disk drive configuration,
at which point IIS produced a value of 8001.
However, both of these figures are on clearly better hardware - and
in one circumstance the better hardware didn't do better.
</p><p>
Thus, in these configurations the GNU/Linux plus TUX system was
given inferior hardware yet still sometimes won on performance.
Since other factors may be involved, it's hard to judge - there are
pathological situations where ``better hardware'' can have worse performance,
or there may be another factor not reported that had a more significant effect.
Hopefully in the future there will be many head-to-head tests
in a variety of identical configurations.
</p><p>
Note that
TUX is intended to be used as a ``web accelerator'' for many circumstances,
where it rapidly handles simple requests and then passes more complex
queries to another server (usually Apache).
I've quoted the TUX figures because they're the recent performance
figures I have available.
As of this time I have no SPECweb99 figures or other recent performance
measures for Apache on GNU/Linux,
or for Apache and TUX together; I also don't have TUX reliability figures.
I expect that such measures will appear in the future.
</p><p>
</p></li><li>
<b>Low-level benchmarks by IBM found that GNU/Linux had better
performance than Windows for pipes (an input/output mechanism),
and also process and thread creation.</b>
Ed Bradford (manager of Microsoft Premier Support for IBM Software group)
published in October 2001 the study
<a href="http://www-106.ibm.com/developerworks/linux/library/l-rt4/?open&amp;t=grl,l=252,p=pipes"><i>Pipes in Linux, Windows 2000, and Windows XP</i></a>.
In this study he examined the
the performance of pipes, a common low-level mechanism for
communicating between program processes.
He found the pipes in
Red Hat 7.1 (with Linux kernel version 2.4.2) had a peak I/O rate of
around 700 MB/sec, with a steady state at near 100 MB/sec for
very large block sizes.
In contrast,
Windows 2000 peaked at 500 MB/sec, with a large block steady state of
80 MB/sec.
Windows XP Professional (evaluation version) was especially disappointing;
its peak I/O rate was only 120 MB/sec, with a stead state of 80 MB/sec, all
on the same platform and all running a GUI.
<p>
In February 2002 he published
<a href="http://www-106.ibm.com/developerworks/linux/library/l-rt7/?Open&amp;t=grl,l=252,p=mgth">
<i>Managing processes and threads</i></a>, in which he compared
the performance of
Red Hat Linux 7.2, Windows 2000 Advanced Server ("Win2K"), and
Windows XP Professional ("WinXP"),
all on a Thinkpad 600X with 320MiB of memory.
Linux managed over to create 10,000 threads/second,
while Win2K didn't quite manage 5,000 threads/second 
and WinXP only created 6,000 threads/second.
In process creation, Linux managed 330 processes/second, while
Win2K managed less than 200 processes/second and WinXP less than 160
processes/second.
<!-- Ed Bradford also published a study of block memory motion, but he
     left a number of loose ends.  In that study, he found that generally
     Linux was faster, with the exception of
     (1) using a 4-byte pointer and transferring less than 200 KB, or
     (2) using partially unwound "double *" method the transfer is less
         than 10 KB.
     However, he concludes by saying that
     "It seems this time that I have created more questions than answers."
     In particular, little explanation for these differences is given.
     In the end, I've decided not to reference his work because there
     were just too many loose ends. The effect seems to be real, so others
     might want to reference it. -->

</p></li><li>
<b>Fastcenter ran Oracle performance tests
and found that, on the same hardware, Microsoft Windows 2000 Server was
capable of only 85% of the throughput of SuSE Linux Enterprise 7.</b>
<a href="http://www.fastcenter.com/">Fastcenter</a>
ran a benchmark of database performance tests using Oracle on exactly the
same hardware
(twin 1 GHz processors, 2 Gibibytes of ram, and two 10K RPM 160MB/sec disks).
They compared
Microsoft Windows 2000 Server (Version 5.0.2195, build 2195, no patches)
with SuSE Enterprise Linux Server 7 (standard installation).
Both operating systems were installed essentially as "stock" installations;
<a href="http://www.fastcenter.com/">see FastCenter's benchmarks
for more information</a>.
The Windows 2000 server instance build only achieved
70% of the performance of the Linux
(on Windows 2000 server this action took 5 minutes and 56 seconds, while
on SuSE Linux the same action took 4 minutes and 12 seconds).
Once installed, Microsoft's
Windows 2000 server averaged 85% of the throughput of SuSE Linux
(it varied depending on the load, but
in all cases SuSE Linux had better performance).
Fastcenter is a partner with SuSE Corporation - a Linux distributor -
so take that into account.
For more information, see Fastcenter's paper.
<!-- There is variance in comparing XP performance vs. 2000, and that's
    outside the scope of this paper.  Rather get into the details of an
    irrelevant issue, I've commented it out.
<p>
<li>
<b>InfoWorld found that Windows XP is significantly slower than
Windows 2000</b>.
InfoWorld's October 29, 2001 issue includes the article
``Waiting for Windows XP'' (pp. 53-57), where they benchmarked
Windows XP (final release code) against Windows 2000 (with Service Pack 2),
using CSA Research's Benchmark Studio Professional
using several different scenarios
(see their article for more details and quantitative figures).
They found that XP was always the poorer performer, and that its results
got worse as load increased.
For example, the number of seconds required for Windows 2000 vs. Windows XP
on a Pentium 4 (1.5 GHz) for different scenarios using the default UI
was as follows:
29.13s vs. 39.43s (baseline), 35.05s vs. 43.30s (scenario 1),
42.12s vs. 56.72s (scenario 2), and 53.60s vs. 81.07s (scenario 3).
Totaling each situation (as though someone had to perform each benchmark,
one after the other) totals to 159.9s vs. 220.52; thus Windows XP is
(220.52-159.9)/159.9 =
38% <i>slower</i> than Windows 2000.
In short, InfoWorld stated
``the results of our benchmark tests indicate that Windows XP is
significantly slower than Windows 2000, especially under heavy load.
Unless investing in new hardware for demanding users is an option,
companies should stick with Windows 2000.''
In short, Windows XP is out, but it hasn't reduced Linux's performance lead
in at least a number of circumstances.
-->
<!-- This ends the main list of performance measures: -->
</li></ol>
<p>

All operating systems in active development
are in a constant battle for performance improvements over their rivals.
The history of comparing Windows and GNU/Linux helps put this in perspective:
</p><ol>
<li>
<b>Ziff-Davis found that GNU/Linux with Apache beat
Windows NT 4.0 with IIS by 16%-50% depending on the GNU/Linux distribution.</b>
<a href="http://www.zdnet.com/sp/stories/issue/0,4537,2196115,00.html">Ziff-Davis compared Linux and Windows NT's performance at web serving</a>.
They found that
``Linux with Apache beats NT 4.0 with IIS,
hands down. SuSE, the least effective Linux, is 16%
faster than IIS, and Caldera, the leader, is 50% faster.''
<p>
</p></li><li>
<b><a href="http://www.mindcraft.com/whitepapers/nts4rhlinux.html">Mindcraft
released a report in April 1999</a> that claimed that
Microsoft Windows NT Server 4.0 is 2.5 times faster than Linux (kernel 2.2)
as a File Server and 3.7 times faster as a Web Server when running on
a 4-CPU SMP system</b>.
Several people and organizations, such
<a href="http://lwn.net/1999/features/MindCraft1.0.phtml">Linux
Weekly News (LWN)</a> and
<a href="http://www.kegel.com/mindcraft_redux.html">Dan Kegel</a>,
identified serious problems with this study.
An obvious issue was that NT was specially tuned by Microsoft's
NT experts, at Microsoft, while GNU/Linux was not tuned at all.
Another issue is that the price/performance wasn't considered
(nor was total expenditure kept constant - for the same amount of
money, the GNU/Linux system could have had better hardware).
Mindcraft claimed they asked for help, but they didn't use the documented
methods for getting help nor did they purchase a support contract.
Many were especially offended that even though this study was funded
by Microsoft (one of the contestants) and held at their facility,
neither Mindcraft's initial announcement nor its paper made
any mention of this conflict-of-interest - and
it could be easily claimed that their configuration
was designed to put GNU/Linux at a disadvantage.
Their configuration was somewhat bizarre - it assumed all web pages
were static
(typical big sites tend to use many dynamically generated pages)
and that there were
100 or so clients connected via 100baseT
(in 1999 a more typical situation would be that
most clients are using slower 28.8 or 56 Kbps modems).
<p>
Careful examination of the benchmark did find some legitimate Linux kernel
problems, however. These included a TCP bug,
the lack of ``wake one'' semantics, and SMP bottlenecks
(see <a href="http://www.kegel.com/mindcraft_redux.html">Dan Kegel's pages</a>
for more information).
The Linux kernel developers began working on the weaknesses identified
by the benchmark.
</p><p>
</p></li><li>
<b>PC Week confirmed that Windows did indeed do better in this
less probable configuration.</b>
In June 30, 1999, Mindcraft released their
<a href="http://www.mindcraft.com/whitepapers/openbench1.html">Open
Benchmark</a> in conjunction with PC Week.
While this didn't excuse Mindcraft's biases,
it did make a convincing case that there were legitimate problems
in the Linux kernel and Apache that made GNU/Linux a
poorer-performing product in this somewhat improbable configuration
(serving static web pages to clients with high-speed connections).
Note that this configuration was considerably different
than Ziff-Davis's, so the benchmarks don't necessarily conflict; it's merely
that different assumptions can produce different results
(as I've already stressed).
<p>
</p></li><li>
<b>The German magazine c't found that web sites with NT was better at
static content and dual network connections, but GNU/Linux was
better for sites with dynamic content and single connections.</b>
Their article <a href="http://www.heise.de/ct/english/99/13/186-1"><i>Mixed
Double: Linux and NT as Web Server on the Test Bed</i></a>
examined Windows NT with IIS against GNU/Linux (kernel 2.2.9)
with Apache on a machine with four Pentium II Xeon CPUs.
They found that the performance winner depended on the situation
(by now that should not be a surprise).
If the web server primarily served static web pages through
two high-performance network cards, NT's performance was better.
However, they also noted that in sophisticated web sites this
result didn't apply, because such sites tend to have primarily dynamic content,
and that few sites had this kind of dual-network connection
(when only one network board was available,
GNU/Linux generally had an edge).
They concluded that
``Mindcraft's result can't be transferred to situations with
mainly dynamic contents - the
common case in nearly every sophisticated web site...
In the web server areas most relevant for practical use,
Linux and Apache are already ahead by at least one nose.
If the pages don't come directly from the system's main memory,
the situation is even reverted to favour Linux and Apache:
Here, the OpenSource movement's prime products leave
their commercial competitors from Redmond way behind.''
See their paper for more figures and background.
<p>
</p></li><li>
<b>Network Computing found that GNU/Linux with Samba ran
at essentially the same speed as Windows for file serving.</b>
In their article
<a href="http://www.networkcomputing.com/1011/1011f1.html">
``Is it Time for Linux''</a>,
<!-- May 31, 1999 -->
Network Computing compared Red Hat Linux v5.2 running Samba 2.0.3
against Microsoft Windows NT Server Enterprise Edition on a
Pentium II-based HP NetServer LPr, stressing the machine
with multiple reads and writes of small, medium and large
files over the course of several hours.
<p>
For file serving, they discovered only
``negligible performance differences between the two for average workloads...
[and] depending on the degree of tuning performed on each
installation, either system could be made to surpass the other
slightly in terms of file-sharing performance.''
Red Hat Linux slightly outperformed NT on file writes, while NT edged
out Red Hat Linux on massive reads.
Note that their configuration was primarily network-limited;
they stated ``At no point were we able to push the CPUs much over 50-percent
utilization-the single NIC, full duplex 100BASE-T environment
wouldn't allow it.''
</p><p>
They also noted that ``examining the cost difference between
the two licenses brings this testing into an entirely new light...
the potential savings on licenses alone is eye-opening.
For example, based on the average street price of $30 for a Windows
NT client license, 100 licenses would cost around $3,000, plus
the cost of an NT server license (around $600). Compare this to
the price of a Red Hat Linux CD, or perhaps even a free
download, and the savings starts to approach the cost of a
low-end workgroup server. Scale that up to a few thousand
clients and you begin to see the savings skyrocket.''
See this paper's section on
<a href="#tco">total cost of ownership</a>.
</p><p>
</p></li><li>
<b>The Linux developers'
various efforts to improve performance appear to have paid off.</b>
In June 2000, Dell measured the various SPECweb99 values noted above.
</li></ol>
<p>
There are other benchmarks available, but I've discounted them on
various grounds:
</p><ol>
<li>
A more recent set of articles from eWeek on June 2001, shows some eye-popping
performance numbers for GNU/Linux with TUX.
However, although they compare it to Microsoft IIS, they don't include
Microsoft's SWC (Scaleable Web Cache), Microsoft's response to TUX -
and omitting it makes this comparison unfair in my opinion.
You can read more at
<a href="http://www.zdnet.com/enterprise/stories/main/0,10228,2776383,00.html">
``Tux: Built for Speed''</a>,
<a href="http://www.zdnet.com/enterprise/stories/main/0,10228,2776519,00.html">
``Smart Coding pays off Big''</a>, and
<a href="http://www.kegel.com/nt-linux-benchmarks.html">Kegel's
detailed remarks</a>.
<p>
</p></li><li>
The ZDNet article
<a href="http://www.zdnet.com/zdnn/stories/news/0,4586,2760874,00.html">
<i>Take that! Linux beats MS in benchmark test</i></a>,
loudly trumpeted that
GNU/Linux was the May 2001 performance leader in the
TPC-H decision support (database) benchmark (``100Gb'' category).
However, this result should not be taken very seriously;
the hardware that Linux ran on was more powerful than that
of the runner-up (Windows 2000).
Frankly, the more surprising fact than its top score (which can be easily
explained by the hardware) is its mere measurement at all with this benchmark -
traditionally only Microsoft's numbers are reported for this benchmark
at this range.
For more information, see
<a href="http://www.tpc.org/tpch/results/h-ttperf.idc">the TPC results.</a>
</li></ol>
<p>
More information on various benchmarks is available from Kegel's
<a href="http://www.kegel.com/nt-linux-benchmarks.html">NT vs. Linux
Server Benchmark Comparisons</a>, 
<a href="http://www.spec.org/">SPEC</a>, and the
<a href="http://dmoz.org/Computers/Performance_and_Capacity/Benchmarking/">
dmoz entry on benchmarking</a>.
</p><p>
Remember, in benchmarking everything depends on the
configuration and assumptions that you make.
Many systems are constrained by network bandwidth; in such circumstances
buying a faster computer won't help at all.
Even when network bandwidth isn't the limitation,
neither Windows nor GNU/Linux do well
in large-scale symmetric multiprocessing (SMP) configurations; if you
want 64-way CPUs with shared memory, neither are appropriate
(Sun Solaris, which is not OSS/FS, does much better in this configuration).
On the other hand, if you want massive distributed (non-shared) memory,
GNU/Linux does quite well, since you can buy more CPUs with a given amount
of money.
If massive distribution can't help you and you need very high performance,
Windows isn't even in the race;
today Windows 2000 only runs on Intel x86 compatible chips, while GNU/Linux runs
on much higher performance processors as well as the x86.

</p><h1><a name="scaleability">Scaleability</a></h1>

Which brings us to the topic of scaleability, a simple term
with multiple meanings:

<ol>
<li>
<b>GNU/Linux and NetBSD (both OSS/FS) support a wider
range of hardware platforms and performance
than any other operating system.</b>
Many people mean by ``scaleability'' to answer the question,
``can you use the same software system for both small and large projects?''
Often the implied issue is that you'd like to start with a modest system,
but have the ability to grow the system as needs demand without
expensive modifications.
Here OSS/FS is unbeatable; because many people can identify scaleability
problems, and because its source code can be optimized for its
platform, the scaleability of many OSS/FS products is amazing.
Let's specifically look at GNU/Linux.
GNU/Linux works on
<a href="http://www.linuxdevices.com/articles/AT8728350077.html">PDAs</a>
(including the
<a href="http://www.agendacomputing.com/">Agenda
VR3</a>),
<a href="http://www.timesofindia.com/300301/30intw1.htm">obsolete hardware
(so you needn't throw the hardware away)</a>,
common modern PC hardware,
over a dozen different chipsets (not just Intel x86s),
<a href="http://linas.org/linux/i370.html">mainframes</a>,
<a href="http://gb.lwn.net/2000/features/FSLCluster">massive clusters</a>,
and a <a href="http://www.idg.net/crd_linux_473938_102.html">number
of supercomputers</a>.
GNU/Linux can be used for massive parallel processing; a common approach for
doing this is the
<a href="http://www.tldp.org/HOWTO/Beowulf-HOWTO.html">Beowulf
architecture</a>.
<a href="http://www.cs.sandia.gov/cplant">Sandia's ``CPlant''</a>
runs on a set of systems running GNU/Linux, and it's
the forty-second most powerful computer in the world as of June 2001
(number 42 on the <a href="http://www.top500.org/list/2001/06/">TOP 500 Supercomputer
list, June 2001</a>).
There's even a prototype implementation of GNU/Linux on
a <a href="http://www.freeos.com/articles/3800">wrist watch</a>,
And GNU/Linux runs on a vast number of different CPU chips, including
the <a href="http://www.linux.org/projects/ports.html">x86, Intel Itanium,
ARM, Alpha, IBM AS/400 (midrange), SPARC, MIPS, 68k, and Power PC</a>.
Another operating system that widely scales to many other hardware
platforms is
<a href="http://www.netbsd.org/">NetBSD</a>.
<p>
Thus, you can buy a small GNU/Linux or NetBSD system and
grow it as your needs grow;
indeed, you can replace small hardware with massively parallel or
extremely high-speed processors or very different CPU architectures
without switching operating systems.
Windows CE/ME/NT scales down to small platforms, but not to large ones, and
it only works on x86 systems.
Many Unix systems (such as Solaris) scale well to specific large platforms but
not as well to distributed or small platforms.
These OSS/FS systems are some of the most scaleable programs around.

<!-- Scaleability: Watch, 486s, -->
<!-- Top 500 supercomputers: http://www.top500.org -->
<!-- http://lwn.net/2001/0322/a/top500.php3 -->

</p><p>
</p></li><li>
<b>OSS/FS development processes can scale to develop large software systems.</b>
At one time it was common to ask
if the entire OSS/FS process is ``scaleable,'' that is, if OSS/FS
processes could really develop large-scale systems.
Bill Gates' 1976 ``Open Letter to Hobbyists'' asked rhetorically,
 ``Who can afford to do professional work for
nothing?  What hobbyist can put three man-years into programming,
finding all bugs, documenting his product, and distribute it for free?''
He presumed these were unanswerable questions - but he was wrong.
See <a href="http://www.dwheeler.com/sloc">my reports estimating
GNU/Linux's size</a>.
For Red Hat Linux 6.2, I found the size to be over 17 million source
lines of code (SLOC).  Implemented traditionally it would have taken
4,500 person-years and over $600 million to implement this distribution.
For Red Hat Linux 7.1, I found it to have over 30 million SLOC,
representing 8,000 person-years or $1 billion (a ``Gigabuck'').
Most developers ascribe to the design principle that components should be
divided into smaller components where practical - a practice also applied
to GNU/Linux - but some components aren't easily divided, and thus some
components are quite large themselves (e.g., over 2 million lines of code
for the kernel, mostly in device drivers).
Thus, it's no longer reasonable to argue that OSS/FS cannot scale to
develop large systems -- because it clearly can.
</li></ol>

<h1><a name="security">Security</a></h1>
Quantitatively measuring security is very difficult.
However, here are a few attempts to do so, and they suggest
that OSS/FS is often superior to proprietary systems.
I'll concentrate in particular on comparing OSS/FS to Windows systems.
<p>
</p><ol>
<li>
<b>J.S. Wurzler Underwriting Managers' ``hacker insurance'' costs 5-15% more
if Windows is used instead of Unix or GNU/Linux for Internet operation.</b>
At least one insurance company has indicated that Windows NT is
less secure than Unix or GNU/Linux systems, resulting in higher premiums
for Windows-based systems.
It's often difficult to find out when a company has been successfully
cracked; companies often don't want to divulge such information to
the public for a variety of reasons.
Indeed, if consumers or business partners lost trust in a company,
the resulting loss might be much greater than the original attack.
However, insurance companies that insure against cracking can require
that they get such information (as a condition of coverage),
and can compute future premiums based on that knowledge.
According to CNET,
Okemos, Mich.-based J.S. Wurzler Underwriting Managers,
one of the earliest agencies to offer ``hacker insurance''
(and thus more likely to have historical data for premium calculation),
has begun
<a href="http://news.cnet.com/news/0-1003-200-6077282.html">
charging its clients anywhere from 5 to 15 percent more if
they use Microsoft's Windows NT software
instead of Unix or GNU/Linux for their Internet operations</a>.
Walter Kopf, senior vice president of underwriting, said that
``We have found out that the possibility for loss
is greater using the NT system.''
He also said the
decision is based on findings from hundreds of security assessments
the company has done on their small and midsize business clients over
the past couple of years. 
<p>
</p></li><li>
<b>Most defaced web sites are hosted by Windows,
and Windows sites are disproportionately defaced more often than
explained by its market share.</b>
Another way to look at security is to look at the operating system
used by defaced web sites, and compare them to their market share.
A ``defaced'' web site is a site that has been broken into and has its
content changed (usually in a fairly obvious way, since subtle modifications
are often not reported).
The advantage of this measure is that unlike other kinds of security
break-ins (which are often ``hushed up''), it's often very difficult
for victims to hide the fact that they've been successfully attacked.
Historically, this information was maintained by Attrition.org.
A summary can be found in
<a href="http://www.vnunet.com/News/1116081">James Middleton's article</a>,
with the actual data found in
<a href="http://attrition.org/mirror/attrition/os-graphs.html">Attrition.org's
web site</a>.
Attrition.org's data showed that 59% of defaced systems
ran Windows, 21% Linux, 8% Solaris, 6% BSD, and 6% all others in the
period of August 1999 through December 2000.
Thus, Windows systems have has nearly 3 times as many defacements as
GNU/Linux systems.
This would make sense if there were 3 times as many Windows systems,
but no matter which figures you use, that's simply not true.
<p>
Of course, not all sites are broken through their web server and OS -
many are broken through exposed passwords, bad web application programming,
and so on.
But if this is so, why is there such a big difference in the number
of defacements based on the operating system?
No doubt some other reasons could be put forward
(this data only shows a correlation not a cause), but
this certainly suggests that OSS/FS can have better security.
</p><p>
<a href="http://news.cnet.com/news/0-1003-200-5994475.html?tag=rltdnws">
Attrition.org has decided to abandon keeping track
of this information due to the difficulty of keeping up with the
sheer volume of broken sites</a>, and it appeared that tracking this
information wouldn't be possible.
However, <a href="http://defaced.alldas.de/">defaced.alldas.de</a>
has decided to perform this valuable service.
Their recent reports show that this trend has continued;
on July 12, 2001, they report that
66.09% of defaced sites ran Windows, compared to
17.01% for GNU/Linux, out of 20,260 defaced websites.
<!-- Unknown 9.84%, Solaris 3.17%, IRIS 1.43%, FreeBSD 0.93%,
     BSDI 0.77%, SCO 0.38%, NetBSD 0.11%, AIX 0.07%, HP-US 0.04%,
     Tru64 Unix 0.04%, Digital Unix 0.03%, MaxOS 0.03%, OpenBSD 0.02%,
     out of 20,260 defaced websites. -->
</p><p>
</p></li><li>
<b>The Bugtraq vulnerability database suggests that the least vulnerable
operating system is OSS/FS, and that all the OSS/FS operating systems in its
study were less vulnerable
than Windows in 1999-2000.</b>
One approach to examining security is to use a vulnerability database;
an analysis of one database is the
<!-- was http://www.securityfocus.com/vdb/stats.html -->
<a href="http://www.securityfocus.com/cgi-gin/vulns.pl">Bugtraq
Vulnerability Database Statistics</a> page.
As of September 17, 2000, here are the total number of
vulnerabilities for some leading operating systems:

<center>
<table border="1" cellpadding="2" summary="GNU/Linux is Number 1">
<tbody><tr bgcolor="#bac0ff"><td>OS</td><td>1997</td><td>1998</td><td>1999</td><td>2000</td></tr>
<tr bgcolor="#ccccfe"><td>Debian GNU/Linux</td><td>2</td><td>2</td><td>30</td><td>20</td></tr>
<tr bgcolor="#ccccfe"><td>OpenBSD</td><td>1</td><td>2</td><td>4</td><td>7</td></tr>
<tr bgcolor="#ccccfe"><td>Red Hat Linux</td><td>5</td><td>10</td><td>41</td><td>40</td></tr>
<tr bgcolor="#ccccfe"><td>Solaris</td><td>24</td><td>31</td><td>34</td><td>9</td></tr>
<tr bgcolor="#ccccfe"><td>Windows NT/2000</td><td>4</td><td>7</td><td>99</td><td>85</td></tr>
</tbody></table>
</center>

<p>
You shouldn't take these numbers very seriously.
Some vulnerabilities are more important than others (some may provide little
if exploited or only be vulnerable in unlikely circumstances),
and some vulnerabilities are being actively exploited (while others have
already been fixed before exploitation).
Open source operating systems tend to include many applications that are
usually sold separately in proprietary systems (including Windows and
Solaris) - for example,
Red Hat 7.1 includes two relational database systems, two word processors,
two spreadsheet programs, two web servers, and a large number of text editors.
In addition, in the open source world, vulnerabilities are discussed
publicly, so vulnerabilities may be identified for
software still in development (e.g., ``beta'' software).
Those with small market shares are likely to have less analysis.
The ``small market share'' comment won't work with GNU/Linux, of course, since
we've already established that GNU/Linux is the #1 or #2 server OS (depending on
how you count them).
Still, this clearly shows that the three OSS/FS OSs listed
(Debian GNU/Linux, OpenBSD, and Red Hat Linux) did much better by this measure
than Windows in 1999 and (so far) in 2000.
Even if a bizarre GNU/Linux distribution was created explicitly to
duplicate all vulnerabilities present in any major GNU/Linux distribution,
this intentionally bad GNU/Linux distribution would
still do better than Windows (it would have
88 vulnerabilities in 1999, vs. 99 in Windows).
The best results were for OpenBSD, an OSS/FS operating system that
for years has been specifically focused on security.
It could be argued that its smaller number of vulnerabilities is because
of its rarer deployment, but the simplest explanation is that OpenBSD
has focused strongly on security - and achieved it better than the rest.
</p><p>

This data is partly of interest because
<a href="http://abcnews.go.com/sections/tech/FredMoody/moody000802.html">one
journalist, Fred Moody, failed to understand his data sources</a> - he used
these figures to try to show show that GNU/Linux had worse security.
<!-- Many question Fred Moody's objectivity;
    see http://www.pjprimer.com/moody.html -->
He took these numbers and then added the GNU/Linux ones so each Linux
vulnerability was counted at least twice (once for every distribution it
applied to plus one more).
By using these nonsensical figures
he declared that GNU/Linux was worse than anything.
<!-- was at:
 http://www.securityfocus.com/templates/forum_message.html?forum=2&amp;head=2782&amp;id=2782-->
If you read his article, you also need to read the rebuttal by the
<a href="http://web.archive.org/web/20010617174123/http://www.securityfocus.com/templates/forum_message.html?forum=2&amp;head=2782&amp;id=2782">
the rebuttal by the manager of the Microsoft Focus Area at SecurityFocus</a> to
understand why the journalist's article was so wrong.
</p><p>
In 2002,
<a href="http://www.vnunet.com/News/1128907">another journalist (James Middleton) made the same mistake</a>, apparently not learning from
previous work.
Middleton counted the same Linux vulnerability up to <i>four</i> times.
What's bizarre is that he even reported the individual numbers showing
that specific Linux systems were actually <i>more</i> secure by
using Bugtraq's vulnerability list through August 2001, and somehow
he didn't realize what it meant.
He noted that
Windows NT/2000 suffered 42 vulnerabilities, while
Mandrake Linux 7.2 notched up 33 vulnerabilities,
Red Hat Linux 7.0 suffered 28, Mandrake 7.1 had 27 and Debian 2.2 had 26.
In short, all of the GNU/Linux distributions had significantly fewer
vulnerabilities by this count.
It's not entirely clear what was being considered as being ``in'' the
operating system in this case, which would of course make a difference;
there are some hints that vulnerabilities in some
Windows-based products (such as Exchange)
weren't counted while vulnerabilities in
the same functionality (e.g., sendmail) <i>were</i> counted.
It also appears that many of the Windows attacks were more dangerous
(which were often remote attacks actively exploited),
as compared to the GNU/Linux ones (which were often local attacks,
found by looking at source and not actively exploited at the time).
I would appreciate links to someone who's analyzed these issues more
carefully.
The funny thing is that given all these errors, the paper gives evidence
that the GNU/Linux distributions were <i>more</i> secure.
</p><p>
Indeed, as noted in Bruce Schneier's
<a href="http://www.counterpane.com/crypto-gram-0009.html">Crypto-gram
of September 15, 2000</a>, vulnerabilities are affected by other
things such as how many attackers exploit the vulnerability,
the speed at which a fix is released by a vendor, and
the speed at which they're applied by administrators.
Nobody's system is invincible.
</p><p>
A more recent
<!-- http://www.techrepublic.com/article/jhtml?id=r00220010917mco01.htm&amp;fromtm=e102-3 -->
analysis by John McCormick in Tech Republic
compared
Windows and Linux vulnerabilities using
numbers through September 2001.
<!-- "By the Numbers: Comparing Windows security to Linux", John McCormick,
      Sep. 24, 2001. -->
This is an interesting analysis, showing that although Windows NT lead
in the number of vulnerabilities in 2000, using the 2001 numbers
through September 2001, Windows 2000 had moved to the ``middle of the pack''
(with some Linux systems having more, and others having fewer,
vulnerabilities).
However, it appears that in these numbers, bugs in Linux applications
have been counted with Linux, while bugs in Windows applications haven't -
and if that's so, this isn't really a fair comparison.
As noted above, typical Linux distributions bundle
many applications that are separately purchased from Microsoft.
</p><p>
</p></li><li>
<b>Red Hat (an OSS/FS vendor) responded more rapidly than Microsoft
or Sun to advisories; Sun had fewer advisories to respond to yet took
the longest to respond.</b>
Another data point is that SecurityPortal has compiled a
<a href="http://securityportal.com/cover/coverstory20000117.html">list
of the time it takes for vendors to respond to vulnerabilities</a>.
They concluded that:

<blockquote>
How did our contestants [fare]?
Red Hat had the best score, with 348 recess days on 31 advisories,
for an average of 11.23 days from bug to patch.
Microsoft had 982 recess days on 61 advisories,
averaging 16.10 days from bug to patch.
Sun proved itself to be very slow, although
having only 8 advisories it accumulated 716 recess days,
a whopping three months to fix each bug on average.
</blockquote>

Their table of data for 1999 is as shown:

<!-- This table's format was modified slightly because htmldoc had lots of
     trouble generating a good PDF file from the original.
     In particular, I removed "border" from the outside table.
     I removed "nosave" to satisfy the W3C's validator at
     validator.w3c.org.  DAW. -->
<center><table align="center" cellspacing="0" cellpadding="0" bgcolor="#444444">
<tbody><tr><td align="center" valign="middle">
<table border="0" align="center" cellspacing="1" cellpadding="3" bgcolor="#444444">
<tbody><tr>
    <td bgcolor="#a6d2ff" colspan="4" align="center" nowrap=""><font size="+1" face="Helvetica"><b>1999 Advisory Analysis</b></font></td>
</tr>    
<tr>
<td bgcolor="#dddddd" nowrap=""><font size="-2" face="Helvetica"><b>Vendor</b></font></td>
<td bgcolor="#dddddd" nowrap=""><font size="-2" face="Helvetica"><b>Total Days, Hacker Recess</b></font></td>
<td bgcolor="#dddddd" nowrap=""><font size="-2" face="Helvetica"><b>Total Advisories</b></font></td>
<td bgcolor="#dddddd" nowrap=""><font size="-2" face="Helvetica"><b>Recess Days/Advisory</b></font></td>

</tr><tr><td align="left" bgcolor="#eeeeee"><font size="+1">Red Hat</font></td><td align="center" bgcolor="#eeeeee"><font size="-1">348</font></td>
<td align="center" bgcolor="#eeeeee"><font size="-1">31</font></td>
<td align="right" bgcolor="#eeeeee"><font size="-1">11.23</font></td></tr>

<tr><td align="left" bgcolor="#eeeeee"><font size="+1">Microsoft</font></td><td align="center" bgcolor="#eeeeee"><font size="-1">982</font></td>
<td align="center" bgcolor="#eeeeee"><font size="-1">61</font></td>
<td align="right" bgcolor="#eeeeee"><font size="-1">16.10</font></td></tr>

<tr><td align="left" bgcolor="#eeeeee"><font size="+1">Sun</font></td><td align="center" bgcolor="#eeeeee"><font size="-1">716</font></td>
<td align="center" bgcolor="#eeeeee"><font size="-1">8</font></td>
<td align="right" bgcolor="#eeeeee"><font size="-1">89.50</font></td></tr>


<!-- <tr><td bgcolor="#A6D2FF" colspan="5" align="right"><font size="-1" face="Helvetica">
 &nbsp; </td></tr> -->
</tbody></table></td></tr></tbody></table>
</center>

<p>
Clearly this table uses a different method for counting security problems
than the previous table.
Of the three noted here, Sun's Solaris had the fewest vulnerabilities,
but it took by far the longest to fix security problems identified.
Red Hat was the fastest at fixing security problems, and placed in the
middle of these three in number of vulnerabilities.
It's worth noting that the OpenBSD operating system (which is
OSS/FS) had fewer reported vulnerabilities than all of these.
Clearly, having a proprietary operating system doesn't mean you're
more secure - Microsoft had the largest number of security advisories,
by far, using either counting method.

</p><p>
</p></li><li>
<b>A 2002 survey of developers
found that GNU/Linux systems are relatively immune from attacks from outsiders.
</b>
Evans Data Corp.'s
<a href="http://www.businesswire.com/cgi-bin/f_headline.cgi?bw.040802/220982285">Spring 2002 Linux Developer Survey</a>
surveyed more than 400 GNU/Linux developers,
and found that Linux systems are relatively immune from attacks from outsiders.
Even though computer attacks have almost doubled annually since 1988
(according to CERT), 78% of the respondents to the GNU/Linux developers
survey have never experienced an unwanted intrusion
and 94% have operated virus-free.
Clearly, the survey shows that GNU/Linux ``doesn't get broken into
very often and is even less frequently targeted by viruses,''
according to Jeff Child (Evans Data Corp.'s Linux Analyst).
The interpretation of this data varies; Child notes that
it's much harder to hack a knowledgeable owner's system
(and most Linux developers have hands-on, technical knowledge)
and that because there are fewer [desktop] GNU/Linux systems there are fewer
viruses being created to attack GNU/Linux.
The developers being surveyed attributed the low incidence of attacks to
the Open Source Software (OSS) environment;
``more than 84% of Linux developers believe that Linux
is inherently more secure than software not created in an OSS environment,''
and they ranked ``Linux's security roughly comparable in security
to Solaris and AIX (two very secure operating systems
long trusted by large enterprises)
and above any of the Windows platforms by a significant margin.''


<p>
</p></li><li>
<b>Apache has a better security record than Microsoft's IIS, as measured
by reports of serious vulnerabilities.</b>
Eweek's July 20, 2001 article
<a href="http://www.zdnet.com/eweek/stories/general/0,11011,2792860,00.html">
``Apache avoids most security woes''</a> examined security advisories
dating back to Apache 1.0.
They found that Apache's last serious security problem
(one where remote attackers could run arbitrary code on the server)
was announced in January 1997.
A group of less serious problems (including a buffer
overflow in the server's logresolve utility) was
announced and fixed in January 1998 with Apache
1.2.5. In the three and a half years since then,
Apache's only remote security problems have been a
handful of denial-of-service and information leakage
problems (where attackers can see files or directory listings they shouldn't). 
<!-- They don't mention in their article, for example, that
     Apache 1.3.20 fixed a vulnerability in the Win32 and OS/2 ports,
     where a client could cause a fault in the child process that had to
     be manually cleared.  There were no identified means of using this
     as an exploit to compromise server data, so this was purely a DoS.
     See http://httpd.apache.org/dist/httpd/CHANGES_1.3.
     Thus, it appears the article really is correct. -->
<p>
In contrast, in the article
<a href="http://www.zdnet.com/eweek/stories/general/0,11011,2792859,00.html">
``IT bugs out over IIS security</a>,'' eWeek determined that
Microsoft has issued
<a href="http://www.microsoft.com/technet/treeview/default.asp?url=/technet/itsolutions/security/current.asp?productid=17&amp;servicepackid=0">
21 security bulletins for IIS from January 2000 through June 2001</a>.
Determining what this number means is a little difficult, and the article
doesn't discuss these complexities, so I've examined Microsoft's bulletins
myself to find their true significance.
Not all of the bulletins have the same significance, so just stating that
there were ``21 bulletins'' doesn't give the whole picture.
However, it's clear that several of these
bulletins discuss dangerous vulnerabilities
that allow an external user to gain control over the system.
I count 5 bulletins on such highly dangerous vulnerabilities
for IIS 5.0 (in the period from January 2000 through June 2001), and
previous to that time, I count 3 such bulletins for IIS 4.0
(in the period of June 1998 through December 1999).
Feel free to examine the bulletins yourself; they are
MS01-033, MS01-026, MS01-025, MS01-023, MS00-086,
MS99-025, MS99-019, and MS99-003.
<!-- For IIS 5.0,
    I count one in June 2001, three in May 2001, and one in November 2000.

    MS01-033: Unchecked Buffer in Index Server ISAPI Extension Could Enable...
    MS01-026: Superfluous Decoding Operation Could Allow Command Execution...
    MS01-025: Index Server Search Function Contains Unchecked Buffer
    MS01-023: Unchecked Buffer in ISAPI Extension could Enable Compromise...
    MS00-086: Web Server File Request Parsing

    Before January 2000 only previous IIS versions are listed;
     for IIS 4.0, I count July 1999, June 1999, and February 1999.

    July 1999: MS99-025: Unauthorized Access to IIS Servers through ODBC...
    June 1999: MS99-019: Malformed HTR Request
    Feb  1999: MS99-003: IIS Malformed FTP List Request

-->
The <a href="http://www.cert.org/advisories/CA-2001-19.html">Code Red</a>
worm, for example, exploited a vast number of IIS sites through
the vulnerabilities identified in the June 2001 security bulletin MS01-033.
</p><p>
In short, by totaling the number of reports of dangerous
vulnerabilities (that allow attackers to execute arbitrary code),
I find a total of 8 bulletins for IIS from June 1998 through
June 2001, while Apache had zero such vulnerabilities for that time
period.
Apache's last such report was in January 1998, and that one
affected the log analyzer not the web server itself.
As was noted above, the last such dangerous vulnerability in Apache itself
was announced in January 1997.
</p><p>
It's time-consuming to do this kind of analysis, so I haven't repeated
the effort more recently.
However, it's worth noting
<a href="http://www.eweek.com/article/0,3658,s=1884&amp;a=25302,00.asp">
eWeek's April 10, 2002 article</a>
noting that ten more IIS flaws have been found
in IIS Server 4.0, 5.0, and 5.1, some of which would
allow attackers to  crash the IIS service or allow the
attacker to run whatever code he chooses.
<!-- "Microsoft warns of 10 IIS Flaws, Dennis Fisher -->
</p><p>
Even this doesn't give the full story, however; a vulnerability in IIS 
tends to be far more dangerous than an equivalent vulnerability in Apache,
because Apache wisely follows the good security practice of
``least privilege.''
IIS is designed so that anyone who takes over IIS can take over
the <i>entire system</i>, performing actions such as
reading, modifying, or erasing any file on the system.
In contrast, Apache is installed with very few privileges by default, so
even taking over Apache gives attackers relatively few
privileges. For example, cracking Apache does not give attackers the
right to modify or erase most files.
This is still not good, of course, and an attacker may be able to
find another vulnerability to give them complete access, but an Apache
system presents more challenges to an attacker than IIS.
</p><p>
The article claims there are four reasons for Apache's strong security,
and three of these reasons are simply good security practices.
Apache installs very few server extensions by default
(a ``minimalist'' approach),
all server components run as a non-privileged user
(supporting ``least privilege'' as noted above), and
all configuration settings are centralized
(making it easy for administrators to know what's going on).
However, the article also claims that one of the main reasons Apache is
more secure than IIS is that its
``source code for core server files is well-scrutinized,'' a task that
is made much easier by being OSS/FS, and it could be argued that OSS/FS
encourages the other good security practices.
</p><p>
Simple counts of vulnerability notices aren't necessarily a good measure,
of course.
A vendor could intentionally release fewer bulletins - but since Apache's
code and its security is publicly discussed, it seems unlikely that
Apache is releasing fewer notices.
Fewer vulnerability notices could result if the product isn't well scrutinized
or is rarely used - but this simply isn't true for Apache.
Even the trend line isn't encouraging - using the months of the bulletins
(2/99, 6/99, 7/99, 11/00, three in 5/01, and 6/01), I find the
time in months between new major IIS vulnerability announcements to be
4, 1, 18, 6, 0, 0, 1, and 3 as of September 2001;
this compares to 12 and 44 as of September 2001 for Apache.
Given these trends, it looks like IIS's security is slowly improving,
but it has little likelihood of meeting Apache's security in the near future.
Indeed, these vulnerability counts are corroborated by other measures such
as the web site defacement rates.
</p><p>
The issue here isn't whether or not a particular program is invincible
(what nonsense!) - the issue here is which is more likely to resist
future attacks, based on past performance.
It's clear that Apache has much a better security record than IIS, so
much so that Gartner Group decided to make an unusual recommendation
(described below).

</p><p>
</p></li><li>
<b>IIS was attacked 1,400 times more frequently than Apache in 2001,
and Windows was attacked more than all versions of Unix.</b>
SecurityFocus co-founder and CEO Arthur Wong reported an analysis of
the various vulnerabilities and attacks (based on SecurityFocus's data) in
the February 2002 article
<a href="http://www.cnn.com/2002/TECH/internet/02/25/2002.security.idg/index.html"><i>RSA: Security in 2002 worse than 2001, exec says</i></a>.
IIS was attacked 17 million times, while Apache was attacked only 12,000 times;
this is a particularly shocking comparison, since there are significantly
more Apache systems.
In 2001, Windows systems were attacked 31 million times, while Unix systems
were attacked 22 million times.
See the article for more information.

<p>
</p></li><li>
<b>The Gartner Group is recommending that businesses switch from
Microsoft IIS to Apache or iPlanet due to IIS's poor security track record,
noting that enterprises had spent $1.2 billion simply fixing Code
Red (IIS-related) vulnerabilities by July 2001.</b>

Microsoft's IIS has such a bad security record that in September 2001,
<a href="http://news.cnet.com/news/0-1003-201-7239473-0.html?tag=nbs">
Gartner Group announced a recommendation</a> that
``businesses hit by both Code Red and Nimda immediately investigate
alternatives to IIS,
including moving Web applications to Web server software from
other vendors such as iPlanet and Apache.
Although those Web servers have required some security patches, they
have much better security records than IIS and are not under active
attack by the vast number of virus and worm writers.''
Microsoft is sometimes a Gartner Group customer, so this announcement
is especially surprising.
<p>
<!-- "Lack of Security Processes Keeps Sending Enterprises to 'Code Red',
     1 August 2001, John Pescatore, Note Number FT-14-2441. -->
In a <a href="http://www.gartner.com/DisplayDocument?id=336339">background
document by Gartner</a>,
they discuss Code Red's impacts further.
By July 2001, Computer Economics (a research firm) estimated that
enterprises worldwide had spent $1.2 billion fixing vulnerabilities in
their IT systems that Code Red could exploit (remember, Code Red is designed
to only attack IIS systems; systems such as Apache are immune).
To be fair, Gartner correctly noted that the problem is not just that
IIS has vulnerabilities; part of the problem is that enterprises using IIS
are not keeping their IT security up to date, and Gartner openly wondered why
this was the case.
However, Gartner also asked the question, ``why do Microsoft's software
products continue to provide easily exploited openings for such attacks?''
This was prescient, since soon after this the ``Nimba'' attack surfaced
which attacked IIS, Microsoft Outlook, and other Microsoft products.

</p><p>
<!-- http://seattlep-i.nwsource.com/business/aptech_story.asp?category=1700&amp;slug=Microsoft%20Gartner -->
A brief aside is in order here.
Microsoft spokesman Jim Desler tried to counter Gartner's recommendation,
trying to label it as ``extreme'' and
saying that ``serious security vulnerabilities have been found in
all Web server products and platforms..  this is an industrywide challenge.''
While true, this isn't the whole truth.  As Gartner points out,
``IIS has a lot more security vulnerabilities than other products
and requires more care and feeding.''
It makes sense to select the product with the best security
track record, even if no product has a perfect record.


</p><p>
</p></li><li>
<b>The majority of the most serious security problems
only apply to Microsoft's products, and not to OSS/FS products, as suggested
by the CERT/CC's ``most frequent, high-impact types of
security incidents and vulnerabilities'' and the ICAT database.</b>
Some security vulnerabilities are more important than others, for a
variety of reasons.
Thus, some analysis centers try to determine what's ``most important,''
and their results suggest that OSS/FS just doesn't have as many vulnerabilities.
<p>
The CERT Coordination Center (CERT/CC) is federally funded to study
security vulnerabilities and perform related activities such as publishing
security alerts.
I sampled their list of <a href="http://www.dwheeler.com/frozen/cert_current_activity.html">
``current activity'' of the most frequent, high-impact security
incidents and vulnerabilities on September 24, 2001</a>,
and found yet more evidence
that Microsoft's products have poor security compared to others
(including OSS/FS).
Four of the six most important security vulnerabilities
were specific to Microsoft:
W32/Nimda, W32/Sircam, cache corruption on Microsoft DNS servers, and
``Code Red'' related activities.
Only one of the six items primarily affected non-Microsoft products
(a buffer overflow in telnetd); while this particular vulnerability is
important, it's worth noting that many open source systems
(such as Red Hat 7.1) normally don't enable
this service (telnet) in the first place and thus are less likely to be
vulnerable.
The sixth item (``scans and probes'') is a general note that there is
a great deal of scanning and probing on the Internet, and that there are
many potential vulnerabilities in all systems.
Thus, 4 of 6 issues are high-impact vulnerabilities are specific to Microsoft,
1 of 6 are vulnerabilities primarily affecting Unix-like systems
(including OSS/FS operating systems),
and 1 of 6 is a general notice about scanning.
Again, it's not that OSS/FS products never have security vulnerabilities -
but they seem to have fewer of them.
</p><p>
The <a href="http://icat.nist.gov/">ICAT</a> system provides a searchable
index and ranking for the vulnerabilities cross-references by CVE.
I sampled its top ten list on December 19, 2001; this top ten list
is defined by the number of requests made for a particular vulnerability
in ICAT (and including only vulnerabilities within the last year).
In this case, 8 of the top 10 vulnerabilities only affect proprietary systems
(in all cases, Windows).
Only 2 of 10 affect OSS/FS systems (#6, CAN-2001-0001, a weakness in
PHP-Nuke 4.4, and #8, CVE-2001-0013, a new vulnerability found in an
old version of BIND - BIND 4).
Obviously, by itself this doesn't prove that there are fewer serious
vulnerabilities in OSS/FS programs, but it is suggestive of it.

<!--

Here's the detailed "ICAT Top Ten List" from 12/19/01.
ICAT contained the CVE database last updated 12/06/01, 3311 vulnerabilities.

"the top ten most popular vulnerabilities as defined by the number of requests made for a particular vulnerability in ICAT. Only vulnerabilities published within the last year are included in order to keep the list focused on relevant problems."

1  CVE-2001-0333
Directory traversal vulnerability in IIS 5.0 and earlier allows remote attackers to execute arbitrary commands by encoding .. (dot dot) and "\" characters twice.
2 CAN-2001-0500
Buffer overflow in ISAPI extension (idq.dll) in Index Server 2.0 and Indexing Service 2000 in IIS 6.0 beta and earlier allows remote attackers to execute arbitrary commands via a long argument to Internet Data Administration (.ida) and Internet Data Query (.idq) files.
3 CVE-2001-0241
Buffer overflow in Internet Printing ISAPI extension in Windows 2000 allows remote attackers to gain root privileges via a long print request that is passed to the extension through IIS 5.0.
4 CVE-2001-0002
Internet Explorer 5.5 and earlier allows remote attackers to obtain the physical location of cached content and open the content in the Local Computer Zone, then use compiled HTML help (.chm) files to execute arbitrary programs.
5 CAN-2000-1200
Windows NT allows remote attackers to list all users in a domain by obtaining the domain SID with the LsaQueryInformationPolicy policy function via a null session and using the SID to list the users.
6 CAN-2001-0001
cookiedecode function in PHP-Nuke 4.4 allows users to bypass authentication and gain access to other user accounts by extracting the authentication information from a cookie.
7 CVE-2001-0004
IIS 5.0 and 4.0 allows remote attackers to read the source code for executable web server programs by appending "%3F+.htr" to the requested URL, which causes the files to be parsed by the .HTR ISAPI extension, aka a variant of the "File Fragment Reading via .HTR" vulnerability.
8 CVE-2001-0013
Format string vulnerability in nslookupComplain function in BIND 4 allows remote attackers to gain root privileges.
9 CVE-2001-0335
FTP service in IIS 5.0 and earlier allows remote attackers to enumerate Guest accounts in trusted domains by preceding the username with a special sequence of characters.
10 CVE-2001-0003
Web Extender Client (WEC) in Microsoft Office 2000, Windows 2000, and Windows Me does not properly process Internet Explorer security settings for NTLM authentication, which allows attackers to obtain NTLM credentials and possibly obtain the password, aka the "Web Client NTLM Authentication" vulnerability.
-->

</p><p>
</p></li><li>
<b>Computer viruses are overwhelmingly more prevalent
on Windows than any other system.</b>
Virus infection has been a major cost to users of Microsoft Windows.
The LoveLetter virus alone is estimated to have cost $960 million
in direct costs and $7.7 billion in lost productivity,
and the anti-virus software industry sales total nearly $1 billion annually.
Dr Nic Peeling and Dr Julian Satchell's
<a href="http://www.govtalk.gov.uk/interoperability/egif_document.asp?docnum=430"><i>Analysis of the Impact of Open Source Software</i></a>
includes an analysis of the various data sources for virus counts,
noting the disproportionate vulnerability of Windows systems.
Here is what they said:
<!-- section 2.13 discusses virus counts -->
<p>
</p><blockquote>
The numbers differ in detail, but all sources agree that computer
viruses are overwhelmingly more prevalent on Windows than any other system.
There are about 60,000 viruses known for Windows,
40 or so for the Macintosh, about 5 for commercial Unix versions,
and perhaps 40 for Linux.
Most of the Windows viruses are not important,
but many hundreds have caused widespread damage.
Two or three of the Macintosh viruses were widespread
enough to be of importance. None of the Unix or
Linux viruses became widespread - most were confined to the laboratory.
</blockquote>
<p>
Many have noted that one reason Windows is attacked more often is
simply because there are so many Windows systems in use.
Windows is an attractive target for virus writers simply
because it is in such widespread use.
For a virus to spread, it has to transmit itself to
other susceptible computers; on average,
each infection has to cause at least one more.
The ubiquity of Windows machines makes it easier
for this threshold to be reached.
</p><p>
There may be a darker reason: there are many who do not like Microsoft's
business practices, and perhaps this contributes to the problem.
Some of Microsoft's business practices have been proven in court to be
illegal, but the U.S. government appears unwilling to effectively punish
or stop those practices.
Some computer literate people appear to be taking their frustration out
on users of Microsoft's product.
This is absolutely wrong, and in most countries illegal.
It is extremely unethical to attack an innocent user of a Microsoft
product simply because of Microsoft's policies, and I condemn such behavior.
At this point, although this has been speculated many times, I have
not found any evidence that this is an important motivator of many
actual attacks.
On the other hand, if you are choosing products, do you really
want to choose the product whom people may have a vendetta against?
</p><p>
However, the reasons given above don't explain the
disproportionate vulnerability of Windows.
A simpler explanation, and one that is easily proven, is that
Microsoft has made a number of design choices over the years in Windows
that have allowed the execution of untrusted
code, and this has made Windows a very easy target.
Examples include execution of start-up macros in Word,
execution of attachments in Outlook, and lack of write protection on
system directories in Windows 3.1/95/98.
This may be because Microsoft has assumed that customers
will buy their products whether or not Microsoft secures them;
since until recently there's been little competition, there was no
need to spend money on ``invisible'' attributes such as security.
It's also possible that Microsoft is still trying to adjust to an
Internet-based world;
the Internet would not have developed as it has without Unix-like systems,
while for many years Microsoft ignored the Internet and then
suddenly had to play ``catch-up'' in the early 1990s.
<!-- http://hallinternet.com/net_history_trends/219.shtml -->
Microsoft has sometimes claimed that they want to make sure that their
products are ``easy to use''; while it's true that sometimes security
features can make a product harder to use,
typically security features that are carefully designed into a product
don't seriously harm security.
Besides, what's so easy to use about a system that has to be
reformatted because another virus got in?
But for whatever the reason, it's demonstrably true that
Microsoft's designers have intentionally made decisions that weakened
Windows' security.
</p><p>
In contrast,
while it's possible to write a virus for OSS/FS systems, their design
makes it more difficult for viruses to spread... showing that
Microsoft's design decisions were not inevitable.
It appears that
OSS/FS developers tend to select design choices that limit the damage
of viruses, perhaps in part because their code is subject to
public inspection and comment.
For example,
OSS/FS programs generally do not support start-up macros nor execution
of mail attachments that can be controlled by attackers.
Also, leading OSS/FS operating systems (such as
GNU/Linux and the *BSDs) have always had
write protection on system directories.
<a href="http://www.roaringpenguin.com/mimedefang/anti-virus.php3">
Another discussion on why viruses don't seem to significantly
affect OSS/FS systems is available from Roaring Penguin</a>.
OSS/FS systems are <i>not</i> immune to malicious code,
but they are certainly more resistant.
</p><p>
</p></li><li>
<b>According to a Network Security evaluation, an OSS/FS vulnerability
scanner (Nessus) was found to be the best (most effective).</b>
On January 8, 2001,
Network Computing's article
<a href="http://www.networkcomputing.com/1201/1201f1b1.html">
<i>Vulnerability Assessment Scanners</i></a>.
reported an evaluation of nine network scanning tools,
most of them proprietary.
In their evaluation, Network Computing set up demonstration
systems with 17 of the most common and
critical vulnerabilities; they then used
the various network scanning tools to see how effectively each
of the tools detected these vulnerabilities.
Sadly, not one product detected all vulnerabilities; the best scanner
was the OSS/FS program Nessus Security Scanner, which found 15 of the 17
(which also received their top total score); the next best was
a proprietary scanner which only found 13.5 out of 17.
<p>
In their words,
</p><blockquote>
Some of us were a bit skeptical of the open-source Nessus
project's thoroughness until [Nessus] discovered the greatest
number of vulnerabilities. That's a hard fact to argue with,
and we are now eating our words ...
[Nessus] got the highest overall score
simply because it did more things right than the other products.
</blockquote>
<p>
I agree with the authors that ideally a network vulnerability scanner
should find every well-known vulnerability,
and that ``even one hole is too many.''
Still, perfection is rare in the real world.
More importantly,
a vulnerability scanner should only be part of the process to secure an
organization - it shouldn't be the sole activity.
Still, this evaluation suggests that an organization
will be <i>more</i> secure, not less secure, by using an OSS/FS program.
It could be argued that this simply shows that this particular OSS/FS
program had more functionality - not more security - but in this case,
the product's sole functionality was to improve security.
</p></li></ol>

<p>
One serious problem is that there are strong economic disincentives
for proprietary vendors to make their software secure.
For example, if vendors make their software more secure,
they would often fail to be ``first'' in a given market;
this often means that they will lose that market.
Since it is extremely difficult for customers to distinguish
proprietary software with strong security from those with poor security,
the poor products tend to eliminate the good ones (after all, they're
cheaper to develop and thus cost less).
Governments have other disincentives as well.
For a discussion of some of the economic disincentives for secure software, see
<a href="http://www.acsac.org/2001/abstracts/thu-1530-b-anderson.html">
<i>Why Information Security is Hard - an Economic Perspective</i> by
Ross Anderson</a> (Proceedings of the
Annual Computer Security Applications Conference (ACSAC),
December 2001, pp. 358-365).
It's not clear that OSS/FS always avoids these disincentives, but
it appears in at least some cases it does.
For example, OSS/FS source code is public, so the difference in security
is far more visible than in proprietary products.

</p><p>
One of the most dangerous security problems with proprietary software
is that if intentionally malicious code is snuck into it,
such code is extremely difficult to find.
Few proprietary vendors have other developers
examine <i>all</i> code in great detail -
their testing processes are designed to catch mistakes (not malice)
and often don't look at the code at all.
<!--
If you doubt, consider all the ``Easter eggs'' in proprietary software,
such as flight simulators in spreadsheet programs - rarely are these
Easter eggs approved by management (see
<a href="http://www.eggscentral.com">Eggs Central</a> and
<a href="http://www.eeggs.com">EEggs</a>, which actually collect lists
of them).
The problem isn't Easter eggs themselves - the problem is that,
since they easy slip through without management approval, it's clear that
malicious code could slip into proprietary software in many
organizations too.
-->
In contrast, malicious code can be found by anyone when the source code
is publicly available, and with OSS/FS, there are incentives for arbitrary
people to review it (such as to add new features or perform a security review).
Thus, someone inserting malicious code to an OSS/FS project
runs a far greater risk of detection.
Here are two examples, one confirmed, one not confirmed:
</p><ol>
<li>
Some time between 1992 and 1994,
Borland inserted an intentional ``back door'' into their database server,
``InterBase'', as a secret username and fixed password.
This back door allowed any local or remote user
to manipulate any database object and install arbitrary programs,
and in some cases could lead to controlling the machine as ``root''.
This vulnerability stayed in the product for at least 6 years - no
one else could review the product,
and Borland had no incentive to remove the vulnerability.
Then Borland released its source code on July 2000 as an OSS/FS project.
The "Firebird" project began working with the source code,
and uncovered this serious security problem with InterBase in
December 2000 (only 5 months after release).
By January 2001 the CERT announced the existence of this back door
as CERT advisory CA-2001-01.
What's discouraging is that the backdoor can be easily found
simply by looking at an ASCII dump of the program (a common cracker trick),
so it's quite possible that this vulnerability was exploited many times
in the intervening years.
Once this problem was found by open source developers reviewing the code,
it was patched quickly.
</li><li>
Mohammad Afroze Abdul Razzak,
arrested by Mumbai (Bombay) police Oct. 2, 2001, claims that
<a href="http://www.newsbytes.com/news/01/173039.html">
Osama bin Laden's Al Qaeda network were able to gain employment
at Microsoft and attempted to plant
"trojans, trapdoors, and bugs in Windows XP."</a>
This was reported to Ravi Visvesvaraya Prasad,
a New Delhi information systems and telecommunication consultant,
and then reported by the
<a href="http://www.newsbytes.com/about/index.html">
Washington Post's Newsbytes division</a>.
<!--
(``Suspect Claims Al Qaeda Hacked Microsoft - Expert''
by Brian McWilliams, Newsbytes, Redmond, Washington, USA,
17 Dec 2001, 2:08 PM CST).
-->
This claim has not been confirmed; indeed, I'm somewhat skeptical.
The problem, however, is that this is impossible to disprove.
Even if this particular case isn't true, note that this threat
is unfortunately a credible threat to proprietary software, because
very few of its users can review the code.
This is far less dangerous to OSS/FS software, because of the
worldwide review that's possible (including the ability to see
the changes made in each version).
</li></ol>

<p>
<a href="http://techupdate.zdnet.com/techupdate/stories/main/0,14179,2859555,00.html">Bruce Perens, in ``Open sourcers wear the white hats''</a>,
makes the interesting claim that most of the people reviewing
proprietary products looking for security flaws
(aside from one or two paid reviewers) are ``black hats,''
outsiders who disassemble the code or try
various types of invalid input in search of a flaw that they can exploit
(and not report).
There is simply little incentive, and many roadblocks,
for someone to search for security flaws simply to improve someone else's
proprietary product.
``Only a black hat would disassemble code to look for security flaws.
You won't get any `white hats' doing this
for the purpose of [just] closing the flaws.''
In contrast, he believes many open source developers <i>do</i> have
such an incentive.
I think this article slightly overstates the case;
there are other incentives (such as fame) that can motivate a few people
to review some other company's proprietary product for security.
Still, he has a point;
even formal reviews often only look at designs (not code),
proprietary code is often either unreviewed or poorly reviewewed,
and there are many cases (including the entire OpenBSD system) where
legions of developers review open source code for security issues.
As he notes,
``open source has a lot of `white hats' looking at the source.
They often do find security bugs while working on
other aspects of the code, and the bugs are reported and closed.''

</p><p>
Now it should be obvious from these figures that OSS/FS systems
are not magically invincible from security flaws.
Indeed, some have argued that making the source code available gives
attackers an advantage (because they have more information to make an
attack).
While OSS/FS gives attackers more information, this
ignores opposing forces: having the source code
also gives the defenders more information (because they can also
examine its original source code), and in addition, the defenders can
improve the code.
For a longer description of these issues, see
<a href="http://www.dwheeler.com/secure-programs/Secure-Programs-HOWTO/open-source-security.html">my discussion on open source and security</a>
(part of my book on
<a href="http://www.dwheeler.com/secure-programs">writing secure software</a>).
However, from these figures, it appears that OSS/FS systems are often
<i>better</i> - not just equal - in their resistance to attacks.



</p><h1><a name="tco">Total Cost of Ownership (TCO)</a></h1>
Total cost of ownership (TCO) is an important measure;
it doesn't matter if a product starts out cheaply if it costs you more
down the line.
However, TCO is extremely sensitive to the set of assumptions you make.
<p>
Indeed, whatever product you use or support, you can probably find a
study to show it has the lowest TCO for some circumstance.
Not surprisingly, both
<!-- was http://www.microsoft.com/NTServer/nts/exec/Compares/LowerTCO.asp -->
Microsoft and
<a href="http://www.sun.com/servers/workgroup/tco/metastudy.html">Sun</a>
provide studies showing that they have the lowest TCO
(but see my comments later about Microsoft's study).
<a href="http://www.vnunet.com/Analysis/85833">Xephon</a> has a study
determining that mainframes are the cheapest per-user
(due to centralized control) at £3450 per user per year;
Centralized Unix cost £7350 per user per year, and a
decentralized PC environment costs £10850 per user per year.
<a href="http://www.xephon.com/">Xephon</a> appears to be a
mainframe-based consultancy, though,
and would want the results to come out this way.
There are indeed situations where applying a mainframe makes sense..
but as we'll see in a moment, you can use OSS/FS in such environments too.
</p><p>
In short, what has a smaller TCO depends on your environment and needs.
To determine TCO you have to identify all the important cost drivers
(the ``cost model'') and estimate their costs.
Don't forget ``hidden'' costs, such as administration costs, upgrade costs,
technical support, end-user operation costs, and so on.
<!-- I once referenced the Gartner Group's TCO model, but they've reorganized
     their web site and it appears they no longer have a linkable location.
     It WAS at:
         http://www.gartner.com/public/static/consulting/tco/tcochart.html
     Their model used four categories: capital expenditure, technical support,
     administration and end-user operations, each broken down into
     items like costs of labour and training. 
     Some mention of it is at:
       http://www.zdnet.co.uk/pcmag/labs/1998/06/corp_pc/8.html
-->
However, OSS/FS has a number of strong cost advantages in various
categories that, in many cases, will result in its having the smallest TCO.
</p><p>
</p><ol>
<li><b>OSS/FS costs less to initially acquire.</b>
OSS/FS costs much less to get initially.
OSS/FS isn't really ``free'' in the monetary sense to get;
the ``free'' in ``free software'' refers to freedom, not price
(usually summarized as ``free speech, not free beer'').
You'll still spend money for paper documentation, support, training,
system administration, and so on, just as you do with proprietary systems.
In many cases, the actual programs in OSS/FS distributions
can be acquired freely by downloading them
(<a href="http://www.linux.org/dist/index.html">linux.org provides some
pointers on how to get distributions</a>).
However, most people (especially beginners and those without high-speed
Internet connections)
will want to pay a small fee to a distributor for
a nicely integrated package with CD-ROMs, paper documentation, and support.
Even so, OSS/FS is far less expensive to acquire.
<p>
For example, look at some of the price differences when trying
to configure a server (say a public web server or an intranet
file and email server, in which you'd like to use C++ and an RDBMS
for some portions of it).
This is an example, of course; different missions would involve different
components.
I used the prices from ``Global Computing Supplies'' (Suwanee, GA),
September 2000, and rounded to the nearest dollar.
Here's a quick summary of some costs:
</p><p>
</p><center>
<table border="1" cellpadding="2" summary="GNU/Linux costs far less">
<tbody><tr bgcolor="#bac0ff"><td> </td><td>Microsoft Windows 2000</td><td>Red Hat Linux</td></tr>
<tr bgcolor="#ccccfe"><td>Operating System</td><td>$1510 (25 client)</td><td>$29 (standard), $76 deluxe, $156 professional (all unlimited)</td></tr>
<tr bgcolor="#ccccfe"><td>Email Server</td><td>$1300 (10 client)</td><td>included (unlimited)</td></tr>
<tr bgcolor="#ccccfe"><td>RDBMS Server</td><td>$2100 (10 CALs)</td><td>included (unlimited)</td></tr>
<tr bgcolor="#ccccfe"><td>C++ Development</td><td>$500</td><td>included</td></tr>
</tbody></table>
</center>
<p>
Basically,
Microsoft Windows 2000 (25 client) costs $1510;
their email server Microsoft Exchange (10-client access) costs $1300,
their RDBMS server SQL Server 2000 costs $2100 (with 10 CALs),
and their C++ development suite Visual C++ 6.0 costs $500.
Red Hat Linux 6.2 (a widely-used GNU/Linux distribution) costs
$29 for standard (90 days email-based installation support),
$76 for deluxe (above plus 30 days telephone installation support), or
$156 for professional (above plus SSL support for encrypting web traffic);
in all cases it includes all of these functionalities
(web server, email server, database server, C++, and much more).
A public web server with Windows 2000 and an RDBMS might cost
$3610 ($1510+$2100) vs. Red Hat Linux's $156,
while an intranet server with Windows 2000 and an email server
might cost $2810 ($1510+$1300) vs. Red Hat Linux's $76.
</p><p>
Both packages have functionality the other doesn't have.
The GNU/Linux system always comes with an unlimited number of licenses;
the number of clients you'll actually use depends on your requirements.
However, this certainly shows that no matter what,
Microsoft's server products cost thousands of dollars more
per server than the equivalent GNU/Linux system.
</p><p>
For another in-depth analysis comparing the initial costs
GNU/Linux with Windows, see
<a href="http://www.cyber.com.au/cyber/about/linux_vs_windows_pricing_comparison.pdf"><i>Linux vs. Windows: The Bottom Line</i></a>
by <a href="http://www.cyber.com.au/">Cybersource Pty Ltd</a>.
Here's a summary of their analysis (in 2001 U.S. dollars):
</p><center>
<table border="1" cellpadding="2" summary="GNU/Linux costs far less">
<tbody><tr bgcolor="#bac0ff"><td> </td><td>Microsoft Solution</td><td>OSS/FS (GNU/Linux) Solution</td><td>Savings by using GNU/Linux</td></tr>
<tr bgcolor="#ccccfe"><td>Company A (50 users)</td><td>$69,987</td><td>$80</td><td>$69,907</td></tr>
<tr bgcolor="#ccccfe"><td>Company B (100 users)</td><td>$136,734</td><td>$80</td><td>$136,654</td></tr>
<tr bgcolor="#ccccfe"><td>Company C (250 users)</td><td>$282,974</td><td>$80</td><td>$282,894</td></tr>
</tbody></table>
</center>
<p>
<a href="http://consultingtimes.com/Serverheist.html">Consulting Times</a>
found that as the number of mailboxes got large, the three-year TCO
for mainframes with GNU/Linux became in many cases quite compelling.
For 50,000 mailboxes, an Exchange/Intel
solution cost $5.4 million, while the Linux/IBM(G6)
solution cost $3.3 million.
For 5,000 mailboxes, Exchange/Intel cost $1.6 million, while
Groupware on IFL cost $362,890.
For yet another study, see the
<a href="http://www.jimmo.com/Linux-NT_Debate/Cost_Comparison.html">
Cost Comparison from jimmo.com</a>.
Obviously, the price difference depends on exactly what functions you need
for a given task, but for many common situations, GNU/Linux costs
far less to acquire.
</p><p>
</p></li><li>
<b>Upgrade costs are typically far less.</b>
Long-term upgrade costs are far less for OSS/FS systems.
For example, upgrading a Microsoft system will typically cost around half the
original purchase.
What's worse, you are essentially at their mercy for long-term pricing,
because there is only a single supplier
(see <a href="http://osopinion.com/perl/story/9849.html"><i>Microsoft
Turns the Screws</i></a>).
In contrast, the GNU/Linux systems can be downloaded (free), or simply
re-purchased (generally for less than $100), and the single upgrade 
be used on every system.
This doesn't include technical support, but the technical support can
be competed (a situation that's not practical for proprietary software).
If you don't like your GNU/Linux supplier (e.g., they've become too
costly), you can switch.
<p>
</p></li><li><b>OSS/FS can often use older hardware more efficiently than
proprietary systems, yielding
smaller hardware costs and sometimes eliminating the need for new
hardware.</b>
OSS/FS runs faster on faster hardware, of course, but many OSS/FS
programs can use older hardware more efficiently than proprietary systems,
resulting in lower hardware costs - and in some cases requiring no new costs
(because ``discarded'' systems can suddenly be used again).
For example, the
<a href="http://www.microsoft.com/windows2000/server/evaluation/sysreqs/default.asp">minimum requirements for Microsoft Windows 2000 Server
(according to Microsoft)</a> are a Pentium-compatible CPU (133 MHz or higher),
128 MiB of RAM minimum (with 256MiB the ``recommended minimum''), and a
2 GB hard drive with at least 1.0 GB free.
According to Red Hat,
Red Hat Linux 7.1 (a common distribution of GNU/Linux) requires at a minimum
an i486 (Pentium-class recommended), 32MiB RAM (64MiB recommended), and
650MB hard disk space (1.2 GB recommended).
<p>
In Scientific American's August 2001 issue, the article
<a href="http://www.sciam.com/2001/0801issue/0801hargrove.html">
The Do-It-Yourself Supercomputer</a> discusses how the researchers built a
powerful computing platform with a large number of obsolete,
discarded computers and GNU/Linux.
The result was dubbed the ``Stone Soupercomputer''; by May 2001 it
contained 133 nodes, with a theoretical peak performance of 1.2 gigaflops.
</p><p>
</p></li><li>
<b>When used as an application server based system, the total costs
for hardware drop by orders of magnitude</b>.
Many people make the mistake of deploying OSS/FS workstations (such
as GNU/Linux or the *BSDs) the same way they would deploy Windows systems.
Although it's possible, this is an unnecessarily expensive approach
if they're installing a set of workstations
for typical productivity applications (e.g., word processing, spreadsheets,
etc. for an office),
For many, a better approach is to provide each user
with a very old GNU/Linux-based machine
which is merely a graphics display (an ``X terminal''),
and then run the actual applications on an ``application server''
that is shared by all the users.
See
<a href="http://www.linuxworld.com/site-stories/2001/0823.xterminal.html">
How to create a Linux-based network of computers for peanuts</a> for
more information about this.
With this application server approach, workstations can cost about $30 each
(using ``obsolete'' machines), a server (shared by many users) can cost
about $1000 each, and nearly all system administration is centralized
(reducing administration costs).
A nice side-effect of this approach is that
users can use any workstation just by logging in.
A more detailed discussion of this approach is given in
<a href="http://www.linuxworld.com/site-stories/2002/0403.tco.html">
Paul Murphy's article, <i>Total cost of ownership series revisited</i></a>.
This is how the City of Largo, Florida,
and many other organizations use GNU/Linux.
<p>
</p></li><li>
<b>As the number of systems and hardware performance increases,
this difference in initial and upgrade costs becomes even more substantial.</b>
As the number of servers increases, proprietary solutions become
ever more expensive.
First, many proprietary systems (including Microsoft) sell per-client
licenses; this means that even if your hardware can support more clients,
you'll have to pay more to actually use the hardware you've purchased.
Secondly, if you want to use more computers, you have to pay for more
licenses in proprietary systems.
In contrast,
for most GNU/Linux distributions, you can install as many copies as you like
for no additional fee, and there's no performance limit built
into the software.
There may be a fee for additional support, but
you can go to competing vendors for this support.
<p>
According to
<!-- "Linux slips slowly into the enterprise realm" by Deni Connor, 3/19/01 -->
<a href="http://www.nwfusion.com/news/2001/0319specialfocus.html">Network World Fusion News</a>, Linux is increasingly being
used in healthcare, finance, banking, and retail because of its
cost advantages when large numbers of identical sites and servers are built.
According to their calculations
for a 2,000 site deployment, SCO UnixWare would cost $9 million,
Windows would cost $8 million, and Red Hat Linux costs $180.
</p><p>
</p></li><li><b>There are many other factors; their effect varies on what
you're trying to do.</b>
There are many other factors in TCO, but it's difficult to categorize their
effects in general, and it's generally difficult to find justifiable
numbers for these other effects.
Windows advocates claim that system administrators are cheaper and
easier to find than Unix/Linux administrators, while GNU/Linux and Unix
advocates argue that fewer such administrators are needed
(because administration is easier to automate and the systems are
more reliable to start with).
Some GNU/Linux advocates have told me that GNU/Linux lends itself to hosting
multiple services on a single server in cases where Windows installations
must use multiple servers.
License compliance administration can be costly for proprietary systems
(e.g., time spent by staff to purchase CALS, keep track of licenses,
and undergo audits) - a cost that simply isn't relevant to OSS/FS.

<p>
</p></li><li><b>For many circumstances, the total cost savings can be substantial;
for example, savings exceeding $250,000 <i>per year</i>
were reported by 32% of the Chief Technical Officers (CTOs)
surveyed in a 2001 InfoWorld survey;
60% of these CTOs saved more than $50,000 annually.</b>
The August 27, 2001 InfoWorld (pages 49-50) reported on a survey of 40 CTOs
who were members of the InfoWorld CTO network.
In this survey, 32% using OSS reported savings greater than $250,000;
12% reported savings between 100,001 and $250,000; and 16% reported
saving between $50,001 and $100,000.
Indeed, only 8% reported annual savings less than $10,000
(so 92% were saving $10,000 or more annually).
<!-- Other numbers: $25,001 to $50,000: 16%; $10,001 to $25,000: 16% -->
A chief benefit of OSS, according to 93% of the CTOs, was reduced cost
of application development or acquisition; 72% said that a chief benefit
was reduced development or implementation time (multiple answers were allowed).
The CTOs reported using or planning to use OSS for web servers (65%),
server operating systems (63%), web-application servers (45%),
application development testing (45%), and desktop operating system (38%),
among other uses.
<!-- 28% said they weren't using or planning to use OSS.  Of course,
     many of these CTOs may not know they're using OSS; OSS often
     ``sneaks'' into organizations without their permission, just like
     PC's did in the early 1980's. -->
InfoWorld summarized it this way:
``in early 2000, it seemed as if no one was using open-source software
for business-critical tasks... a vast majority of today's
corporate IT executives are now using or plan to use OSS operating systems
and web servers for their enterprise applications.''
<p>
</p></li><li>
<b>Many organizations have reported significant savings when using
OSS/FS.</b>
Here are a few examples of specific organizations saving money
through OSS/FS:
<ul>
<li>
The paper <a href="http://www.robval.com/linux/desktop/index.asp"><i>Linux
as a Replacement for Windows 2000</i></a>
is an example of an analysis comparing Red Hat Linux 7.1 to
Windows 2000; in this customer's case, using Linux instead
of Windows 2000 saved $10,000.
The reviewer came from a Windows/DOS background, and after performing
an intensive hands-on Linux project lasting several months, determined
that ``you will be stunned by the bang for the buck
that ... open source software offers.''
<p>
</p></li><li>
Intel's IT Vice President, Doug Busch,
<a href="http://www.zdnet.com/zdnn/stories/news/0,4586,5098955,00.html">
reported savings of $200 million</a> by replacing
expensive Unix servers with cheaper servers running GNU/Linux.
<p>
</p></li><li>
<a href="http://news.cnet.com/news/0-1003-200-7720536.html?tag=owv">
Amazon.com was able to cut $17 million in technology expenses in a single
quarter</a>, largely because of a switch to Linux.
Amazon spent
$54 million on technology and content expenses in its third quarter
(ending Sept. 30), compared with $71 million in the year-ago quarter, and
executives expected that technology costs as a portion of net sales
would decrease by 20% this year.
<p>
</p></li><li>
<a href="http://techupdate.zdnet.com/techupdate/stories/main/0,14179,2860180,00.html">The city of Largo, Florida</a>
reports a savings of $1 million per year using GNU/Linux and ``thin clients.''
</li></ul>
<p>
There are many other reports from those who have switched
to OSS/FS systems; see
<a href="#usereports">
http://www.dwheeler.com/oss_fs_why.html#usereports</a>
for more information.
<!-- Here is the end of the TCO list: -->
</p></li></ol>

<p>
<!-- http://www.microsoft.com/NTServer/nts/exec/Compares/LowerTCO.asp -->
Microsoft's TCO study (mentioned earlier) is probably not useful as
a starting point for estimating your own TCO.
Their study reported the average TCO at sites using Microsoft products
compared to the average TCO at sites using Sun systems, but
although the Microsoft systems cost 37% less to own,
the Solaris systems handled larger databases, more
demanding applications connecting to those databases,
63% more concurrent connections, and 243% more hits per day. 
In other words, the Microsoft systems that did less work were less expensive.
This is not a useful starting point
if you're using TCO to help determine which system to buy --
to make a valid comparison by TCO,
you need to compare the TCOs of systems that both perform the
job that you need to do.
A two-part analysis by Thomas Pfau (see
<a href="http://fud-counter.nl.linux.org/tech/TCO.html">part 1</a> and
<a href="http://fud-counter.nl.linux.org/tech/TCO2.html">part 2</a>)
identifies this and many other flaws in the study.
</p><p>
Again, it's TCO that matters, not just certain cost categories.
However, given these large differences, in many situations
OSS/FS has a smaller TCO than proprietary systems.
At one time it was claimed that OSS/FS installation took more time,
but nowadays OSS/FS systems can be purchased pre-installed and automatic
installers result in equivalent installation labor.
Some claim that system administration costs are higher, but studies like Sun's
suggest than in many cases the system administration costs are lower,
not higher, for Unix-like systems (at least Sun's).
For example, on Unix-like systems it tends to be easier to automate tasks
(because you can, but do not need, to use a GUI) - thus over time
many manual tasks can be automated (reducing TCO).
Retraining costs can be significant - but now that GNU/Linux has modern
GUI desktop environments, there's anecdotal evidence that this cost is
actually quite small (I've yet to see serious studies quantitatively
evaluating this issue).
In short, it's often hard to show that a proprietary solution's purported
advantages really help offset their demonstrably larger costs in other
categories when there's a competing mature OSS/FS product for the
given function.
</p><p>
</p><p>
Does this mean that OSS/FS always have the lowest TCO? No! As I've
repeatedly noted, it depends on its use.
But the notion that OSS/FS <i>always</i> has the larger TCO is simply wrong.

</p><h1><a name="non_quantitative">Non-Quantitative Issues</a></h1>
<p>
In fairness, I must note that not all issues can be quantitatively measured,
and to many they are the most important issues.
The issues most important to many
include freedom, protection from license litigation, and flexibility.
Another issue that's hard to measure is innovation.

</p><p>
</p><ol>
<li>
<a name="single-source"><b>OSS/FS protects its users from the
risks and disadvantages of single source solutions.</b></a>
While ``free software'' advocates use the term ``freedom,'' and
some businesses emphasize different terms such as ``multiple sources'',
``alternate supply channels'', and ``the necessity of multiple vendors'',
the issue is the same: users do not want to be held hostage by any one vendor.
Businesses often prefer to buy products in which there is a large set
of competing suppliers, because it reduces their risk;
they can always switch to another supplier if they're not satisfied,
the supplier raises their prices substantially,
the original supplier goes out of business.
This translates into an effect on the products themselves:
if customers can easily choose and switch
between competing products, the products' prices
go down and their quality goes up.
Conversely, if there is a near monopoly for a given product, over time the
vendor will continuously raise the cost to use the product and limit its
uses to those that benefit the monopolist.
Users who are unwilling to leave single source solutions often pay
dearly later as their single source raises their costs.
<p>
Historically, proprietary vendors eventually lose to vendors selling
products available from multiple sources, even when their proprietary
technology is (at the moment) better.
Sony's Betamax format lost to VHS in the videotape market,
IBM's microchannel architecture lost to ISA in the PC architecture market,
and Sun's NeWS lost to X-windows in the networking graphics market, all
because customers prefer the reduced risk (and eventually reduced costs)
of non-proprietary products.
This is sometimes called ``commodification'', a term disparaged by
proprietary vendors and loved by users.
Since users spend the money, users eventually find someone who will provide
what they want, and then the other suppliers discover that they must
follow or give up the market area.
</p><p>
With OSS/FS, users can choose between distributors, and
if a supplier abandons them they can switch to another supplier.
As a result, suppliers will be forced to provide good quality products
and services for relatively low prices, because users can switch if they don't.
Users can even band together and maintain the product themselves
(this is how the Apache project was founded), making it possible for groups
of users to protect themselves from abandonment.
</p><p>
</p></li><li>
<a name="licensing-litigation"><b>OSS/FS protects its users
from licensing management and litigation.</b></a>
Proprietary vendors make money from the sale of licenses, and are
imposing increasingly complex mechanisms on consumers to
manage these licenses.
For example, Microsoft's Windows XP requires <a href="http://www.licenturion.com/xp/fully-licensed-wpa.txt">product activation</a> -
a scheme that means that an accumulation of hardware changes
requires a new activation code.
<p>
Proprietary vendors
also litigate against those who don't comply with their complex licensing
management requirements, creating increased legal risks for users.
For example, the Business Software Alliance (BSA) is
a proprietary software industry organization sponsored by Microsoft,
Macromedia, and Autodesk, and spends considerable time
searching for and punishing companies who cannot prove they are complying.
As noted in the
<a href="http://www.sfgate.com/cgi-bin/article.cgi?file=/gate/archive/2002/02/07/bsa.DTL">SF Gate (Feb. 7, 2002)</a>,
the BSA encourages disgruntled employees to call the BSA if they know
of any license violations.
"If the company refuses to settle or if the BSA feels the company is
criminally negligent and deliberately ripping off software,
the organization may decide to get a little nastier and organize a raid:
The BSA makes its case in front of a federal court in the company's
district and applies for a court order.
If the order is granted, the BSA can legally storm
the company's offices, accompanied by U.S. marshals,
to search for unregistered software."
<!--
"Risky Business:
Tangling with the Business Software Alliance can mean big problems"
Joyce Slaton, Special to SF Gate 
February 7, 2002 
-->
<!-- For an interesting discussion of Austin, TX's negotiations of a
license with Microsoft, see
http://www.linuxworld.com/site-stories/2001/0820.austin.html -->
</p><p>
In contrast,
OSS/FS users have no fear of litigation from the use and copying of OSS/FS.
Licensing issues do come up when OSS/FS software is modified and then
redistributed, but to be fair, proprietary software essentially forbids
this action (so it's a completely new right).
Even in this circumstance, redistributing modified OSS/FS software
generally requires following only a few simple rules (depending on the license),
such as giving credit to previous developers
and releasing modifications under the same license as the original program.
</p><p>
</p></li><li>
<a name="greater-flexibility"><b>OSS/FS has greater flexibility.</b></a>
OSS/FS users can tailor the product as necessary to meet their needs
in ways not possible without source code.
Users can tailor the product themselves, or hire whoever
they believe can solve the problem (including the original developer).
Some have claimed that this creates the ``danger of forking,'' that is,
of multiple incompatible versions of a product.
This is ``dangerous'' only to those who believe competition is evil -
we have multiple versions of cars as well.
And in practice, the high cost of maintaining software yourself has
resulted in a process in which the change is contributed back to the
community.
If it's not contributed (e.g., it solves a problem that needed solving but
only for a particular situation),
then it's still a win for the user - because
it solved a user's problem which would not have been solved otherwise.
<p>
For example,
<a href="http://www.guardian.co.uk/online/story/0,3605,267330,00.html">
in 1998 Microsoft decided against developing an Icelandic version
of Windows 95</a> because the limited size of the market couldn't justify
the cost.
Without the source code, the Islandic people had little recourse.
However, OSS/FS programs can be modified, so Icelandic support was
immedately added to them, without any need for negotiation with a vendor.
Users never know when they will have a specialized need not
anticipated by their vendor; being able to change the source code makes
it possible to support those unanticipated needs.
</p><p>
</p></li><li>
<a name="encourages-innovation"><b>There are good reasons
to believe OSS/FS encourages, not quashes, innovation.</b></a>
Microsoft publicly claims that OSS/FS (in particular its most common license,
the GPL) will eliminate innovation, but the facts undermine these claims.
Most IT managers don't believe these claims by Microsoft; in 2000
a Forrester Research study interviewed 2,500 IT managers and found that
84% of them predicted that open source software would be the spark
behind major innovations throughout the industry.
<!-- Source: "Open Source Security: A Look at the Security Benefits of
     Source Code Access" by TruSecure, August 2001, page 4. -->
Indeed, when examining <a href="http://www.dwheeler.com/innovation">the
most important software innovations</a>, it's quickly discovered that
Microsoft invented no key innovations,
nor was Microsoft the first implementor of any of them.
In fact,
<a href="http://www.dwheeler.com/innovation/microsoft.html">there
is significant evidence that Microsoft is not an innovator at all</a>.
In contrast, a number of the key innovations were OSS/FS projects.
For example,
<a href="http://news.cnet.com/news/0-1014-201-8155733-0.html">
Tim Berners-Lee, inventor of the World Wide Web,
stated in December 2001</a> that
`` A very significant factor [in widening the Web's use beyond
scientific research] was that the software was
all (what we now call) open source.
It spread fast, and could be improved fast - and it
could be installed within government and large industry
without having to go through a procurement process.''
Note that this didn't end after the ideas were originally developed;
the #1 web server in 2001 (Apache) is open source and the #2 web browser
in 2001 (Netscape Navigator) is almost entirely open source, ten years
after the original development of the web.
Indeed, recent court cases
give strong evidence that the only reason the proprietary
Internet Explorer was the #1
web browser was due to years of illegal use of monopoly power by Microsoft.
<p>
This history of innovation shouldn't be surprising;
OSS/FS approaches are based on the
scientific method, allowing anyone to make improvements or add
innovative techniques and then make them immediately available to the public.
<a href="http://linuxtoday.com/stories/8242.html">Eric Raymond
has made a strong case for why innovation is more likely, not less likely,
in OSS/FS projects</a>.
The <a href="http://sweetcode.org/">Sweetcode</a> web site
reports on innovative free software.
Here's what Sweetcode says about their site:
``Innovative means that the software reported here isn't just a
clone of something else or a minor add-on to something else or a
port of something else or yet another implementation of
a widely recognized concept...
Software reported on sweetcode should surprise you in some interesting way.''
</p><p>
If Microsoft's proprietary approaches were better for research, then
you would expect that to be documented in the research community.
However, the opposite is true;
the paper
<a href="http://www.dyncorp-is.com/darpa/meetings/win98aug/wars.html">``NT Religious Wars: Why Are DARPA Researchers Afraid of Windows NT?''</a>
found that, in spite of strong pressure by paying customers,
computer science researchers strongly resisted basing research on Windows.
Reasons given were: developers believe Windows is terrible,
Windows really is terrible, Microsoft's highly restrictive
non-disclosure agreements are at odds with researcher agendas,
and there is no clear technology transition path
for operating system
and network research products built on Windows (since only Microsoft
can distribute changes to its products).
Microsoft's own secret research (later leaked as
<a href="http://www.opensource.org/halloween/halloween1.html">``Halloween I''</a>)
found that
``Research/teaching projects on top of Linux are
easily `disseminated' due to the wide availability of Linux source.
In particular, this often means that new research ideas are
first implemented and available on Linux before they are
available / incorporated into other platforms.''
<!--
Microsoft research also believed that
OSS had weaknesses for innovation:
(1) ``future innovations which require changes to the
core architecture / integration model are going
to be incredibly hard for the OSS team to aborb
because it simultaneously devalues their precedents and skillsets''
http://www.opensource.org/halloween/halloween1.html#comment15

(2) They also claim that OSS is structured more from 20:20 hindsight
than strong, visionary leadership
http://www.opensource.org/halloween/halloween1.html#comment6

Both these claims are nonsense; see Raymond's counter to such claims.
There are many OSS/FS projects that approach problems in truly new ways, and
OSS/FS also has its visionaries (who are hailed as such).
-->
Stanford Law School professor Lawrence Lessig
(the "special master" in Microsoft's antitrust trial)
noted that
<a href="http://news.cnet.com/news/0-1014-201-7921483-0.html">
``Microsoft was using its power to protect itself against new innovation''</a>
and that Microsoft's practices generally threaten technical innovation -
not promote it.
</p><p>
Given an entire site dedicated to linking to innovative
OSS/FS projects, OSS/FS's demonstrated history in key innovations,
Microsoft's failure to demonstrate innovation itself,
reports from IT managers supporting OSS/FS, reports of
dissatisfaction by researchers and others
about Microsoft's proprietary approaches,
and Microsoft's own research finding
that new research ideas are often first implemented and available
on Linux before other platforms,
the claim that OSS/FS quashes innovation is demonstrably false.
</p></li></ol>
<p>
While I cannot quantitatively measure these issues, these
issues (particularly the first three)
are actually the most important issues to many.
</p><p>

</p><h1><a name="fears">Unnecessary Fears</a></h1>
Some avoid OSS/FS, not because of the issues noted earlier, but
because of unnecessary fears of OSS/FS.
Let's counter some of them:

<ol>
<li>
<a name="ossfs-better-supported"><b>Is proprietary software fundamentally
better supported than OSS/FS? No.</b></a>
There are actually two kinds of support for OSS/FS: traditional
paid-for support and informal community support.
There are many organizations who provide traditional support for a fee;
since these can be competed (an option not available for proprietary
software), you can often get an excellent price for support.
For example,
many GNU/Linux distributions include installation support when you
purchase their distribution, and for a fee they'll provide additional
levels of support.
As an alternative, you can also
get unpaid support from the general community of users and developers through
newsgroups, mailing lists, web sites, and other electronic forums.
While this kind of support
is non-traditional, many have been very satisfied with it.
Indeed, in 1997 InfoWorld awarded the ``Best Technical Support'' award
to the ``Linux User Community,''
beating all proprietary software vendors' technical support.
Many believe this is a side-effect of the Internet's pervasiveness -
increasingly users and developers are directly communicating with each other
and finding such approaches to be more effective than the alternatives
(for more on this business philosophy, see
<a href="http://www.cluetrain.com/">The Cluetrain Manifesto</a>).
Using this non-traditional approach effectively for support
requires following certain rules; for more on these rules, consult
<a href="http://www.tuxedo.org/%7Eesr/faqs/smart-questions.html">``How
to ask smart questions''</a>.
But note that there's a choice; using OSS/FS does not require you
to use non-traditional support (and follow its rules),
so those who want guaranteed traditional
support can pay for it just as they would for proprietary software.
<p>
</p></li><li>
<a name="ossfs-gives-more-legal-rights">
<b>Does proprietary software give you more legal rights than OSS/FS? No.</b></a>
Some have commented that ``with OSS/FS you give
up your right to sue if things go wrong.''
The obvious retort is that essentially all proprietary software licenses
<i>also</i> forbid lawsuits - so this isn't a difference at all!
Anyone who thinks that they can sue Microsoft or other shrink-wrap
proprietary vendors when things go wrong is simply fooling themselves.
In any case, most users aren't interested in sueing vendors -
they want working systems.
See
<a href="http://www.linuxjournal.com/article.php?sid=5073">
``A Senior Microsoft Attorney Looks at Open-Source Licensing''</a>, where
Bryan Pfaffenberger argues that
``With open-source software...
you are, in principle, walking into the deal with your eyes wide open.
You know what you're getting, and if you don't, you can find someone who does.
Open-source licenses enable the community of users
to inspect the code for flaws and to trade knowledge about such flaws,
which they most assuredly do. Such licenses allow
users to create derivative versions of the code that repair
potentially hazardous problems the author couldn't foresee.
They let users determine whether the program contains adequate safeguards
against safety or security risks. In contrast, the
wealthy software firms pushing UCITA are asking us
to buy closed-source code that may well contain flaws, and even
outright hazards attributable to corporate negligence -- but they won't
let us see the code, let alone modify it. You don't
know what you're getting.''
Finally, if the software goes wrong and it's very important, you can
fix it yourself or pay to have it fixed; this option greatly reduces
risk, and this option doesn't exist for proprietary software.
<p>
</p></li><li>
<a name="ossfs-protects-from-abandonment">
<b>Does OSS/FS expose you to greater risk of abandonment? No.</b></a>
Businesses go out of business, and individuals lose interest in products,
in both the proprietary and OSS/FS world.
A major difference, however, is that all OSS/FS programs are automatically
in escrow - that is, if their original developer stops supporting the
product, any person or group can step forward to support it instead.
This has been repeatedly demonstrated in OSS/FS.
For example, the
<a href="http://www.gimp.org/%7Esjburges/gimp-history.html">
GIMP is a bitmapped graphical editor that was abandoned by its
original developers</a> (what's worse, they abandoned it
before its initial release and failed to arrange for anyone else
to succeed them).
Nevertheless, even in this worst-case situation, after a period of time
other users came forward and continued its development.
As another example,
<a href="http://apache.rcbowen.com/ApacheServer.html#Introduction_What_is_Apache">
NCSA abandoned its web server ``httpd'', so some of its users banded
together to maintain it - its results became Apache, the world's most popular
web server</a>.
<p>
</p></li><li>
<a name="ossfs-economically-viable">
<b>Is OSS/FS economically viable? Yes.</b></a>
There are companies that are making money on OSS/FS, or using OSS/FS
to support their money-making activities.
Many papers have been written about how to make money using OSS/FS, such as
<a href="http://www.tuxedo.org/%7Eesr/writings/magic-cauldron/">
Eric S. Raymond's ``The Magic Cauldron''</a> and
<a href="http://www-106.ibm.com/developerworks/linux/library/license.html?dwzone=linux">Donald K. Rosenberg's ``How to make money
with open-source software.''</a>
OSS/FS isn't compatible with some business models, but capitalism does
not guarantee that businesses can remain unchanged in changing environments.
<p>
Also, looking only at companies making money from OSS/FS misses
critical issues, because that analysis
looks only at the supply side and not the demand side.
Consumers are saving lots of money and gaining many other benefits
by using OSS/FS, so there is a strong economic basis for its success.
Anyone who is saving money will fight to keep the savings, and
it's often cheaper
for consumers to work together to pay for small improvements in an OSS/FS
product than to keep paying and re-paying for a proprietary product.
For many, money is still involved - but it's money saved, not money
directly acquired as profit.
Some OSS/FS vendors have done poorly financially - but many
proprietary vendors have also done poorly too.
Luckily for consumers, OSS/FS products are not tied
to a particular vendor's financial situation as much
as proprietary products are.
</p><p>
Fundamentally, software is economically different than physical goods;
it is infinitely replicable, it costs essentially nothing to reproduce,
and it can be developed by thousands of programmers working together
with little investment
(driving the per-person development costs down to very small amounts).
Thus, the marginal cost of deploying a copy of a software package quickly
approaches zero.
This explains how Microsoft got so rich so
quickly (by selling a product that costs nearly nothing to replicate),
and why many OSS/FS developers can afford to give software away.
See
<!-- Was at: http://www.freeos.com/articles/4087 -->
<a href="http://www.mech.kuleuven.ac.be/%7Ebruyninc/linux/economy-oss.html">``Open Source-onomics:
Examining some pseudo-economic arguments about Open Source''</a>
by Ganesh Prasad, which counters ``several myths about the economics
of Open Source.''
</p><p>
</p></li><li>
<a name="wont-destroy-industry">
<b>Will OSS/FS destroy the software industry or developers? No.</b></a>
It's certainly possible that many OSS/FS products will eliminate their
proprietary competition, but that's the nature of competition.
No one mourns the loss of buggy whip manufacturers, who were driven
out of business by a superior approach to transportation (cars).
If OSS/FS approaches pose a significant threat to proprietary development
approaches, then proprietary vendors need to either find ways to compete
or join the OSS/FS movement.
As far as developers go, OSS/FS doesn't require that developers work for
free; many OSS/FS products are developed or improved by employees
(whose job is to do so) and/or by contract work
(who contract to make specific improvements in OSS/FS products).
Indeed, there has been a recent shift in OSS/FS away from volunteer
programmers and towards paid development by experienced developers.
Again, see
<a href="http://www.freeos.com/articles/4087">Ganesh Prasad's article</a>
for more information.
<p>
Karen Shaeffer has written an interesting piece,
<a href="http://www.neuralscape.com/bmodels/disrupt.html">Prospering
in the Open Source Software Era</a>,
which discusses what she views to be the effects of OSS/FS, for example,
it has the disruptive effect of commoditizing what used to be proprietary
property and it invites innovation (as compared to proprietary software
which constrained creativity).
She believes the big winners will be end users and the software developers,
because
``the value of software no longer resides in the code base -- it
resides in the developers who can quickly adapt and extend
the existing open source code to enable businesses to
realize their objectives concerned with emerging opportunities.
This commoditization of source code represents a quantum step forward
in business process efficiency -- bringing the developers
with the expertise into the business groups who have the innovating ideas.''

</p><p>
</p></li><li>
<a name="ossfs-is-compatible-with-capitalism">
<b>Is OSS/FS compatible with Capitalism? Yes.</b></a>
Years ago some tried to label OSS/FS as ``communistic''
or ``socialistic'' (i.e., anti-capitalist), but that rhetoric has failed.
One article explaining why OSS/FS and capitalism are compatible is
<a href="http://www.osopinion.com/perl/story/?id=9748">
<i>How Does the Capitalist View Open Source?</i></a>.
This paper shows that OSS/FS is quite consistent with capitalism: it
increases wealth without violating principles of property ownership
or free will.
See the earlier notes on economic viability.
<p>
</p></li><li>
<a name="ossfs-wont-destroy-ip">
<b>Is OSS/FS a ``destroyer of intellectual property''?  No.</b></a>
You can use OSS/FS products (e.g., a word processor)
to develop private and proprietary information, and you can keep
the information as confidential and proprietary as you want.
What you can't do is use someone else's material in
a way forbidden by law... and this is true for all software, not just OSS/FS.
One interesting case is the ``General Public License'' (GPL), the most common
OSS/FS license.
Software covered by the GPL can be modified, but any release of that modified
software must include an offer for the source code under the same GPL license.
<p>
Microsoft complains that the GPL does not allow them to take such code
and make changes that it can keep proprietary, but this is hypocritical.
Microsoft doesn't allow others to make and distribute changes to
Microsoft software <i>at all</i>,
so the GPL grants far <i>more</i> rights to customers than Microsoft does.
Even more amusingly, customers often pay <i>more</i> to Microsoft
to receive these fewer rights.
</p><p>
In some cases Microsoft will release source code under its
``shared source'' license, but that license (which is not OSS/FS) prohibits
distributing software in source or object form
for commercial purposes under any circumstances.
<!-- http://www.microsoft.com/windows/embedded/ce.net/previous/downloads/source/license.asp -->
Examining Microsoft's shared source license also shows that it
has even more stringent restrictions on intellectual property rights.
For example, it states that ``if you sue anyone over patents that you think may
apply to the Software for a person's use of the Software,
your license to the Software ends automatically,'' and
the patent rights Microsoft is licensing only apply
to the Software, not to any derivatives you make.''
A longer analysis of this license, and the problems it causes developers,
is available at
<a href="http://www.shared-source.org/">http://www.shared-source.org</a>;
the FSF has also posted a press release on why they believe
the <a href="http://www.gnu.org/press/2001-05-04-GPL.html">GPL protects
software freedoms</a>.
</p><p>
It's true that organizations that modify GPL'ed software must yield any
patent and copyright rights for those additions they release, but
such organizations do so voluntarily (no one can <i>force</i> anyone
to modify GPL code) and with full knowledge (all GPL'ed software comes
with a license clearly stating this).
And such grants only apply to those particular modifications;
organizations can hold other unrelated rights if they wish to do so.
Since organizations can't make such changes at all to proprietary software
in most circumstances, and generally can't redistribute changes in the few
cases where it <i>can</i> make changes,
this is a fair exchange, and organizations get far more rights with the GPL
than with proprietary licenses (including the ``shared source'' license).
</p><p>
Besides,
<a name="microsoft-sells-gpl">
<i>Microsoft sells a product which has GPL'ed components</i></a>.
<a href="http://www.microsoft.com/windows2000/interix/default.asp#section6">
Microsoft's Interix product</a> provides
an environment which can run UNIX-based applications and scripts
on the Window NT and Windows 2000 operating systems.
There's nothing wrong with this; clearly, there are a lot of Unix
applications, and since Microsoft wants to sell its operating systems
it decided to sell a way to run those applications on its own products.
But many of the components of Interix are covered by the GPL;
<a href="ftp://ftp.microsoft.com/developr/interix/GPL.TXT">
see Microsoft's ftp site to see the list of Interix components that are
covered by the GPL, along with a copy of the GPL text</a>
(<a href="http://www.dwheeler.com/frozen/microsoft-interix-gpl.txt">here
is my local copy</a>).
The problem is not what Microsoft is actually doing, but what it's saying.
Microsoft says no one should use the GPL, while simultaneously
making money using the GPL.
Bradley Kuhn (of the FSF) bluntly said, ``It's hypocritical for them to
benefit from GPL software and criticize it at the same time.''
Microsoft is certainly aware of this use of the GPL;
even Microsoft Senior Vice President Craig Mundie
acknowledged this use of GPL software.
<a href="http://www.osopinion.com/perl/story/11454.html">
Kelly McNeill reported this on June 22, 2001</a>,
and when I re-checked on April 23, 2002 this was still true.
<!--
Microsoft Exposed With GPL'd Software!
Contributed by Kelly McNeill
osOpinion.com
June 22, 2001
Also at:
http://www.newsfactor.com/perl/story/11454.html
-->
<a href="http://www.thestandard.com/article/0,1902,27511,00.html">A more
detailed description about this use of the GPL by
Microsoft is given in The Standard on June 27, 2001</a>.
<!--
Microsoft Uses Open Source, Despite Critical Stance 
By IDG
Issue Date: Jun 27 2001

Despite its aggressive criticism of the open source movement, the
software giant has quietly been publishing source code for one of its
own products for the past two years.
-->
Perhaps in the future Microsoft will try to remove many of these GPL'ed
components so that this embarrassing state of affairs won't continue.
But even if these components are removed
in the future, this doesn't change the fact
that Microsoft has managed to sell products that include
GPL-covered code without losing any of its own intellectual property rights.
</p><p>
<a name="is-gpl-virus">Although the GPL
is sometimes called a ``virus'' by proprietary vendors</a>
because of the way it encourages others to also use the GPL license,
it's only fair to note that
<a name="proprietary-also-virus">many proprietary products
also have virus-like effects</a>.
Many proprietary products with proprietary data formats or protocols
have ``network effects,'' that is,
once many users begin to use that product, that group
puts others who don't use the same product at a disadvantage.
For example, once some users pick a particular product
such as a proprietary operating system or
word processor, it becomes increasingly
difficult for other users to use a different product.
Over time this enforced use of a particular proprietary product
also spreads ``like a virus.''
</p><p>
Certainly many technologists and companies don't believe Microsoft
that the GPL will destroy their businesses.
Many seem too busy mocking Microsoft's claims instead
(for an example, see
<a href="http://www.theregister.co.uk/content/4/19836.html">
John Lettice's June 2001 article `` Gates: GPL will eat your economy,
but BSD's cool''</a>).
</p><p>
Perhaps Microsoft means the GPL ``destroys'' intellectual property
because the owners of competing software may be driven out of business.
If so, this is hypocritical; Microsoft has driven many companies out
of business, or bought them up at fractions of their original price.
Indeed, sometimes the techniques that Microsoft used have
later been proven in court to be illegal.
In contrast, there are excellent reasons to believe that
<a href="http://moglen.law.columbia.edu/publications/lu-12.html">the GPL
is on very solid legal ground</a>.
``Destruction'' of one organization by another through legal competition
is quite normal in capitalistic economies.
</p><p>
The GPL does not ``destroy'' intellectual property; instead, it creates
a level playing field where people can contribute improvements voluntarily
to a common project without having them ``stolen'' by others.
You could think of the GPL as creating a consortium; no one is
required to aid the consortium, but those who do must play by its rules.
It's understandable that Microsoft would want to take this consortium's
results and take sole ownership of derivative works,
but there's no reason to believe that a world where the GPL cannot be used
is really in its customers' best interests.
</p><p>
</p></li><li>
<!-- Someday I may merge this with the non-quantitative issues,
particularly the "single source" and "flexibility" comments.
-->
<a name="source-access-is-important"><b>Is having the ability
to view and change source code really
valuable/important for many people? Surprisingly, yes.</b></a>
It's certainly true that few people need <i>direct</i> access to source code;
only developers or code reviewers need the ability to access and change code.
But not having access to how your computer is controlled is still
a significant problem.
Bob Young of Red Hat uses the analogy of
<a href="http://zdnet.com.com/2100-11-520393.html?legacy=zdnn" name="car-hood-welded-shut">
having your car's hood welded shut</a> to explain why even
non-technical users need access to the source code.
Here is his explanation, in his own words:
<blockquote>
Open source gives the user the benefit of control over the
technology the user is investing in...
The best analogy that illustrates this benefit is with the way we buy cars.
Just ask the question, "Would you buy a car with the hood welded shut?"
and we all answer an emphatic "No."
So ask the follow-up question, "What do you know about
modern internal-combustion engines?" and the answer for most
of us is, "Not much."
<p>
We demand the ability to open the hood of our cars because it gives us,
the consumer, control over the product we've bought and takes it away
from the vendor.
We can take the car back to the dealer; if he does a good job,
doesn't overcharge us and adds the features we need,
we may keep taking it back to that dealer.
But if he overcharges us, won't fix the problem we are
having or refuses to install that musical horn we always wanted -- well,
there are 10,000 other car-repair companies that would be
happy to have our business.
</p><p>
In the proprietary software business, the customer has no
control over the technology he is building his business around.
If his vendor overcharges him, refuses to fix the bug
that causes his system to crash or chooses not to introduce
the feature that the customer needs, the customer has no choice.
This lack of control results in high cost,
low reliability and lots of frustration.
</p></blockquote>
<p>
To developers, source code is critical.
Source code isn't necessary to break the security of most systems, but
to really fix problems or add new features it's quite difficult without it.
<a name="gates-used-os-source">Microsoft's Bill Gates
has often claimed that most developers don't need
access to operating system source code</a>, but
<a href="http://www.theregister.co.uk/content/archive/11671.html">
Graham Lea's article
``Bill Gates' roots in the trashcans of history''</a>
exposes that Gates actually extracted operating system source code from other
companies by digging through their trash cans.
Mr. Gates said,
``I'd skip out on athletics and go down to this computer center.
We were moving ahead very rapidly: Basic, FORTRAN, LISP,
PDP-10 machine language, digging out the operating system listings
from the trash and studying those.''
</p><p>
See also the discussion on the
<a href="#greater-flexibility">greater flexibility</a> of OSS/FS.
</p><p>
</p></li><li>
<a name="anti-microsoft"><b>Is OSS/FS really just an anti-Microsoft
campaign? No.</b></a>
Certainly there are people who support OSS/FS who are against Microsoft,
but it'd be a mistake to view OSS/FS as simply anti-Microsoft.
Microsoft could, at any time, release its operating system as OSS/FS,
take an existing OSS/FS operating system and release it,
or provide applications for OSS/FS systems.
There is no licensing agreement that prevents this.
Indeed,
OSS/FS leaders often note that they are not against Microsoft per se, just
some of its current business practices, and many have repeatedly asked
Microsoft to join them
(e.g., see
<a href="http://perens.com/Articles/StandTogether.html">Free Software Leaders
Stand Together</a>).

<p>
</p></li><li>
<a name="contribute-with-code"><b>I've always assumed there's no free lunch;
isn't there some catch?</b></a>
If there is an OSS/FS product that meets your needs, there really isn't
a catch.
Naturally, if you want services besides the software
itself (such as guaranteed support, training, and so on),
you'll need to pay for those things just like you would for
proprietary software.
However, if you want to affect the future direction of the software -
particularly if you need to have the software changed in some way -
then you have to invest to create those modifications.
Typically these investments involve hiring someone to make those changes,
possibly sharing the cost with others who need the change.
Note that you only need to pay to make a change to the software -
you don't need to pay to use the software, or a per-copy fee, only
the actual cost of the changes.
<p>
For example, when IBM wanted to join the Apache group, IBM discovered there
really wasn't a mechanism to pay in money.
IBM soon realized that the primary ``currency'' in OSS/FS is
software code, so IBM turned the money into code and all turned out very well.
</p><p>
This also leads to interesting effects that explains why many OSS/FS
projects start small for years, then suddenly leap into a mode where they
have a rapidly increasing functionality and user size.
For any particular application, there is a minimum level of acceptable
functionality; below this, there will be very few users.
If that minimum level is large enough, this creates an effect
similar to an ``energy barrier'' in physics;
the barrier can be large enough that most users are not willing to
pay for the initial development of the project.
However, at some point, someone may decide to begin the ``hopeless''
project anyway.
The initial work may take a while, because the initial work is large and
there are few who will help.
However, once a minimum level of functionality is reached,
a few users will start to use it, and a few of them may be willing to help
(e.g., because they want the project to succeed or because they have
specialized needs).
At some point in this growth, it is like passing an energy barrier;
the process begins to become self-sustaining and exponentially increasing.
As the functionality increases, the number of potential users begins to
increase rapidly, until suddenly the project is sufficiently usable for
many users.
A percentage of the userbase will decide to add new features, and
as the userbase grows, so do the number of developers.
As this repeats, there is an explosion in the program's capabilities.
</p></li></ol>

<h1><a name="other">Other Information</a></h1>
<p>
Here are some other related information sources:
</p><ol>
<li><a name="usereports">There are many reports from users who have
switched to OSS/FS</a> that you may find useful.
Oracle's Chairman and CEO, Larry Ellison, said that
<a href="http://www.computerworld.com/cwi/story/0,1199,NAV47_STO67867,00.html">
Oracle will
switch to GNU/Linux to run the bulk of its business applications</a>
no later than summer 2002,
replacing three Unix servers.
<a href="http://news.com.com/2100-1017-827366.html">
Online brokerage E*Trade is moving its computer systems
to IBM servers running GNU/Linux</a>,
citing cost savings and performance as reasons for switching to GNU/Linux
(the same article also notes that
clothing retailer L.L. Bean and financial services giant Salomon Smith Barney
are switching to GNU/Linux as well).
<a href="http://www.forbes.com/home/2002/03/27/0327linux.html">Merrill Lynch</a>
is switching to GNU/Linux company-wide,
and are hoping to save tens of millions of dollars annually
within three to five years. 
<a href="http://li.org/">Linux International</a>
has a set of
<a href="http://li.org/success/">Linux case studies/success stories</a>.
<!-- E*Trade also noted in
      http://www.informationweek.com/story/IWK20020201S0026 -->
Mandrakesoft maintains
<a href="http://www.mandrakebizcases.com/">a site 
recording the experiences of business users
of the Mandrake distribution</a>.
<a href="http://www.redhat.com/solutions/migration/#tools">
Red Hat provides some similar information</a>.
Opensource.org includes some
<a href="http://www.opensource.org/advocacy/case_studies.html">case
studies</a>.
Obviously, the various
large-scale roll-outs that have occurred
suggest that OSS/FS really is viable for enterprise deployments.
Many retailer cash registers are switching to GNU/Linux, according to
<!-- http://www.techweb.com/se/directlink.cgi?IWK20001204S0069 -->
Information Week
("Cash Registers are Ringing up Sales with Linux" by Dan Orzech,
December 4, 2000, Issue 815).
<!-- Home Depot planned to roll out 90,000 terminals running Linux by 2003,
but their management changed and then changed directions.
It doesn't appear to have been a technical problem.
See:
http://www.ihlservices.com/en/2002Press4.asp
as referenced by:
http://linuxtoday.com/news_story.php3?ltsn=2002-03-28-005-26-NW-EM-MR&amp;tbovrmode=1#talkback_area
-->
According to Bob Young (founder of Red Hat),
<a href="http://zdnet.com.com/2100-1104-828802.html">BP
(the petroleum company) is putting 3,000 Linux servers at gas stations</a>.
There are many other reports from users, such as the report
<a href="http://www.robval.com/linux/desktop/index.asp"><i>Linux
as a Replacement for Windows 2000</i></a>
and the results from
<a href="http://news.cnet.com/news/0-1003-200-7720536.html?tag=owv">
Amazon.com.</a>
<!--
http://www.newsalert.com/bin/story?StoryId=Co_Ir0c4bmdaWmtK
for the USJTF; link appears gone.
-->
The U.S. Joint Strike Fighter (JSF) is using GNU/Linux, too.
<a href="http://techupdate.zdnet.com/techupdate/stories/main/0,14179,2825019,00.html">A travel application service provider</a>
saved $170,000 in software costs during the first six months
of using GNU/Linux (for both servers and the desktop);
it also saved on hardware and reported that
administration is cheaper too.
<a href="http://www.crn.com/Sections/CoverStory/CoverStory.asp?ArticleID=31793">CRN's Test Center</a> found that
a GNU/Linux-based network (with a server and 5 workstations)
cost 93% less in software than a Windows-based network,
and found it to be quite capable.
The <a href="http://www.theregister.co.uk/content/4/23667.html">Korean
government</a> announced that it plans to buy 120,000 copies of
Hancom Linux Deluxe this year, enough to switch 23% of its installed base
of Microsoft users to open source equivalents;
by standardising on Linux and HancomOffice,
the Korean government expects to make savings of 80% compared
with buying Microsoft products. 
<a href="http://desktoplinux.com/articles/AT9664091996.html">Adam Wiggins
reports on TrustCommerce's successful transition to Linux on the desktop.</a>
<a href="http://zdnet.com.com/2100-1104-887961.html">An April 22, 2002 report
on ZDNet, titled ``More foreign banks switching to Linux''</a>,
stated that New Zealand's TSB bank
``has become the latest institution to adopt
the open-source Linux operating system.
According to reports, the bank is to
move all its branches to the Linux platform...
in Europe, BP and Banca Commerciale Italiana feature among
the big companies that have moved to Linux.
According to IBM, as many as 15 banks in central London
are running Linux clusters.
Korean Air, which now does all its ticketing on Linux,
and motorhome manufacturer Winnebago, are high-profile examples.''
<p>
OSS/FS is also prominent in Hollywood.
Back in 1996, when GNU/Linux was considered by some to be a risk,
<a href="http://www.linuxjournal.com/article.php?sid=2494">Digital
Domain used GNU/Linux to generate many images in <i>Titanic</i></a>.
After that, it burst into prominence as many others began using it,
so much so that a
<!--
"Linux in Hollywood: A Star Is Born" by Michael Macedonia, US Army Stricom
-->
<a href="http://www.computer.org/computer/homepage/0202/ec/">February 2002
article in IEEE Computer</a> stated that
``it is making rapid progress toward becoming the dominant operating system
in ... motion pictures.''
``Shrek'' and ``Lord of the Rings'' used GNU/Linux to power their
server farms, and now
<a href="http://newsforge.com/newsforge/02/04/24/1643238.shtml?tid=23">DreamWorks SKG
has switched to using GNU/Linux exclusively on both the front and
back ends for rendering its movies.</a>



</p></li><li><a name="ms-claims-oss-dangerous">Microsoft has been trying
to claim that open source is somehow dangerous</a>,
and indeed is its leading critic, yet the
Wall Street Journal's Lee Gomes found that
<!-- was at: http://public.wsj.com/news/hmc/sb992819157437237260.htm.
     Published in 2001. -->
``Microsoft Uses Open-Source Code Despite Denying Use of such Software.''
Here are some interesting quotes from his article:
<blockquote>
... But Microsoft's statements Friday suggest the company has itself been
taking advantage of the very technology it has insisted would bring dire
consequences to others. ``I am appalled at the way Microsoft bashes open
source on the one hand, while depending on it for its business on the
other,'' said Marshall Kirk McKusick, a leader of the FreeBSD development team.
</blockquote>
More recently Microsoft has particularly targeted the GPL license
rather than all open source licenses, claiming that the GPL
is somehow anti-commercial.
But this claim lacks evidence, given the large number of commercial
companies (e.g., IBM, Sun, and Red Hat) who are using the GPL.
Also, see this paper's earlier note that
<a href="#microsoft-sells-gpl">Microsoft itself
makes money by selling a product with GPL'ed components</a>.
The same article closes with this statement:
<blockquote>
In its campaign against open-source, Microsoft has been unable to come up
with examples of companies being harmed by it. One reason, said Eric von
Hippel, a Massachusetts Institute of Technology professor who heads up a
research effort in the field, is that virtually all the available evidence
suggests that open source is ``a huge advantage'' to companies. ``They are
able to build on a common standard that is not owned by anyone,'' he said.
``With Windows, Microsoft owns them.''
</blockquote>
Other related articles include
<a href="http://news.com.com/2010-1078-855155.html">Bruce Peren's comments</a>,
<a href="http://www.osopinion.com/perl/story/?id=9748">
Ganesh Prasad's <i>How Does the Capitalist View Open Source?</i></a>,
and the open letter
<a href="http://perens.com/Articles/StandTogether.html"><i>Free
Software Leaders Stand Together</i></a>.


</li><li><a name="general-ossfs">There are several general
information sites about OSS/FS or Unix that might be of interest</a>,
such as the
<a href="http://www.fsf.org/">Free Software Foundation (FSF)</a>, the
<a href="http://www.opensource.org/">Open Source Initiative website</a>, and the
<a href="http://www.linux.org/info/advocacy.html">Linux.org site</a>.
An older paper is
<a href="http://www.kirch.net/unix-nt">John Kirch's paper,
<i>Microsoft Windows NT Server 4.0 versus UNIX</i></a>.
(<a href="http://web.archive.org/web/20010801155417/www.unix-vs-nt.org/kirch/">
also archived at the Internet Archives</a>).
<!-- http://www.unix-vs-nt.org Unix versus NT site is dead -->
The paper
<a href="http://www.yoderdev.com/oss-future.html">Our Open Source / Free Software Future: It's Just a Matter of Time</a>
argues that within the next few years, the standard
de-facto operating system that nearly everyone uses,
as well as much of the commodity software in widespread use,
will be OSS/FS.
The book
<a href="http://www.tuxedo.org/%7Eesr/writings/cathedral-bazaar">
<i>The Cathedral and the Bazaar</i></a> by Eric Raymond
examines OSS/FS development processes and issues.
A useful collection of many OSS/FS writings, including
the essay <i>The Cathedral and the Bazaar</i>, is in the
<a href="http://www.csaszar.org/interesting.htm">Open Source Reader</a>.
Ganesh C. Prasad has published
<a href="http://www.osopinion.com/Opinions/GaneshCPrasad/GaneshCPrasad2.html">
The Practical Manager's Guide to Linux</a>.
You can see a collection of general information about OSS/FS at
<a href="http://www.dwheeler.com/oss_fs_refs.html">my web page
listing OSS/FS references</a>.

</li><li><a name="halloween-documents">Microsoft inadvertently
advocated OSS/FS</a> in its
leaked internal documents, called the
<a href="http://www.opensource.org/halloween">"Halloween" documents</a>.

</li><li><a href="http://www.ccic.gov/ac/pitac_ltr_sep11.html" name="pitac"><i>Recommendations
of the Panel on Open Source Software For High End Computing</i></a>;
is the report of a panel created
by the (U.S.) President's Information Technology Advisory Committee (PITAC).
It recommends that the ``Federal government should encourage the
development of open source software as an alternate path for
software development for high end computing''.

</li><li><a name="microsoft-linux-myths">Several documents were written to counter Microsoft's statements such
as those in Microsoft's "Linux Myths"</a>.
<!-- was at: http://www.microsoft.com/ntserver/nts/news/msnw/LinuxMyths.asp,
     but it appears to be gone now. -->
This includes
<a href="http://lwn.net/1999/features/MSResponse.phtml">LWN's response</a>
and
<a href="http://dolinux.dyn.dhs.org/dolinux/docs/response.html">Jamin Philip Gray's response</a>, and the
<a href="http://fud-counter.nl.linux.org/">FUD-counter site</a>.
The
<a href="http://www.shared-source.com/">shared source</a> page argues
that Microsoft's ``shared source'' idea is inferior to open source.
<a href="http://www.gnu.org/philosophy/gpl-american-way.html">Richard
Stallman's <i>The GNU GPL and the American Way</i></a>
counters the amusing claim by Microsoft that the GPL was ``un-American.''
The letter
<a href="http://perens.com/Articles/StandTogether.html">Free Software
Leaders Stand Together</a> argues against a number of statements
by Craig Mundie.
You can find many general sites about Microsoft,
including <a href="http://www.geocities.com/cloweth">Cloweth's site</a>.
<!--
Microsoft has posted a Windows vs. Linux piece at:
http://www.microsoft.com/windows/Embedded/sak/evaluation/compare/advantage.asp      
I don't want to link to this Microsoft piece, because it's so
biased and full of errors that I'm afraid a public link to it
might give it more credibility than it deserves.
Linux does support SSO, for example, even though Microsoft says it doesn't.
The fact that GNU/Linux is built as a set of separable pieces
(e.g., Samba, PHP, Perl, Python, etc.)
is considered good design, not the "flaw" Microsoft wishes it was.
Indeed, Microsoft tries to mark GNU/Linux down because GNU/Linux primarily
uses standards instead of Microsoft's proprietary approaches.
GNU/Linux supports the _functionality_ people want.
And the crack about the GPL is amusing.. if someone used Microsoft's
code, have no fear, Microsoft would prosecute in far more onerous ways
than the GPL crowd.
Jeremy, enigmax, at, toocool, (dot) com has said he will write a paper
so that people can get a balanced view of it.

-->

</li><li><a name="microsoft-unisys-anti-unix-campaign">In a story
full of ironies</a>,
<a href="http://news.com.com/2100-1001-870805.html">Microsoft and Unisys
teamed up in a well-funded marketing campaign against Unix</a>, in part
to try to revive Unisys' sagging sales of Windows-based products.
The 18-month, $25 million campaign, dubbed ``We have the Way Out,''
specifically attacked the Unix offerings of Sun, IBM, and Hewlett-Packard,
but since the major OSS/FS operating systems are Unix or Unix-like, it
attacks them as well.
In a delicious irony, it was revealed that
<a href="http://news.com.com/2100-1001-872266.html">the anti-Unix
campaign website is powered by Unix software</a> -
in this case, FreeBSD (an OSS/FS version of Unix) and
the OSS/FS Web server Apache.
Once this was publicly revealed, Microsoft and Unisys quickly switched to
a Windows-based system..  and then
<a href="http://news.com.com/2100-1001-874132.html">the website
failed to operate at all for several days</a>.
If <i>that</i> wasn't enough,
<a href="http://www.theregister.co.uk/content/53/24714.html">
Andrew Orlowski reported in <i>The Register</i></a> a
further analysis of this website,
noting that port 3306 was open on their website -
a port primarily used by MySQL and Postgres.
In other words, it appears that their anti-Unix site was still using
OSS/FS software (not Microsoft's own database) that is primarily
deployed on Unix-like systems.
Even their original imagery turns out to have had serious problems;
the campaign's original graphic
showed a floor almost entirely covered in mauve paint (Sun Microsystem's
color), and the alternative offered was to jump through a window.
<a href="http://www.theregister.co.uk/content/53/24681.html">Many
literate readers will recognize this symbol (the act of throwing
out through, or of being thrown out of, a window)
as <i>defenestration</i>, a way of killing rulers and also
a popular way of inviting kings to commit suicide in 17th century Europe</a>.
In other words, this imagery
suggests that you should use the window[s] to commit suicide (!).
<a href="http://lwn.net/2002/0411/letters.php3">Leon Brooks then
analyzed the site further</a> - and found that the ``way out'' site
used JSP (a technology fathered Sun, Unix specialists).
He also found that the site violated many standards;
the site's content failed the W3C validation suites (Microsoft
is a member of the W3C), and uses a Windows-only character set
that is not only non-standard, but actively conflicts with an 
important international standard (and ironically one which Microsoft is
actively promoting).
If using only Windows is so wonderful, how come the advocacy site can't
conform to international standards?
The real problem here, of course, is that trying to convince
people that Unix is to be avoided at all costs - while
using Unix and then having serious problems when
trying to use an alternative - is both ironic and somewhat hypocritical.
</li><li>
<a href="http://www.salon.com/tech/fsp/2000/09/12/chapter_7_part_one/index.html" name="ibm-significant-support">``How Big Blue Fell For Linux''</a>
is an article on how IBM transitioned to becoming a major backer.
IBM announced that it planned to invest $1 Billion in GNU/Linux in 2001
<i>all by itself</i>
(see the
<a href="http://www.ibm.com/annualreport/2000/flat/toc/2_3_1_intro.html">IBM
annual report</a>).
In 2002
<a href="http://news.com.com/2100-1001-825723.html">IBM reported that they
had already made almost all of the money back</a>;
I and others are a little skeptical of these claims,
but it's clear that IBM has significantly invested in GNU/Linux
and seem to be pleased with the results
(for an example, see their
<a href="http://news.com.com/2100-1001-822771.html">Linux-only mainframe</a>).
This is not just a friendly gesture, of course;
companies like
<a href="http://techupdate.zdnet.com/techupdate/stories/main/0,14179,2860394,00.html">IBM view OSS/FS software as a competitive advantage</a>, because
OSS/FS frees them from control by another organization, and it also
enables customers
to switch to IBM products and services (who were formerly locked into
competitor's products).
Thankfully, this is a good deal for consumers too.
In 2002, IBM had
<a href="http://www.consultingtimes.com/articles/ibm/frye/fryeinterview.html">250 employees working full time to make Linux better</a>.
</li><li>
For a scientifically unworthy but really funny look at what people
who <i>use</i> the various operating systems say, take a look at the
<a href="http://srom.zgp.org/">Operating System Sucks-Rules-O-Meter</a>.
It counts how many web pages make statements like ``Linux rocks''.
It's really just an opinion poll, but if nothing else it's great for a laugh.
<!-- was http://www.operatingsystems.net -->
<!--
<li>
A general-purpose site that tries to compare all operating systems
(with a bent towards Apple's MacOS) is
<a href="http://www.osdata.com">http://www.osdata.com</a>;
it has a lot of interesting information, though it tends towards testimonials
and such instead of quantitative information.

More personal surveys specifically Linux-related is
available from survey.com's
Open Source Unix (OSU) Linux Report, February 2000.
The whole report is expensive; a summary is at
http://www.survey.com/bidw/description_linux.html
-->

</li><li><a name="ossfs-developers">Several studies examine developers</a> (instead of the programs
they write), including
<a href="http://www.ibiblio.org/osrt/develpro.html">``A Quantitative
Profile of a Community of Open Source Linux Developers''</a>,
<a href="http://www.psychologie.uni-kiel.de/linux-study">Herman,
Hertel and Niedner's study (based on questionnaires)</a>, and the
<a href="http://widi.berlios.de/stats.php3">
Who Is Doing It (WIDI)</a> study.
<a href="http://floss1.infonomics.nl/stats.php">Real-time results</a>
are available from the
<a href="http://floss1.infonomics.nl/">Free/Libre Open Source Software
Survey (FLOSS)</a>.
The <a href="http://www.osdn.com/bcg">Boston Consulting Group/OSDN
Hacker Survey</a> (January 31, 2002)
made some interesting observations by randomly sampling SourceForge users
for a survey.
For example,
it gives evidence that open source developers can be divided into four
groups (based on their motivations for writing OSS/FS software):
<ul>
<li>the believers, who do it because they believe source code
should be open (33%),
</li><li>the skill enhancers, who do it for skill improvement (25%),
</li><li>the fun seekers, who do it for a non-work need and
intellectual stimulation (21%), and
</li><li>the professionals, who do it for work needs and
professional status (21%).
</li></ul>
Journalists sometimes like to romanticize OSS/FS developers as being mostly
teenage boys with little experience, but the survey didn't support
that view at all.
The study found that the open source developers surveyed are mostly
experienced professionals, having an average of 11 years of programming
experience; the average age was 28.
</li><li><a name="other-evals">Other evaluations</a> include the
<a href="http://www.gartnerweb.com/public/static/hotc/hc00091281.html">Gartner
Group</a>
and
<a href="http://gnet.dhs.org/stories/bloor.php3">GNet</a> evaluations.

</li></ol>

<p>
For general information on OSS/FS, see my
<a href="http://www.dwheeler.com/oss_fs_refs.html">list of
Open Source Software / Free Software (OSS/FS) references at
http://www.dwheeler.com/oss_fs_refs.html</a>

</p><h1><a name="conclusions">Conclusions</a></h1>
<p>
OSS/FS has significant
<a href="#market_share">market share</a>,
is often the most
<a href="#reliability">reliable software</a>,
and in many cases has the best <a href="#performance">performance</a>.
OSS/FS <a href="#scaleability">scales</a>,
both in problem size and project size.
OSS/FS software generally has far better
<a href="#security">security</a>, particularly when compared to Windows.
<a href="#tco">Total cost of ownership</a>
for OSS/FS is often far less than
proprietary software, particularly as the number of platforms increases.
These statements are not merely opinions; these effects
can be shown <i>quantitatively</i>, using a wide variety of measures.
This doesn't even consider
<a href="#non_quantitative">other issues that are hard to measure</a>,
such as freedom from control by a single source, freedom from
licensing management (with its accompanying litigation),
and increased flexibility.
I believe OSS/FS options should be carefully considered any
time software or computer hardware is needed.

</p><p>
</p><hr>
<p>

<table>
<tbody><tr>
<td>
 
</td>
<td>
<center><b>About the Author</b></center>
<br>
David A. Wheeler is an expert in computer security and has
a long history of working with large and high-risk software systems.
His books include
<a href="http://www.computer.org/cspress/catalog/bp07340.htm"><em>Software
Inspection: An Industry Best Practice</em></a> (published by IEEE CS Press),
<i>Ada 95: The Lovelace Tutorial</i> (published by Springer-Verlag),
and the
<a href="http://www.dwheeler.com/secure-programs"><i>Secure Programming
for Linux and Unix HOWTO</i></a>.
Articles he's written include
<a href="http://www.dwheeler.com/sloc"><i>More than a Gigabuck:
Estimating GNU/Linux's Size</i></a> and
<a href="http://www.dwheeler.com/innovation"><i>The Most Important Software Innovations.</i></a>
Mr. Wheeler's web site is at
<a href="http://www.dwheeler.com/">http://www.dwheeler.com</a>;
you may contact him at
<a href="mailto:dwheeler@dwheeler.com">dwheeler@dwheeler.com</a>,
but you may not send him spam (he reserves the right to charge fees
to those who send him spam).
You can view this article at
<a href="http://www.dwheeler.com/oss_fs_why.html">http://www.dwheeler.com/oss_fs_why.html</a>.
<p>
</p></td>
<td>
<img src="oss_fs_why_20020426.en_files/dwheel1.jpg" alt="Picture of David A. Wheeler" width="150" height="225">
</td>
</tr>
</tbody></table>
</p><p>
Palm PDA users can get a 
<a href="http://www.dwheeler.com/oss_fs_why.pdb">Plucker version of this document</a>; you will need
<a href="http://www.plkr.org/">Plucker</a> to read it.
You can also get
<a href="http://www.dwheeler.com/oss_fs_why.pdf">this document
in PDF format</a>.
</p><p>
<font size="2">
You may reprint this article (unchanged) an unlimited number of times
and distribute local electronic copies.
You may not ``mirror'' this document to the public Internet or other
public electronic distribution systems;
mirrors interfere with my goal of ensuring that
readers can immediately find and get the current version of this document.
Copies clearly identified as old versions and not included in normal
searches as current Internet data are fine;
examples of acceptable copies are
Google caches and the Internet archive's copies.
Please contact David A. Wheeler if you'd like to translate this article.
This article is a research article, not
software nor a software manual.
</font>

<!--
<p>
<font size="2">
PDA users can get
http://pilot.screwdriver.net/convert.prc?url=http%3A%2F%2Fwww.dwheeler.com&amp;title=Why+OSS%2FFS%3F+Look+at+the+Numbers%21
<a href="http://pilot.screwdriver.net/convert.prc?url=http%3A%2F%2Fwww.dwheeler.com">a version of this document in (Aportis) DOC format</a>;
PalmOS users can use open source DOC readers such as
<a href="http://www.32768.com/bill/palmos/cspotrun">CSpotRun</a> or
<a href="http://sourceforge.net/projects/zurk">ZDoc</a> to read it.
</font>

http://www.osopinion.com/Opinions/PaulRosenberg/analysis.pdf
A MATHEMATICAL ANALYSIS OF THE BUSINESS OF SOFTWARE
Paul Rosenberg


-->
<!-- I first posted this some time between August 23, 2000 and Oct 15, 2000;
     I've made MANY modifications since.  See the Internet Archive to see
     a few of the versions. -->
</p></body></html>