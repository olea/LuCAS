<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head>
<link rel="stylesheet" type="text/css" href="http://olea.org/estilo-web/estilo-comun.css" />
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Un paso adelante</title><meta name="generator" content="DocBook XSL Stylesheets V1.58.1" /><meta name="description" content="&#10;&#9;&#10;&#9;  Notas previas para una especificaci&#xF3;n de requisitos de&#10;&#9;  las herramientas ling&#xFC;&#xED;sticas de TLDP&#10;&#9; &#10;&#9;(La comprensi&#xF3;n plena de este documento requiere familiaridad&#10;&#9;con las herramientas ling&#xFC;&#xED;sticas o la lectura previa de &#10;&#9;doc-traducci&#xF3;n-libre &#x2014;&#10;&#9;  http://es.tldp.org/Articulos/0000otras/doc-traduccion-libre/&#10;&#9;&#x2014;, documento producido en el seno de&#10;&#9;TLDP-ES)&#10;       &#10;&#9;En este art&#xED;culo el autor presenta y justifica sus propuestas&#10;&#9;con respecto a la adopci&#xF3;n de est&#xE1;ndares libres de marcado e&#10;&#9;intercambio de informaci&#xF3;n ling&#xFC;&#xED;stica y el desarrollo de&#10;&#9;herramientas libres de ayuda a la escritura y traducci&#xF3;n,&#10;&#9;de manera que puedan someterse a discusi&#xF3;n p&#xFA;blica y adoptarse&#10;&#9;como especificaciones de TLDP. Es en cierto modo un ap&#xE9;ndice&#10;&#9;ejecutivo, las conclusiones de la exposici&#xF3;n realizada en el&#10;&#9;documento anterior. &#10;       &#10;&#9;&#10;&#9;      &#xDA;ltima versi&#xF3;n:&#10;&#9;      &#10;&#9;&#9;http://es.tldp.org/especificaciones/herramientas-linguisticas/herramientas-linguisticas/&#10;&#9;       &#10;&#9;    &#10;&#9;      Fuente:&#10;&#9;      &#10;&#9;&#9;http://cvs.hispalinux.es/cgi-bin/cvsweb/esp-herramientas-linguisticas/un-paso-adelante.xml&#10;&#9;      &#10;&#9;    &#10;      " /></head><body><div class="article" lang="es" xml:lang="es"><div class="titlepage"><div><h1 class="title"><a id="id2748631"></a>Un paso adelante</h1></div><div><h3 class="subtitle"><i>
      Plan de tecnologías lingüísticas libres
    </i></h3></div><div><div class="author"><h3 class="author">Juan Rafael Fernández García</h3></div></div><div><p class="copyright">Copyright © 2003 Juan Rafael Fernández García</p></div><div><div class="legalnotice"><p>
	Permission is granted to copy, distribute and/or modify this
        document under the terms of the GNU Free
        Documentation License, Version 1.1 or any later version
        published by the Free Software Foundation.
      </p></div></div><div><p class="pubdate">
      $Id: un-paso-adelante.xml,v 1.5 2003/09/30 21:00:00 juanfernandez Exp $ 
    </p></div><div><div class="revhistory"><table border="1" width="100%" summary="Revision history"><tr><th align="left" valign="top" colspan="3"><b>Historial de revisiones</b></th></tr><tr><td align="left">Revisión 0.9</td><td align="left">2003-09-30</td><td align="left">jrf</td></tr><tr><td align="left" colspan="3">
	  ‘Demostración’ de las insuficiencias de .po
	</td></tr><tr><td align="left">Revisión 0.8</td><td align="left">2003-09-29</td><td align="left">jrf</td></tr><tr><td align="left" colspan="3">
	  Versión ‘La luz del sol ilumina las ideas’
	</td></tr><tr><td align="left">Revisión 0.7</td><td align="left">2003-09-07</td><td align="left">jrf</td></tr><tr><td align="left" colspan="3">Versión enviada al VI Congreso Hispalinux</td></tr><tr><td align="left">Revisión 0.6</td><td align="left">2003-03-17</td><td align="left">jrf</td></tr><tr><td align="left" colspan="3">Superada tortura de xmllint, victorioso</td></tr><tr><td align="left">Revisión 0.5</td><td align="left">2003-02-26</td><td align="left">jrf</td></tr><tr><td align="left" colspan="3">La información pasa a doc-traduccion-libre</td></tr><tr><td align="left">Revisión 0.4</td><td align="left">2003-02-25</td><td align="left">jrf</td></tr><tr><td align="left" colspan="3">Modificaciones menores del marcado</td></tr><tr><td align="left">Revisión 0.3</td><td align="left">2003-02-24</td><td align="left">jrf</td></tr><tr><td align="left" colspan="3">Revisión del xml</td></tr><tr><td align="left">Revisión 0.2</td><td align="left">2003-02-20</td><td align="left">io</td></tr><tr><td align="left" colspan="3">Primera versión xml</td></tr><tr><td align="left">Revisión 0.1</td><td align="left">2002-17-12</td><td align="left">jrf</td></tr><tr><td align="left" colspan="3">Versión inicial (txt)</td></tr></table></div></div><div><div class="abstract"><p class="title"><b>Resumen</b></p><p>
	<span class="emphasis"><em>
	  Notas previas para una especificación de requisitos de
	  las herramientas lingüísticas de TLDP
	</em></span> 
	(La comprensión plena de este documento requiere familiaridad
	con las herramientas lingüísticas o la lectura previa de 
	doc-traducción-libre —<a href="http://es.tldp.org/Articulos/0000otras/doc-traduccion-libre/" target="_top">
	  http://es.tldp.org/Articulos/0000otras/doc-traduccion-libre/
	</a>—, documento producido en el seno de
	<span class="emphasis"><em>TLDP-ES)</em></span>
      </p><p>
	En este artículo el autor presenta y justifica sus propuestas
	con respecto a la adopción de estándares libres de marcado e
	intercambio de información lingüística y el desarrollo de
	herramientas libres de ayuda a la escritura y traducción,
	de manera que puedan someterse a discusión pública y adoptarse
	como especificaciones de TLDP. Es en cierto modo un apéndice
	ejecutivo, las conclusiones de la exposición realizada en el
	documento anterior. 
      </p><p>
	</p><div class="itemizedlist"><ul type="disc"><li><p>
	      Última versión:
	      <a href="http://es.tldp.org/especificaciones/herramientas-linguisticas/herramientas-linguisticas/" target="_top">
		http://es.tldp.org/especificaciones/herramientas-linguisticas/herramientas-linguisticas/
	      </a> 
	    </p></li><li><p>
	      Fuente:
	      <a href="http://cvs.hispalinux.es/cgi-bin/cvsweb/esp-herramientas-linguisticas/un-paso-adelante.xml" target="_top">
		http://cvs.hispalinux.es/cgi-bin/cvsweb/esp-herramientas-linguisticas/un-paso-adelante.xml
	      </a>
	    </p></li></ul></div><p>
      </p></div></div><hr /></div><div class="toc"><p><b>Tabla de contenidos</b></p><dl><dt><a href="#sec-intro">Introducción</a></dt><dd><dl><dt><a href="#sec-intro-porque">¿Por qué?</a></dt><dt><a href="#sec-intro-propuesta">Propuesta</a></dt><dt><a href="#sec-intro-okapi">Un proyecto paralelo: Okapi Framework</a></dt></dl></dd><dt><a href="#parte-al-grano">Al grano</a></dt><dd><dl><dt><a href="#sec-propuesta-lineas-generales">Propuesta: Líneas generales</a></dt><dt><a href="#sec-diccionarios">Diccionarios desde el punto de vista lexicográfico</a></dt><dt><a href="#sec-correctores-ortograficos">Correctores ortográficos</a></dt><dt><a href="#sec-correctores-ortotipograficos">Correctores ortotipográficos</a></dt><dt><a href="#sec-terminologia">
	    Terminologías y herramientas de gestión terminológica 
	(TMS)
      </a></dt><dt><a href="#sec-correctores-gramaticales">Correctores gramaticales y de estilo</a></dt><dt><a href="#sec-corpora-mono">Corpora monolingües</a></dt><dt><a href="#sec-corpora-multi">Corpora paralelos y multilingües</a></dt><dt><a href="#sec-memorias">Memorias de traducción</a></dt></dl></dd></dl></div><div class="sect1" lang="es" xml:lang="es"><div class="titlepage"><div><h2 class="title" style="clear: both"><a id="sec-intro"></a>Introducción</h2></div></div><p>
      La temática de este documento es lo que en la jerga de la 
      industria se llama <span class="foreignphrase"><i>«localización»</i></span> 
      o incluso <span class="foreignphrase"><i>«globalización»</i></span>, 
      directamente relacionadas con la traducción y 
      sus herramientas. Me he esforzado en atenerme a desarrollos
      posibles a corto y medio plazo, pero continuamente me ha
      asaltado la tentación de entrar en el tema de la traducción 
      automática. Queremos, a corto plazo, que las máquinas nos
      ayuden a traducir; a medio plazo el objetivo es que traduzcan
      por nosotros. Lo que pasa es que los dos objetivos convergen
      en dotar de inteligencia a las máquinas. Pienso que cualquier
      asistente a la traducción 
      necesita reglas: morfológicas, sintácticas, semánticas, necesita 
      una gramática y un «conocimiento del mundo». 
      ¿Cómo traduciría un programa sin inteligencia oraciones como 
      ‘spirits sold here’? ¿cómo puede un humilde
      corrector ortográfico distinguir ‘e implementado’ de 
      ‘he implementado’?
    </p><p>
      Es la hora de que el software libre llegue a la madurez. Eso 
      implica también que conozcamos las tendencias de la industria y 
      adoptemos y trabajemos con los estándares. Este documento trata
      de señalar el camino para el desarrollo de nuestras herramientas 
      de CAT, con el ojo puesto en el desarrollo
      futuro de
      herramientas de traducción más o menos automática. Ahora hay que 
      ponerse técnicos y me temo que la lectura no va a ser fácil; no
      soy un experto y escribo mientras aprendo así que es probable
      que se encuentren inexactitudes que será conveniente corregir. 
      De todos modos no pretendo más de plantear unas perspectivas y 
      unas posibles líneas de desarrollo; está claro que LuCAS y TLDP 
      son ejemplos de éxito como modelos de cooperación libre y que el 
      actual proceso de creación de la editorial libre y la conversión 
      de la documentación a DocBook XML son grandes ideas.
    </p><p>Dice Ismael Olea, hablando del ciclo de desarrollo de TLDP,</p><div class="blockquote"><blockquote class="blockquote"><p>Hay dos niveles de objetivos a cumplir:

	</p><div class="itemizedlist"><ul type="disc"><li><p>
	      herramientas para el fácil mantenimiento y 
	      traducción de nuestra documentación
	    </p></li><li><p>herramientas lingüísticas de calidad</p></li></ul></div><p>
      </p></blockquote></div><p>
      Mi objetivo es avanzar en la segunda opción.
    </p><div class="sect2" lang="es" xml:lang="es"><div class="titlepage"><div><h3 class="title"><a id="sec-intro-porque"></a>¿Por qué?</h3></div></div><p>
        Pensando sobre el <a href="http://olea.org/conferencias/doc-conf-creando-lucasv4/herramientas-reproduccion-publicacion.png" target="_top">
	esquema</a> de Ismael Olea ‘Arquitectura
	para reproducción y publicación’ del borrador de 
        “
	  LuCAS v4: creando el sistema de publicación de TLDP-ES
	”,
	creo que es el momento —intentaré explicar por
	qué— de 
	integrar en el esquema las herramientas lingüísticas y dar el
	paso de 
	adoptar los estándares y potencialidades de la ingeniería
	lingüística actual. 
      </p><p>
        Dependiendo de cómo nos planteemos la realidad de nuestro
        fondo documental, podemos pensar en él como en una gran
        biblioteca electrónica, como un repositorio digital (como el
        DSpace del MIT) o como un gran 
	corpus lingüístico. Cada punto de
        vista plantea posibilidades distintas: en tanto que biblioteca,
        debemos escuchar las aportaciones de los
        bibliotecónomos. Debemos también acordar los metadata y el
        formato de registro de los documentos, para que las búsquedas
        sean cada vez más eficaces. Yo, como lingüista, propongo que lo
        miremos con un nuevo enfoque: somos un gran equipo de
        traductores y redactores de documentación. Una de nuestras
        prioridades debe ser mejorar nuestras herramientas de ayuda a la
        escritura.
      </p><p>
        Disponemos de un fondo de varios cientos de megas de
        documentación técnica, en proceso de transcripción a DocBook XML,
        en su mayor parte traducciones de obras o documentación de tema
        informático que también son de licencia libre y disponemos
        además de las traducciones a multitud de idiomas. Ninguna
        empresa —¿quizás alguna universidad?— dispone de un
        fondo documental como el nuestro para la elaboración de
        terminologías específicas, para la confección de un corpus tan
        completo de la materia ni para la creación de memorias de
        traducción. Ninguna empresa dispone del número de traductores de
        que dispone la comunidad GNU Linux. Y sin embargo las
        características de esta comunidad (en su mayor parte estudiantes
        y profesionales de la informática) hace que el modo de trabajar
        y las herramientas estén a años luz de las posibilidades que
        presenta la investigación actual. No se conocen las experiencias
        que se realizan en la Unión Europea ni las herramientas que
        se utilizan, ignoramos los proyectos universitarios y los
        intentos de estandarización que está realizando la industria.
      </p><p>
        Y la distancia con respecto a la evolución del software
	propietario, lamentablemente, se amplía en lugar de
	acortarse. La comunidad, bajo el nombre de HispaLinux, o de 
	TLDP o de FSF o del que
	sea, debe funcionar como una entidad y personarse y participar
	en los foros donde se están creando estándares (LISA,
	ELRA…) y en los proyectos financiados por
	la Unión Europea (debemos aspirar a recibir financiación de
	las instituciones y de las empresas). Es la hora de
	preguntarse si el modelo de voluntarios a tiempo parcial es
	eficaz para los objetivos que nos proponemos.
      </p><div class="sect3" lang="es" xml:lang="es"><div class="titlepage"><div><h4 class="title"><a id="sec-intro-porque-crisis"></a>La crisis: más allá de los ficheros .po</h4></div></div><p>
	    No podemos olvidar que gettext y .po resuelven algunos de los
	    problemas con los que tiene que enfrentarse todo software de
	    traducción: la extracción de las cadenas que deben ser
	    traducidas,  
	    la segmentación y la referencia al lugar en el código al que 
	    pertenece la cadena, la alineación entre texto fuente y
	    texto traducido, la capacidad de reutilizar traducciones
	    cuando el fichero original ha cambiado, marcando las
	    cadenas equivalentes y aquellas en las que la traducción
	    es sólo aproximada (<span class="foreignphrase"><i>fuzzy</i></span>)
	    o no existe, y el informe automático del número de cadenas
	    traducidas…</p><p>
	    Existen varios intentos de ampliar el uso de nuestras
            herramientas más allá de la traducción de interfaces de
            usuario, a textos libres (ficheros no .po, XML especialmente), 
	    mediante el procedimiento de convertir de alguna manera los 
	    documentos a ficheros .po:
	</p><div class="itemizedlist"><ul type="disc"><li><p>
	      <span class="emphasis"><em>doc-i18n-tool</em></span>
	      (<a href="http://mail.gnome.org/archives/gnome-doc-list/2001-October/msg00034.html" target="_top">
		http://mail.gnome.org/archives/gnome-doc-list/2001-October/msg00034.html
	      </a>)
	    </p></li><li><p>
	      <span class="emphasis"><em>poxml</em></span> 
	      (de KDE)
	    </p></li><li><p>
	      <span class="emphasis"><em>po-debiandoc</em></span>
	    </p></li><li><p>
	      <span class="emphasis"><em>po-pod</em></span>,nuevo
	      miembro de la familia de utilidades
	      po-for-everything (po4a), tal
	      como se presenta en la Debian Weekly News del 3 de dic. de 2002:
	    </p><p>
	      “
		The goal of po-pod is to allow
		translators to work only with well known po files 
		when translating pod documentation. The goal of po4a is
		to ease translations (and more interestingly, the 
		maintainance of translations) by using gettext tools on 
		areas where they were not yet
		expected. 
	      ”
	      (<a href="http://lists.debian.org/debian-i18n-0211/msg00009.html" target="_top">
		http://lists.debian.org/debian-i18n-0211/msg00009.html</a>)
	    </p><p>
		La referencia es 
		<a href="http://www.ens-lyon.fr/~mquinson/deb.html#po-pod" target="_top">
		http://www.ens-lyon.fr/~mquinson/deb.html#po-pod</a>.
	    </p></li></ul></div></div><div class="sect3" lang="es" xml:lang="es"><div class="titlepage"><div><h4 class="title"><a id="sec-intro-porque-kde"></a>Segunda visita a KDE</h4></div></div><p>
	  La segunda y más novedosa parte
	  del «<span class="emphasis"><em><span class="foreignphrase"><i>The KDE Translation
	  HOWTO</i></span></em></span>» es el capítulo titulado
	  «<span class="foreignphrase"><i>Doc Translation</i></span>».
	  <sup>[<a id="id2789198" href="#ftn.id2789198">1</a>]</sup>
	</p><div class="blockquote"><blockquote class="blockquote"><p>
	    To make the documentation easier to maintain, the source files
	    <sup>[<a id="id2789224" href="#ftn.id2789224">2</a>]</sup>
	    were converted to PO format. This format was already used by GUI
	    translators with great success. 
	    <tt>Matthias Kiefer</tt>
	    and his contributors
	    extended KBabel to accomodate for
	    this new task (e.g. colored diff
	    mode, translation database, extended line feed handling). So 
	    KBabel
	    is probably a must have now not only for GUI translators but
	    also for people working on doc translation. 
	  </p><p>
	    <tt>Stephan Kulow</tt> 
	    enabled the 
	    KDE Help Center to parse XML(TM) files 
	    directly and to generate the HTML on the fly.
	  </p></blockquote></div><p>
	  Los traductores del proyecto tienen la ayuda de las
	  herramientas proporcionadas
	  por el paquete <span class="emphasis"><em>poxml</em></span>: 
	  fixsgml, 
	  xml2pot,
	  split2po y por 
	  xmlizer y 
	  checkXML, de 
	  <span class="emphasis"><em>kdelibs3-bin</em></span>.
	</p></div></div><div class="sect2" lang="es" xml:lang="es"><div class="titlepage"><div><h3 class="title"><a id="sec-intro-propuesta"></a>Propuesta</h3></div></div><p>
	Llevo tiempo preguntándome si estos tímidos intentos van bien
	encaminados. Mis objeciones son las siguientes:

	</p><div class="orderedlist"><ol type="1"><li><p>
	      <span class="emphasis"><em>.po no es un formato bien marcado</em></span>
	    </p><p>
	      Me refiero,
	      cualquiera que haya luchado con el Robot del Proyecto de 
	      Traducción Libre me entenderá, a que quede claro que
	      en la tercera línea debe ir el
	      <span class="foreignphrase"><i>e-mail</i></span> del traductor y
	      el <span class="foreignphrase"><i>copyright</i></span>, por
	      ejemplo. La DTD de .po está implícita de forma tácita en
	      las herramientas disponibles, pero no se muestra en el
	      formato.
	    </p></li><li><p>
	      <span class="emphasis"><em>Me temo que .po no permite granularidad en la
	      descomposición del párrafo/cadena en sus
	      componentes</em></span>.  
	    </p><p>
	      El formato nació para traducir
	      <span class="emphasis"><em>mensajes</em></span> (el trabajo del traductor
	      consiste en generar las
	      <span class="foreignphrase"><i>msgstr</i></span> correspondientes a
	      los <span class="foreignphrase"><i>msgid</i></span> extraídos del
	      programa), es decir, cadenas en una 
	      interfaz de usuario, sean los componentes de un menú o
	      los anuncios que se le hace al usuario (‘el fichero
	      ha sido guardado’). Pero es que ni siquiera para
	      esta función es eficaz: la
	      <span class="emphasis"><em>segmentación</em></span> de la salida de 
	      <b><tt>-h</tt></b> de cualquier programa
	      medianamente complejo (pienso ahora en las varias
	      pantallas de nmap) es
	      nula. ¿Cómo va a reutilizarse esta traducción?
	    </p><p>
	      La segmentación a nivel de párrafo en un texto XML es
	      totalmente insuficiente: un párrafo puede ocupar varias
	      páginas. 
	    </p></li><li><p>
	      En consecuencia de las insuficiencias anteriores,
	      <span class="emphasis"><em>no es posible una paralelización de calidad
	      entre la unidad de traducción fuente y la unidad
	      destino</em></span>, puesto que la única 
	      segmentación es en párrafos.
	    </p></li><li><p>
	      Sospecho que <span class="emphasis"><em>cuando reconoce una cadena como
	      «difusa» 
	      (<span class="foreignphrase"><i>fuzzy</i></span>) está actuando
	      únicamente al nivel de
	      <span class="foreignphrase"><i>diff</i></span></em></span>.  
	    </p><p>
	      No se utilizan las posibilidades de las expresiones
	      regulares y de los algoritmos avanzados en el
	      reconocimiento de patrones; no se avanza en la
	      reutilizabilidad de las traducciones.
	    </p></li><li><p>
	      Aunque gettext es
	      extremadamente eficiente en la extracción de cadenas
	      cuando se aplica a un programa adaptado, <span class="emphasis"><em>no hay modo de
	      separar en un texto en XML las cadenas que deben ser
	      traducidas de las que no es necesario o no deben
	      traducirse</em></span>. No hay modo de utilizar los
	      recursos de XLIFF.
	    </p></li><li><p>
	      Una última consecuencia de la no granularidad de
	      la segmentación y alineamiento de los textos XML es la
	      dificultad de crear memorias de traducción útiles.
	    </p></li></ol></div><p>
      </p><p>
	Aunque a efectos del resultado sería indiferente el formato
	que utilizara internamente una utilidad de traducción, en
	nuestro análisis debemos considerar si el formato elegido es
	adecuado para nuestras necesidades (ya no la traducción de
	cadenas en tanto que mensajes de un programa informático,
	sino de documentación).
      </p><p>
	Quizás el camino no esté en convertir los ficheros xml en
	ficheros .po para poder traducirlos, sino por el contrario 
	mantener gettext y el proceso de extracción de mensajes, 
	que funciona bien y es
	estable, convertirlos en ficheros XML bien formados, y
	aplicar herramientas generalizadas que puedan utilizarse
	sobre cualquier fichero XML. Lo que propondría
	sería una especie de .po con marcas, utilizable además para
	texto libre. 
      </p></div><div class="sect2" lang="es" xml:lang="es"><div class="titlepage"><div><h3 class="title"><a id="sec-intro-okapi"></a>Un proyecto paralelo: Okapi Framework</h3></div></div><p>
	El proyecto del Marco de trabajo Okapi
	(<a href="http://okapi.sourceforge.net/" target="_top">
	  http://okapi.sourceforge.net/
	</a>) tiene licencia modelo MIT.
      </p><div class="blockquote"><blockquote class="blockquote"><p>
	  The goal of the «<span class="emphasis"><em>Okapi Framework</em></span>» is to 
	  provide public specifications,
	  open components and libraries to allow tools developers and 
	  localizers
	  to build their own processes the way they want it, while 
	  staying
	  compatible and interoperable with each others.
	</p><p>
	  You can see the Okapi Framework as a set of links, of
	  construction blocks, the glue you can use to put together a
	  process that takes advantage of existing tools as you see
	  fit, by allowing them to work more efficiently together.
	</p><p>
	  The framework uses and promotes open standards when they
	  exist. For the aspects where open standards are not defined
	  yet, the framework offers its own proposals in an effort to
	  prompt and help the definition of the missing open
	  specifications. The ultimate goal is always to adopt the
	  industry standards when they are defined.
	</p></blockquote></div><p>
	Propone XLIFF 1.0 para la extracción de texto,
	TMX 1.3 para intercambiar TMs, TBX para el intercambio de
	terminologías y OLIF para el de lexicones (glosarios para
	sistemas de MT).
      </p></div></div><div class="sect1" lang="es" xml:lang="es"><div class="titlepage"><div><h2 class="title" style="clear: both"><a id="parte-al-grano"></a>Al grano</h2></div></div><p>
      En las secciones que restan de este documento vamos a seguir el
      procedimiento de 
      discutir brevemente las necesidades y herramientas libres
      conocidas y acabaremos por hacer una propuesta, bien de
      adopción de un estándar o desarrollo de una aplicación, bien de
      estudio previo a tomar una decisión.
    </p><p>
      Son de varios géneros las utilidades de ayuda a la escritura y
      al traductor: diccionarios, concordancias, memorias de
      traducción, analizadores de todas clases. No hay espacio en este
      artículo para volver a hacer lo que como decíamos en el resumen
      inicial ya está hecho: subyace a todas las referencias la
      familiaridad tácita con las herramientas o con 
      <a href="http://es.tldp.org/Articulos/0000otras/doc-traduccion-libre/" target="_top">
	http://es.tldp.org/Articulos/0000otras/doc-traduccion-libre/
      </a>. Es el momento de una revisión detallada y de tomar
      decisiones. 
    </p><div class="sect2" lang="es" xml:lang="es"><div class="titlepage"><div><h3 class="title"><a id="sec-propuesta-lineas-generales"></a>Propuesta: Líneas generales</h3></div></div><p>
	En TLDP se usarán estándares libres y abiertos: DocBook, TEI,
	XCES… y cuando se utilice uno propio
	se crearán mecanismos para la importación/conversión
	(recordemos por ejemplo la utilidad incorporada a
	Mimers brunn como conversor de .po a
	TMX). 
      </p><p>
	El formato de los documentos será preferentemente DocBook XML,
	porque permite la 
	separación lógica entre los niveles semántico y de
	presentación y da pie a desarrollos relacionados con la web
	semántica, y las herramientas trabajarán sobre XML.
      </p><p>
	La codificación UTF-8 es lógica en un proyecto con ambiciones
	de universalidad.
      </p></div><div class="sect2" lang="es" xml:lang="es"><div class="titlepage"><div><h3 class="title"><a id="sec-diccionarios"></a>Diccionarios desde el punto de vista lexicográfico</h3></div></div><p>
	En primer lugar evitemos una confusión generalizada,
	<span class="foreignphrase"><i>de verba non est disputandum</i></span>.
	Diccionarios son listas de palabras de un idioma, entradas
	léxicas con su definición, tablas de equivalencias entre dos o
	más lenguas y cualesquiera otras variaciones que se nos
	planteen; nomenclatura, glosario, lexicón, lemario etc. son
	términos que se usan de manera no estable en la literatura y
	no deben impedir que nos entendamos<sup>[<a id="id2850234" href="#ftn.id2850234">3</a>]</sup>. Mucho más interesante es
	plantearnos el campo de realidad cubierto por el diccionario y
	la información que proporciona (morfológica, de uso,
	terminológica…). Por último, distinguiremos diccionarios
	destinados a ser usados por humanos (en forma de libro o como
	consultas a través de una interfaz) de aquellos creados para ser
	usados por máquinas. También deberemos distinguir los que
	tienen una ambición descriptiva y verbal de los diccionarios
	terminológicos, que son como sabemos normativos y su objeto
	son los conceptos de un campo. Al fin y al cabo por lo pronto
	sólo queremos traducir documentación informática ¿o no?
      </p><div class="sect3" lang="es" xml:lang="es"><div class="titlepage"><div><h4 class="title"><a id="sec-diccionarios-discusion"></a>Discusión</h4></div></div><p>
	  Estamos en LuCAS-desarrollo reimpulsando nuestro propio
	  diccionario inglés-español de informática 
	  Giait<sup>[<a id="id2789695" href="#ftn.id2789695">4</a>]</sup> Por otro
	  lado estamos intentando crear el mayor lemario del 
	  español<sup>[<a id="id2789715" href="#ftn.id2789715">5</a>]</sup>. Y periódicamente nos planteamos
	  qué sentido tendría reclamar la liberación del diccionario
	  de la RAE (probablemente muy escaso). 
	  Pero todos estos son
	  pequeños pasos. Ponernos a escribir diccionarios nos
	  convierte en lexicógrafos; pero escribir un diccionario
	  técnico nos obliga a plantearnos los problemas de la
	  terminología. Además, si logramos crear una estructura para
	  la confección de terminologías (partiendo de nuestro corpus
	  de informática pero no quedándonos ahí) podremos crear
	  glosarios y diccionarios, presentar colocaciones y listas de
	  ejemplos. Y hacer un uso normativo (estandarizado,
	  coherente) de nuestra terminología. Estaremos por ejemplo en
	  condiciones de definir como término informático el que
	  aparece en nuestro fondo documental y no aparece recogido en
	  nuestro diccionario general de español (por ahora
	  inexistente).
	</p><p>
	  Pero vayamos poco a poco: un diccionario pensado para su
	  impresión en papel o en pantalla, destinado a ser consultado
	  por humanos, no es lo mismo que una base de datos
	  terminológicos (un lexicón computacional), pensada para su
	  utilización en la traducción automática.
	</p><div class="sect4" lang="es" xml:lang="es"><div class="titlepage"><div><h5 class="title"><a id="sec-tei-p4"></a>Sobre TEI P4</h5></div></div><p>
	    La versión XML de las 
	    <i>TEI P4 Guidelines</i><sup>[<a id="id2789807" href="#ftn.id2789807">6</a>]</sup>, en su
	    capítulo 13 ‘Terminological Databases’, advierte
	  </p><div class="blockquote"><blockquote class="blockquote"><p>
	      Since its first publication, this chapter has been rendered
	      obsolete in several respects, chiefly as a result of the
	      publication of ISO 12200, and a variant of it 
	      (TBX) which
	      has been recently adopted by LISA. Work is currently ongoing
	      in the ISO community to define a generic platform for
	      terminological markup (ISO CD 16642, 
	      TMF: Terminological
	      Markup Framework), in the light of which it is anticipated
	      that the recommendations of the present chapter will be
	      substantially revised.
	    </p></blockquote></div><p>
	    Sí nos interesa el capítulo 12 
	    ‘Print Dictionaries’.
	  </p><div class="blockquote"><blockquote class="blockquote"><p>A simple dictionary entry may contain
	      information about the form of the word treated, its
	      grammatical characterization, its definition, synonyms, or
	      translation equivalents, its etymology, cross-references to
	      other entries, usage information, and examples.
	    </p></blockquote></div><p>
	    Interesante parece estudiar como ejemplo de aplicación de TEI
	    <a href="http://www.human.toyogakuen-u.ac.jp/~acmuller/articles/ddb-ebti2001.htm" target="_top">
	      http://www.human.toyogakuen-u.ac.jp/~acmuller/articles/ddb-ebti2001.htm
	    </a>
	  </p></div><div class="sect4" lang="es" xml:lang="es"><div class="titlepage"><div><h5 class="title"><a id="sec-wordnet"></a>Sobre WordNet</h5></div></div><p>
	    Información en Princeton WordNet
	    (<a href="http://www.cogsci.princeton.edu/~wn/w3wn.html" target="_top">
	      http://www.cogsci.princeton.edu/~wn/w3wn.html</a>) ó
	  </p><p><b><tt>man wndb</tt></b></p><p>
	    WordNet es “an online lexical reference system whose
	    design is inspired by current psycholinguistic theories of
	    human lexical memory”.
	  </p><p>
	    EuroWordNet
	    (<a href="http://www.illc.uva.nl/EuroWordNet/" target="_top">
	      http://www.illc.uva.nl/EuroWordNet/</a>). EuroWordNet 
	    was a
	    European resources and development project (LE-2 4003
	    &amp; LE-4 8328) supported by the Human Language
	    Technology sector of the Telematics Applications
	    Programme, 1996-1999
	  </p><p> 
	    </p><div class="warning" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Problema</h3><p>
		Me temo que la versión europea de wn tiene carácter
		no libre y sus herramientas son no libres
		(editor Polaris; el visor Periscope solamente es
		freeware)
	      </p></div><p>
	  </p></div></div><div class="sect3" lang="es" xml:lang="es"><div class="titlepage"><div><h4 class="title"><a id="sec-diccionarios-propuesta"></a>Propuesta</h4></div></div><div class="orderedlist"><ol type="1"><li><p>
	      El formato estándar para la distribución de
	      diccionarios es .dict (un éxito, multiplicado casi por
	      cuatro desde la primera redacción de
	      <a href="http://es.tldp.org/Articulos/0000otras/doc-traduccion-libre/" target="_top">
		doc-traduccion-libre</a>)
	    </p></li><li><p>Para consultas el estándar es la red 
	      dict, mediante
	      clientes específicos o interfaz 
	      <span class="foreignphrase"><i>web</i></span>
	    </p></li><li><p>
	      Otro tema es cómo escribir diccionarios: propongo con
	      dudas adoptar TEI
	      P4 como la DTD de nuestros diccionarios específicos. En
	      este sentido, es necesario también completar el
	      desarrollo de Giait y 
	      desarrollar herramientas generales que puedan utilizarse
	      para el desarrollo de nuevos diccionarios
	    </p></li><li><p>
	      La gran pregunta, que normalmente no se hace cuando se
	      está haciendo un diccionario, es qué información debe
	      contener. Si queremos poder utilizar herramientas
	      avanzadas de ayuda a la traducción nuestro diccionario
	      debe tener información gramatical leíble por
	      máquinas. Propongo el uso de XCES.
	    </p></li><li><p>
	      Una cuestión previa imprescindible es analizar ISO 12620
	      (sobre <span class="foreignphrase"><i>recordable properties of
	      terms</i></span>) y llegar a un acuerdo (parts of
	      speech, gender, context, subject field…) sobre las
	      propiedades que nos es útil recoger en el diccionario.
	    </p></li><li><p>
	      Nuestra urgencia real es crear terminologías unificadas.
	    </p></li><li><p>
	      Cómo incorporar términos a los diccionarios: ¿modelo
	      ORCA? (contribuciones públicas, revisadas por un moderador) 
	      ¿modelo lista spanglish? (discusiones inter pares)
	    </p><p>
	      Utilizar herramientas de confección de
	      glosarios. Debemos ir hacia diccionarios basados en 
	      <span class="foreignphrase"><i>corpus</i></span>, con herramientas
	      de extracción terminológica que cubren de forma
	      exhaustiva un campo (y nos señalan fehacientemente qué
	      términos faltan por definir) y abandonar el método
	      manual de adición de entradas.
	    </p></li><li><p>
	      Cómo garantizar la calidad de las incorporaciones:
	      Crear sistema de mantenimiento de calidad (incluir entre
	      las marcas de cada término autor, fecha,
	      revisión…)
	    </p><p>
	      Ismael Olea ha propuesto un sistema de ponderación de
	      la autoridad de las aportaciones similar al del sistema
	      <span class="emphasis"><em>advogato</em></span> 
	      (<a href="http://advogato.org/trust-metric.html" target="_top">
	         http://advogato.org/trust-metric.html</a>). 
	    </p></li></ol></div><p> Será necesario adaptar
	  TEItools 
	  (<a href="http://xtalk.msk.su/SGML/TEItools/index-en.html" target="_top"> 
	    http://xtalk.msk.su/SGML/TEItools/index-en.html</a>).
	</p></div><div class="sect3" lang="es" xml:lang="es"><div class="titlepage"><div><h4 class="title"><a id="sec-un-caso-practico"></a>Un caso práctico</h4></div></div><p>
	  Las dos primeras entradas de la letra «a» 
	  en las fuentes del diccionario
	  Giait 
	  son
	</p><pre class="programlisting">
@@ING A, Ampere, Amps 
@@CAS Ampère (amperio), amperios. 
@@DIC Vocablo definido únicamente para el Diccionario
@@FIN

@@ING A Programming Language&quot;, APL 
@@CAS Lenguaje de Programación APL 
@@GLO APL es la sigla de &quot;A Programming Language&quot; (Un Lenguaje de 
@@GLO Programación), que fue un libro escrito en 1962 por el credor del 
@@GLO lenguaje, Kenneth E. Iverson. Basado en lo que antes se había conocido 
@@GLO como la Nomenclatura Iverson, el APL es un lenguaje de programación 
@@GLO extremadamente conciso, diseñado para el manejo de los arreglos 
@@GLO (arrays). Los arreglos pueden ser escalares, vectoriales, tablas o 
@@GLO matrices de dos o más dimensiones, pudiendo estar compuestos de 
@@GLO información numérica o alfanumérica. Bajo la conducción de Iverson, 
@@GLO IBM introdujo en 1966 el APL\360. 
@@GLO Como el APL evita la introducción de las computadoras personales, 
@@GLO originalmente se lo empleó unicamente en mainframes. Desde 1983, han 
@@GLO estado disponibles las versiones de APL para las PC. Debido a su 
@@GLO conjunto de caracteres especiales expandidos, el APL requiere un 
@@GLO teclado especial, o el uso de macros, para el ingreso de datos. 
@@GLO Las versiones actuales de APL para mainframes y PC se denominan
@@GLO APL2.
@@GLO
@@FIN
</pre><p>
	  Convertidas a formato .dict (descomprimido) actualmente
	  quedan así:
	</p><p>A, Ampere, Amps Ampère (amperio), amperios.</p><p>
	  A Programming Language&quot;, APL Lenguaje de
	  Programación APL
	</p><p>
	  Vemos que, aparte de errores tipográficos que habrá que
	  corregir, se ha perdido la información de glosario y el marcado
	  (lo que es inglés, castellano, comentario, glosario…)
	</p><p>¿Cómo aparece?</p><pre class="programlisting">
<tt>11262928 23 n 03 ampere 0 amp 0 A 0 003 @ 11259168 n
0000 #p 11263273 n 0000 %p 11263164 n 0000 | the basic unit of
electric current adopted under the System International d'Unites;
&quot;a typical household circuit carries 15 to 50 amps&quot;
</tt>
</pre><p>
	  ¿Cómo quedaría todo esto en TEI P4? Crearemos el fichero
	  ejemplo.tei.xml (el siguiente fragmento de texto está en
	  UTF-8)
	</p><pre class="programlisting">

&lt;!-- %%%%%%%%%%%%%%%%%%%%%%%%%%% empieza ejemplo.tei.xml --&gt;
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;
&lt;!DOCTYPE TEI.2 PUBLIC &quot;-//TEI P4//DTD Main Document Type//EN&quot; 
  &quot;/usr/local/share/sgml/tei/dtd/tei2.dtd&quot; [
  &lt;!ENTITY % TEI.XML          'INCLUDE' &gt;
  &lt;!ENTITY % TEI.dictionaries 'INCLUDE' &gt;
  ]&gt;

&lt;body&gt;
    &lt;div0 type='dictionary'&gt;
	&lt;!-- English-espaÃ±ol --&gt;
	&lt;entry&gt;
	    &lt;form type=&quot;abbrev&quot;&gt;
		&lt;orth&gt;A&lt;/orth&gt;
	    &lt;/form&gt;
	    &lt;form type=&quot;full&quot;&gt;
	        &lt;lbl&gt;abbreviation for&lt;/lbl&gt;
		    &lt;orth&gt;AmpÃ¨re&lt;/orth&gt;
	        &lt;trans&gt;
		    &lt;tr&gt;A&lt;/tr&gt;
		&lt;/trans&gt;
	    &lt;/form&gt;
	&lt;/entry&gt;

	&lt;entry&gt;
	    &lt;form&gt;
		&lt;orth&gt;AmpÃ¨re&lt;/orth&gt;
	    &lt;/form&gt;
	    &lt;gramGrp&gt;
		&lt;pos&gt;n&lt;/pos&gt;
	    &lt;/gramGrp&gt;
	    &lt;def&gt;&lt;/def&gt;
	    &lt;trans&gt;
		&lt;tr&gt;Amperio&lt;/tr&gt;
		&lt;gen&gt;m&lt;/gen&gt;
	    &lt;/trans&gt;
	&lt;/entry&gt;

	&lt;entry&gt;
	    &lt;form type=&quot;abbrev&quot;&gt;
		&lt;orth&gt;APL&lt;/orth&gt;
	    &lt;/form&gt;
	    &lt;form type=&quot;full&quot;&gt;
	        &lt;lbl&gt;abbreviation for&lt;/lbl&gt;
		    &lt;orth&gt;A Programming Language&lt;/orth&gt;
	        &lt;trans&gt;
		    &lt;tr&gt;APL&lt;/tr&gt;
		&lt;/trans&gt;
	    &lt;/form&gt;
	&lt;/entry&gt;

	&lt;entry&gt;
	    &lt;form&gt;
		&lt;orth&gt;A Programming Language&lt;/orth&gt;
	    &lt;/form&gt;
	    &lt;gramGrp&gt;
		&lt;pos&gt;n&lt;/pos&gt;
	    &lt;/gramGrp&gt;
	    &lt;def&gt;
		APL es la sigla de &quot;A Programming Language&quot; (Un Lenguaje de 
		ProgramaciÃ³n), que fue un libro escrito en 1962 por el credor del 
		lenguaje, Kenneth E. Iverson. Basado en lo que antes se habÃ­a conocido 
		como la Nomenclatura Iverson, el APL es un lenguaje de programaciÃ³n 
		extremadamente conciso, diseÃ±ado para el manejo de los arreglos 
		(arrays). Los arreglos pueden ser escalares, vectoriales, tablas o 
		matrices de dos o mÃ¡s dimensiones, pudiendo estar compuestos de 
		informaciÃ³n numÃ©rica o alfanumÃ©rica. Bajo la conducciÃ³n de Iverson, 
		IBM introdujo en 1966 el APL\360.
		Como el APL evita la introducciÃ³n de las computadoras personales, 
		originalmente se lo empleÃ³ unicamente en mainframes. Desde 1983, han 
		estado disponibles las versiones de APL para las PC. Debido a su 
		conjunto de caracteres especiales expandidos, el APL requiere un 
		teclado especial, o el uso de macros, para el ingreso de datos.
		Las versiones actuales de APL para mainframes y PC se denominan
		APL2.
	    &lt;/def&gt;
	    &lt;trans&gt;
		&lt;tr&gt;Lenguaje de ProgramaciÃ³n APL&lt;/tr&gt;
		&lt;gen&gt;m&lt;/gen&gt;
	    &lt;/trans&gt;
	&lt;/entry&gt;
  &lt;/div0&gt;
&lt;/body&gt;
&lt;!-- %%%%%%%%%%%%%%%%%%%%%%%%%%% acaba ejemplo.tei.xml --&gt;

</pre><p>
	  ¿Cómo convertir esta fuente en un documento
	  distribuible? Utilizaremos 
	  lib-saxon-java, 
	  passivetex y las
	  hojas de estilo XSL creadas por Sebastian Rahtz
	  (<a href="http://www.tei-c.org/Stylesheets/teixsl.html" target="_top">
	    http://www.tei-c.org/Stylesheets/teixsl.html</a>). 
	  Lo siguiente
	  depende de la instalación de cada uno; yo utilizo en estos
	  momentos una Debian Sarge. Passivetex es un paquete
	  <span class="foreignphrase"><i>apt-gettable</i></span>; 
	  descargo las DTD's TEI y las hojas de estilo
	  y las instalo en 
	  <tt>/usr/local/share/sgml/tei/</tt> 
	</p><p>
	  La conversión a fichero .pdf me da errores; teóricamente
	  se haría
	</p><p>
</p><pre class="programlisting">
java -classpath /usr/share/java/saxon.jar \
com.icl.saxon.StyleSheet -o ejemplo.tei.fo ejemplo.tei.xml \
/usr/local/share/sgml/tei/xsl/fo/tei.xsl

pdfxmltex ejemplo.tei.fo 

pdfxmltex ejemplo.tei.fo
</pre><p>
	</p><p>
	  Para generar el fichero
	  <tt>ejemplo.resultado.html</tt>
	</p><p>
</p><pre class="programlisting">
java -classpath /usr/share/java/saxon.jar \
com.icl.saxon.StyleSheet ejemplo.tei.xml \
/usr/local/share/sgml/tei/xsl/html/teihtml.xsl
</pre><p>
	</p><p>Este es el fichero generado:</p><pre class="programlisting">

&lt;!-- %%%%%%%%%%%%%%%%%%%%%%%%%%% empieza ejemplo.resultado.html --&gt;
&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;
&lt;!DOCTYPE html
  PUBLIC &quot;-//W3C//DTD HTML 4.0 Transitional//EN&quot;&gt;

&lt;div class=&quot;teidiv&quot;&gt;
   &lt;h2&gt;&lt;a name=&quot;ej.utf8-div0-d0e3&quot;&gt;&lt;/a&gt;1. 
   &lt;/h2&gt;
   	
   	
   	    
   		A
   	    
   	    
   	        abbreviation for
   		    Amp&amp;egrave;re
   	        
   		    A
   		
   	    
   	
   
   	
   	    
   		Amp&amp;egrave;re
   	    
   	    
   		n
   	    
   	    
   	    
   		Amperio
   		m
   	    
   	
   
   	
   	    
   		APL
   	    
   	    
   	        abbreviation for
   		    A Programming Language
   	        
   		    APL
   		
   	    
   	
   
   	
   	    
   		A Programming Language
   	    
   	    
   		n
   	    
   	    
   		APL es la sigla de &quot;A Programming Language&quot; (Un Lenguaje de 
   		Programaci&amp;oacute;n), que fue un libro escrito en 1962 por el credor del 
   		lenguaje, Kenneth E. Iverson. Basado en lo que antes se hab&amp;iacute;a conocido 
   		como la Nomenclatura Iverson, el APL es un lenguaje de programaci&amp;oacute;n 
   		extremadamente conciso, dise&amp;ntilde;ado para el manejo de los arreglos 
   		(arrays). Los arreglos pueden ser escalares, vectoriales, tablas o 
   		matrices de dos o m&amp;aacute;s dimensiones, pudiendo estar compuestos de 
   		informaci&amp;oacute;n num&amp;eacute;rica o alfanum&amp;eacute;rica. Bajo la conducci&amp;oacute;n de Iverson, 
   		IBM introdujo en 1966 el APL\360.
   		Como el APL evita la introducci&amp;oacute;n de las computadoras personales, 
   		originalmente se lo emple&amp;oacute; unicamente en mainframes. Desde 1983, han 
   		estado disponibles las versiones de APL para las PC. Debido a su 
   		conjunto de caracteres especiales expandidos, el APL requiere un 
   		teclado especial, o el uso de macros, para el ingreso de datos.
   		Las versiones actuales de APL para mainframes y PC se denominan
   		APL2.
   	    
   	    
   		Lenguaje de Programaci&amp;oacute;n APL
   		m
   	    
   	
     
&lt;/div&gt;
&lt;!-- %%%%%%%%%%%%%%%%%%%%%%%%%%% acaba ejemplo.resultado.html --&gt;

</pre></div></div><div class="sect2" lang="es" xml:lang="es"><div class="titlepage"><div><h3 class="title"><a id="sec-correctores-ortograficos"></a>Correctores ortográficos</h3></div></div><p>
	Tema relacionado con el de los diccionarios en tanto que
	listas de palabras.
      </p><p>
        «Nuestro» proyecto es el 
	<span class="emphasis"><em>COES</em></span>: Herramientas para
        Procesamiento de Lenguaje Natural en Español
        (<a href="http://www.datsi.fi.upm.es/~coes/" target="_top">
	http://www.datsi.fi.upm.es/~coes/</a>), de 
	Santiago Rodríguez &amp;
        Jesús Carretero, que se distribuye como software de libre
        disposición desde finales de 1994.
      </p><p>
        COES consta de un diccionario de unos 53.000 términos y un
        corrector ortográfico integrado en la utilidad Unix 
	ispell y desarrollos derivados
	(aspell). Hay
        que señalar que se ha ampliado el conjunto de herramientas
        lingüísticas con un diccionario de sinónimos/antónimos. Su
        particularidad es ser sensible a las reglas morfológicas de las
        palabras y no sólo a las raíces.
      </p><p>
        Un corrector ortográfico «inteligente» 
	tiene que hacer
        algo más que comparar las palabras del texto con una lista de
        palabras correctas; para distinguir «a» de 
	«ha» tiene
        que tener reglas — acabaremos necesitando un mínimo
        análisis morfológico y sintáctico.
      </p></div><div class="sect2" lang="es" xml:lang="es"><div class="titlepage"><div><h3 class="title"><a id="sec-correctores-ortotipograficos"></a>Correctores ortotipográficos</h3></div></div><p>
	Estudiar la implementación de OpenOffice.
      </p></div><div class="sect2" lang="es" xml:lang="es"><div class="titlepage"><div><h3 class="title"><a id="sec-terminologia"></a>
	    Terminologías y herramientas de gestión terminológica 
	(TMS)
      </h3></div></div><div class="sect3" lang="es" xml:lang="es"><div class="titlepage"><div><h4 class="title"><a id="sec-terminologia-discusion"></a>Discusión</h4></div></div><p>
	Las ventajas que ofrece la utilización de terminologías son
	evidentes: uniformidad entre las traducciones, corrección y
	reusabilidad. Debemos también en el seno de TLDP clarificar
	los problemas ante los que se enfrenta toda terminología:
      </p><div class="orderedlist"><ol type="1"><li><p>eliminación de la sinonimia</p></li><li><p>reducción de la polisemia/homonimia</p></li><li><p>neologismos</p></li></ol></div></div><div class="sect3" lang="es" xml:lang="es"><div class="titlepage"><div><h4 class="title"><a id="sec-terminologia-o-mt"></a>Terminología y Memorias de traducción</h4></div></div><p>
	  Discutido en
	  <a href="http://www.lisa.org/sigs/phpBB/viewtopic.php?topic=22&amp;forum=1&amp;4" target="_top">
	    http://www.lisa.org/sigs/phpBB/viewtopic.php?topic=22&amp;forum=1&amp;4
	  </a>
	</p><p>Responde Kara Warburton: </p><div class="blockquote"><blockquote class="blockquote"><p>
	    There are many differences between Terminology and 
	    Translation Memory. The first that comes to mind is 
	    that terminology management must occur in the source
	    language to ensure high quality source texts before a
	    translation memory even exists. Inconsistency and
	    inappropriate terms in the source language is an
	    increasing and costly problem especially for global
	    companies. And if the inconsistency is unchecked, then
	    it breaks the TM — there will be no match with
	    previous texts which are &quot;supposed&quot; to be the
	    same. A second difference is that terminology management
	    can document features of words in a much more granular
	    fashion than translation memory which only records
	    strings of text. Definitions, contexts, usage notes,
	    references to related terms, and so forth can assist
	    translators to select an appropriate target term when
	    the TM does not suggest one. And if there is an exact TM
	    match, what if the meaning is different which can
	    frequently occur)? Then the TM match is incorrect and
	    the translator has to think more carefully about the
	    terms. In this case, having a terminology system can
	    provide the translator with the required information to
	    correct the TM. In addition, grammatical information can
	    be recorded which can provide data to more advanced
	    systems down the road, such as search engines and
	    machine translation engines.
	  </p></blockquote></div><p>
	  Ingrid Haussteiner lo confirma: 
	  </p><div class="blockquote"><blockquote class="blockquote"><p>
	      I think your question is very interesting and I
	      think from a translator's perspective this is not an
	      either-or thing but rather a question of many ifs.
	    </p><p>
	      If you can choose, I would implement a terminology 
	      management system first, have people test and hone their
	      discipline and raise their awareness of the
	      difficulties involved in using consistent terminology
	      (even more so if it is multilingual), analyzing
	      choices and making suggestions 
	      (descriptive terminology work).
	    </p><p>
	      If you implement a translation memory system after
	      some time, you might see that people slack off a bit
	      as far as terminology work is concerned. The big
	      argument is that the TM system integrates with the
	      TMS.
	    </p><p>
	      You most certainly know that EBMT (Example-Based
	      Machine Translation) seems to be one of the buzzwords
	      in the industry. Some translation software already support
	      something like &quot;assembling&quot;, others are keen
	      to do so. So, in the future, when working with TM
	      technology, it will be an invaluable asset to have
	      quality terminology as the TM also assembles from
	      there where it finds no exact or fuzzy matches.
	    </p></blockquote></div><p>
	</p><p>
	  Mark D. Childress remacha: 
	  </p><div class="blockquote"><blockquote class="blockquote"><p>
	      I've heard this discussion at a conference or two
	      recently. It seems to me all terminologists shudder at
	      the thought but some have problems communicating why the
	      sole use of translation memories is Not A Good Thing,
	      whereas their use together with term databases is.
	    </p><p>
	      The simplest way to explain to those people not
	      familiar with either, but who make the decisions
	      whether to support terminology management or not:
	    </p><div class="itemizedlist"><ul type="disc"><li><p>
		  Translation memory: Descriptive — The way
		  things are
		</p></li><li><p>
		  Terminology database: Prescriptive — The way
		  things should be
		</p></li></ul></div><p>
	      Then ask them: Should one use a term that is
	      (potentially) incorrect, simply because that's the way
	      it is? Because if that's so, a term use error
	      continues to multiply because there's no prescriptive
	      ruling to use as a reference to standardize the
	      term. As Kara mentioned in her post, it will
	      ultimately make the translation memory useless.
	    </p><p>
	      Alternately, one can correct the problem each time it
	      appears in &quot;reality&quot; and count the cost of
	      correction, whereas defining the &quot;ideal&quot;
	      goes a long way towards reducing the occurrence of the
	      problem to begin with. Basic quality rule: Get it
	      right the first time, and reduce your follow-on costs.
	    </p></blockquote></div><p>
	</p></div><div class="sect3" lang="es" xml:lang="es"><div class="titlepage"><div><h4 class="title"><a id="sec-terminologia-propuesta"></a>Propuesta</h4></div></div><p>
	  El proceso que ha iniciado Javier Fernández Serrador de
	  unificación de las terminologías de los distintos proyectos
	  de traducción de interfaces de usuario (Gnome, KDE, es@li,
	  LuCAS) es fundamental. Propone la creación de una base de
	  datos que incluya las traducciones consensuadas de los
	  términos, y aquellas que se consideren incorrectas.
	</p><p>
	  Cuando las herramientas estén maduras para utilizarlas
	  (pienso en los 
	  desarrollos deseables de gtranslator y kbabel) implementar 
	  bases de datos terminológicos en TBX.
	</p></div></div><div class="sect2" lang="es" xml:lang="es"><div class="titlepage"><div><h3 class="title"><a id="sec-correctores-gramaticales"></a>Correctores gramaticales y de estilo</h3></div></div><p>
	¿Tengo que explicar que un corrector gramatical 
	necesita una gramática?
      </p><p>
	Estudiar <span class="emphasis"><em>diction</em></span>. 
	Aplicarlo al español.
      </p></div><div class="sect2" lang="es" xml:lang="es"><div class="titlepage"><div><h3 class="title"><a id="sec-corpora-mono"></a>Corpora monolingües</h3></div></div><div class="sect3" lang="es" xml:lang="es"><div class="titlepage"><div><h4 class="title"><a id="section_corpora_mono_discusion"></a>Discusión</h4></div></div><p>
	  Según Catherine Ball
	  (<a href="http://www.georgetown.edu/cball/corpora/" target="_top">
	    http://www.georgetown.edu/cball/corpora/</a>):
	</p><div class="blockquote"><blockquote class="blockquote"><p>
	    To create a rudimentary <span class="emphasis"><em>concordancer</em></span> 
	    is a simple
	    programming task: it is a matter of indexing words to
	    lines, sorting the words alphabetically, and displaying
	    each word in a fixed amount of context. However, most of
	    the generally-available concordancers have many additional
	    features, including options for producing full or partial
	    concordances, sorting in a variety of orders, searching
	    for collocations, and producing basic text statistics.
	  </p></blockquote></div></div><div class="sect3" lang="es" xml:lang="es"><div class="titlepage"><div><h4 class="title"><a id="sec-corpora-mono-propuesta"></a>Propuesta</h4></div></div><div class="orderedlist"><ol type="1"><li><p>
	      <span class="emphasis"><em>Herramientas estadísticas</em></span>. 
	      Como traductor opino que necesitamos concordancias
	      (cf. XConcord, de Cíbola; tea, de Masao Utiyama), 
	      colocaciones, 
	      análisis de patrones léxicos y agrupaciones de palabras
	      (cf. Paai's text utilities)
	    </p></li><li><p>
	      <span class="emphasis"><em>Anotaciones</em></span> (taggers). Mediante 
	      etiquetado XCES si pretendemos
	      avanzar en el campo de la traducción automática.
	      Desarrollar/liberar herramientas de desambiguación
	    </p></li><li><p>
	      Extracción de términos
	    </p></li><li><p>
	      Diferenciación entre cadenas que deben ser traducidas y
	      las que no (XLIFF)
	    </p></li></ol></div></div></div><div class="sect2" lang="es" xml:lang="es"><div class="titlepage"><div><h3 class="title"><a id="sec-corpora-multi"></a>Corpora paralelos y multilingües</h3></div></div><div class="sect3" lang="es" xml:lang="es"><div class="titlepage"><div><h4 class="title"><a id="section-corpora-multi-discusion"></a>Discusión</h4></div></div><p>
	  Es evidente el interés de estas herramientas. Condiciones
	  previas son
	</p><div class="orderedlist"><ol type="1"><li><p>la <span class="emphasis"><em>segmentación</em></span>, el
	      <span class="emphasis"><em>recuento de palabras</em></span> y
	    </p></li><li><p>
	      el <span class="emphasis"><em>alineado</em></span> de los textos paralelos, 
	      en una primera fase a nivel de párrafo
	      (insuficiente). Comprender la distinción entre
	      alineación oracional y léxica. Lucha contra
	      paralelizaciones ruidosas y ambigüedad sintáctica y/o
	      semántica. 
	    </p></li></ol></div><div class="itemizedlist"><ul type="disc"><li><p>
	      WA Gale y KW Church sientan las bases teóricas
	      (estadísticas) sobre el alineado de texto en 
	      “A Program
	      for Aligning Sentences in Bilingual Corpora”, 
	      de 1993 
	      (<a href="http://citeseer.nj.nec.com/gale93program.html" target="_top">
	      http://citeseer.nj.nec.com/gale93program.html</a>; el
	      documento incluye la fuente del programa)
	    </p></li><li><p>XAlign, de Cíbola</p></li><li><p>Aportaciones del proyecto MULTEXT (multext_align)</p></li><li><p>
	      Desarrollos matemáticos a la última que escapan a mi
	      comprensión actual, en las páginas de Dan Melamed
	      (<a href="http://www.cs.nyu.edu/~melamed/interests.html" target="_top">
		http://www.cs.nyu.edu/~melamed/interests.html</a>)
	    </p></li></ul></div><p>
	  Melamed ha creado también gran número de utilidades libres.
	</p></div><div class="sect3" lang="es" xml:lang="es"><div class="titlepage"><div><h4 class="title"><a id="sec-corpora-multi-propuesta"></a>Propuesta</h4></div></div><p>
	  Segmentación y alineado a nivel de párrafo están 
	  favorecidos por el marcado DocBook de nuestros documentos.
	</p><p>
	  El alineado a nivel de párrafo no es suficiente. Ver SRX, de
	  Oscar/LISA. 
	</p><p>
	  Nuestro problema es definir las Unidades de Traducción.
	</p><p>
	  Creación de memorias de traducción: «un corpus alineado y
	  anotado constituye una memoria de traducción» (Abaitua,
	  Tradumática 0, 2001).
	</p></div></div><div class="sect2" lang="es" xml:lang="es"><div class="titlepage"><div><h3 class="title"><a id="sec-memorias"></a>Memorias de traducción</h3></div></div><div class="sect3" lang="es" xml:lang="es"><div class="titlepage"><div><h4 class="title"><a id="sec-memorias-discusion"></a>Discusión</h4></div></div><p>
	  Las TM contribuyen a la calidad de las traducciones en dos
	  aspectos: su coherencia y su exactitud.
	</p></div><div class="sect3" lang="es" xml:lang="es"><div class="titlepage"><div><h4 class="title"><a id="sec-memorias-propuesta"></a>Propuesta</h4></div></div><p>
	  gtranslator y kbabel utilizan de forma natural MT cuando
	  trabajan con ficheros .po.
	</p><p>
	  Deben extenderse para utilizar MT cuando trabajen con
	  documentación y textos libres. Para ello es necesario haber
	  resuelto los problemas de la segmentación, alineación y
	  marcado.
	</p><p>
	  Los proyectos deben trabajar (o al menos ser capaces de
	  importar y exportar) el formato TMX.
	</p></div></div></div><div class="footnotes"><br /><hr width="100" align="left" /><div class="footnote"><p><sup>[<a id="ftn.id2789198" href="#id2789198">1</a>] </sup>
	      <a href="http://i18n.kde.org/translation-howto/doc-translation.html" target="_top">
		http://i18n.kde.org/translation-howto/doc-translation.html
	      </a>.
	    </p></div><div class="footnote"><p><sup>[<a id="ftn.id2789224" href="#id2789224">2</a>] </sup>
		El formato de la documentación había empezado siendo
		.html, fue LinuxDoc .sgml
		con KDE 1.x y DocBook .sgml en KDE 2.0.
	      </p></div><div class="footnote"><p><sup>[<a id="ftn.id2850234" href="#id2850234">3</a>] </sup>Basta consultar la clasificación de dos páginas del
	    término «diccionario» que hace Martínez De Sousa 
	    <i>
	      Diccionario de lexicografía práctica
	    </i> para llegar a la conclusión de que lo
	    importante no es cómo se llama sino qué clase de diccionario
	    queremos.</p></div><div class="footnote"><p><sup>[<a id="ftn.id2789695" href="#id2789695">4</a>] </sup>
	  <a href="http://cvs.hispalinux.es/cgi-bin/cvsweb/rl-dicc/" target="_top">
	    http://cvs.hispalinux.es/cgi-bin/cvsweb/rl-dicc/</a>.</p></div><div class="footnote"><p><sup>[<a id="ftn.id2789715" href="#id2789715">5</a>] </sup><a href="http://lemarios.olea.org" target="_top">
		http://lemarios.olea.org</a></p></div><div class="footnote"><p><sup>[<a id="ftn.id2789807" href="#id2789807">6</a>] </sup>
		(<a href="http://www.tei-c.org/Guidelines2/index.html" target="_top">
		  http://www.tei-c.org/Guidelines2/index.html</a>. 
		El documento puede
		descargarse comprimido como
		<a href="http://www.tei-c.org/Guidelines2/p4html.tar.gz" target="_top">
		  http://www.tei-c.org/Guidelines2/p4html.tar.gz</a>
	      </p></div></div></div></body></html>
