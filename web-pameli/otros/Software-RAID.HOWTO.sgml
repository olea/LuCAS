<!DOCTYPE linuxdoc system
[ <!ENTITY CurrentVer "0.90.3 - Alpha">
  <!ENTITY mdstat "<TT>/proc/mdstat</TT>">
  <!ENTITY ftpkernel "<TT>ftp://ftp.fi.kernel.org/pub/linux</TT>">
  <!ENTITY fstab "<TT>/etc/fstab</TT>">
  <!ENTITY raidtab "<TT>/etc/raidtab</TT>">
]
>

<ARTICLE>

<TITLE>RAID-Software CÓMO
<AUTHOR>Jakob &Oslash;stergaard
        (<htmlurl
        url="mailto:jakob@ostenfeld.dk"
        name="jakob@ostenfeld.dk">)
<DATE>v. &CurrentVer;, 22 mayo 1999

<ABSTRACT>
Este CÓMO describe cómo usar un RAID software bajo Linux. Debería usar
los parches RAID disponibles en  <htmlurl
name="ftp://ftp.fi.kernel.org/pub/linux/daemons/raid/alpha"
url="ftp://ftp.fi.kernel.org/pub/linux/daemons/raid/alpha">. El CÓMO se
puede encontrar en <htmlurl
name="http://ostenfeld.dk/~jakob/Software-RAID.HOWTO/"
url="http://ostenfeld.dk/~jakob/Software-RAID.HOWTO/">.
</ABSTRACT>

<TOC>

<SECT>Introducción
<P>
Este CÓMO ha sido escrito por Jakob &Oslash;stergaard basándose en un gran
número de mensajes de correo entre el autor, Ingo Molnar  (<htmlurl
url="mailto:mingo@chiara.csoma.elte.hu"
name="mingo@chiara.csoma.elte.hu">) -- uno de los desarrolladores de RAID --,
la lista de correo linux-raid (<htmlurl
url="mailto:linux-raid@vger.rutgers.edu"
name="linux-raid@vger.rutgers.edu">) y otras diversas personas.
<P>
La razón por la que se ha escrito este CÓMO incluso aunque ya exista un
RAID-Software CÓMO es que el anterior CÓMO describe el antiguo estilo de RAID
software que se encuentra en los núcleos existentes. Este CÓMO describe el uso
del ``nuevo estilo'' de RAID que se ha desarrollado más recientemente. El nuevo
estilo de RAID tiene muchas características no presentes en el anterior estilo
de RAID.
<P>
Parte de la información de este CÓMO le puede parecer trivial si es un
entendido en RAID. Simplemente, sáltese esas partes.
<P>

<SECT1>Declinación de responsabilidades
<P>
La declinación obligatoria de responsabilidades:
<P>
Aunque RAID parece estable para mí, y estable para muchas otras personas,
puede no funcionar para ti. Si pierdes todos tus datos, tu trabajo, eres
golpeado por un camión o cualquier otra cosa, no es culpa mía ni de los
desarrolladores. ¡Conciéncese de que usa el software RAID y esta información
por su cuenta y riesgo!. No hay ningún tipo de garantía de que ningún
software ni esta información sean correctos de ninguna manera, ni adecuados
para cualquier tipo de uso. Haga copia de seguridad de sus datos antes de
experimentar con esto. Más vale estar seguro que lamentarse.
<P>

<SECT1>Requisitos
<P>
Este CÓMO asume que está usando una de las últimas versiones 2.2.x o 2.0.x
del núcleo con un parche raid0145 acorde y la versión 0.90 del paquete
raidtools. Ambos se pueden encontrar en <HTMLURL
name="ftp://ftp.fi.kernel.org/pub/linux/daemons/raid/alpha"
url="ftp://ftp.fi.kernel.org/pub/linux/daemons/raid/alpha">. El parche RAID,
el paquete raidtools y el núcleo deben concordar tanto como sea posible. En
ocasiones puede ser necesario usar un núcleo antiguo si no hay parches raid
disponibles para el último núcleo.
<P>

<SECT>¿Por qué RAID?
<P>
Puede haber muchas buenas razones para usar RAID. Unas pocas son: la
posibilidad de combinar varios discos físicos en un único dispositivo 
``virtual'' más grande, mejoras en el rendimiento y redundancia.
<P>

<SECT1>Detalles técnicos
<P>
El RAID de Linux puede funcionar sobre la mayoría de los dispositivos de
bloques. No importa si usa dispositivos IDE, SCSI o una mezcla de ambos.
Incluso algunas personas han usado el dispositivo de bloques de red (Network
Block Device, NBD) con más o menos éxito.
<P>
Asegúrese de que el bus (o buses) de los discos son lo suficientemente
rápidos. No debería tener 14 discos UW-SCSI en un único bus UW, si cada
disco puede dar 10MB/s y el bus sólo puede sostener 40MB/s.
Además, sólo debería tener un dispositivo por bus IDE. El uso de discos
como maestro/esclavo es horrible para el rendimiento. IDE es realmente
malo accediendo a más de un disco por bus. Naturalmente, todas las placas
madre modernas tienen dos buses IDE, por lo que puede configurar dos discos
en RAID sin comprar más tarjetas controladoras.
<P>
La capa RAID no tiene absolutamente nada que ver con la capa del sistema de
ficheros. Puede poner cualquier sistema de ficheros sobre un dispositivo
RAID, justo como con cualquier otro dispositivo de bloques.
<P>

<SECT1>Términos
<P>
La palabra ``RAID'' significa ``RAID software de Linux''. Este CÓMO no trata
ningún aspecto de RAID hardware.
<P>
Cuando se describen configuraciones, es útil referirse al número de discos y
sus tamaños. En todos los casos se usa la letra <BF>N</BF> para denotar el
número de discos activos en el array (sin contar los discos de reserva). La
letra <BF>S</BF> es el tamaño del disco más pequeño en el array, a menos que
se diga otra cosa. La letra <BF>P</BF> representa el rendimiento de un
disco en el array, en MB/s. Cuando se use, supondremos que los discos son
igual de rápidos, que no siempre puede ser cierto.
<P>
Note que se supone que las palabras ``dispositivo'' y ``disco'' significan
lo mismo. Normalmente, los dispositivos usados para construir un dispositivo
RAID son particiones de discos, no necesariamente discos enteros. Pero,
normalmente, combinar varias particiones de un disco no tiene sentido, por
lo que las palabras dispositivo y disco simplemente significan ``particiones
de discos diferentes''.
<P>

<SECT1>Niveles RAID
<P>
Lo siguiente es una breve descripción de lo que soportan los parches RAID de
Linux. Parte de esta información es información RAID absolutamente básica,
aunque he añadido unas pocas reseñas de lo que hay de especial en la
implementación de Linux de los niveles. Simplemente, sáltese esta sección si
conoce RAID. Regrese después cuando tenga problemas   :)
<P>
Los actuales parches RAID para Linux soportan los siguientes niveles:
<ITEMIZE>
<ITEM><BF>Modo Lineal (Linear mode)</BF>
<ITEMIZE>
<ITEM>Dos o más discos se combinan en un único dispositivo físico. Los
discos se ``adjuntan'' unos a otros de tal manera que las escrituras en el
dispositivo RAID primero llenarán el disco 0, a continuación el disco 1 y
así sucesivamente. Los discos no tienen por qué ser del mismo tamaño. De
hecho, los tamaños no importan para nada aquí   :)
<ITEM>No existe redundancia en este nivel. Si un disco falla perderá todos
sus datos con toda probabilidad. Sin embargo, puede tener suerte y recuperar
algunos datos, ya que el sistema de ficheros simplemente habrán perdido un
gran puñado de datos consecutivos.
<ITEM>El rendimiento de las lecturas y las escrituras no se incrementará
para lecturas/escrituras individuales. Pero si varios usuarios usan el
dispositivo, puede tener la suerte de que un usuario use efectivamente el
primer disco y el otro usuario acceda a ficheros que por casualidad residan
en el segundo disco. Si esto ocurre, verá un aumento en el rendimiento.
</ITEMIZE>
<ITEM><BF>RAID-0</BF>
<ITEMIZE>
<ITEM>También llamado modo ``stripe'' o distribución por bandas. Como el modo
lineal salvo que las lecturas y escrituras se realizan en paralelo en los
dispositivos. Los dispositivos deben tener aproximadamente el mismo tamaño.
Puesto que todos los accesos se realizan en paralelo, los discos se llenan
por igual. Si un dispositivo es mucho mayor que los otros dispositivos, el
espacio extra todavía se utilizará en el dispositivo RAID durante las
escrituras en el extremo superior del dispositivo RAID, aunque sólo se
accederá a este disco más grande. Naturalmente, esto perjudica el rendimiento.
<ITEM>Como en el modo lineal, tampoco hay redundancia en este nivel.
A diferencia del modo lineal, no será capaz de recuperar ningún dato si un
disco falla. Si elimina un disco de un grupo RAID-0, el dispositivo RAID no
perderá simplemente un bloque consecutivo de datos, sino que se llenará con
pequeños agujeros por todo el dispositivo. Probablemente, e2fsck no será
capaz de recuperar gran cosa de dicho dispositivo.
<ITEM>El rendimiento de las lecturas y las escrituras se incrementará, ya
que las lecturas y las escrituras se realizan en paralelo sobre los
dispositivos. Normalmente, ésta es la razón principal para usar RAID-0. Si
los buses a los discos son suficientemente rápidos, puede obtener casi N*P
MB/seg.
</ITEMIZE>
<ITEM><BF>RAID-1</BF>
<ITEMIZE>
<ITEM>Este es el primer modo que realmente tiene redundancia. RAID-1 se
puede usar en dos o más discos con cero o más discos de reserva. Este modo
mantiene en un disco un duplicado exacto de la información en el otro(s)
disco(s). Por supuesto, los discos deben ser del mismo tamaño. Si un disco
es mayor que otro, su dispositivo RAID será del tamaño del disco más
pequeño.
<ITEM>Si se eliminan (o fallan) hasta N-1 discos, todos los datos todavía
permanecerán intactos. Si existen discos de reserva disponibles y el sistema
(es decir, las controladoras SCSI o los chipsets IDE, etc.) sobreviven al
desastre, comenzará inmediatamente la reconstrucción de un duplicado en uno
de los discos de reserva, después de la detección del fallo del disco.
<ITEM>Normalmente, el rendimiento de las lecturas aumenta hasta casi N*P,
mientras que el rendimiento de las escrituras es el mismo que el de un único
dispositivo o, tal vez, incluso menos. Las lecturas se pueden hacer en
paralelo pero, cuando se escribe, la CPU debe transferir N veces la cantidad
de datos que normalmente transferiría (recuerde, se deben enviar N copias
idénticas de todos los datos a los discos).
</ITEMIZE>
<ITEM><BF>RAID-4</BF>
<ITEMIZE>
<ITEM>Este nivel de RAID no se usa con mucha frecuencia. Se puede usar sobre
3 o más discos. En lugar de duplicar completamente la información, guarda
información de paridad en un único disco y escribe datos a los otros discos
de forma parecida a un RAID-0. Ya que uno de los discos se reserva para
información de paridad, el tamaño del array será (N-1)*S, donde S es el
tamaño del disco más pequeño del array. Como en un RAID-1, los discos deben
ser del mismo tamaño o de lo contrario tendrá que aceptar que el valor de S
en la fórmula (N-1)*S anterior será el tamaño del disco más pequeño del
array.
<ITEM>Si un disco falla, se puede usar la información de paridad para
reconstruir todos los datos. Si dos discos fallan, se perderán todos los
datos.
<ITEM>La razón por la que este nivel no se usa con mucha frecuencia es que la
información de paridad se guarda en un único disco. Esta información se debe
actualizar <EM>cada</EM> vez que se escribe en uno de los otros discos. Por
eso, el disco de paridad se convertirá en un cuello de botella si no es
mucho más rápido que los otros discos. Sin embargo, si por pura casualidad
tuviera muchos discos lentos y un disco muy rápido, este nivel de RAID
podría ser muy útil.
</ITEMIZE>
<ITEM><BF>RAID-5</BF>
<ITEMIZE>
<ITEM>Este es quizás el modo RAID más útil cuando uno desea combinar un
mayor número de discos físicos y todavía conservar alguna redundancia.
RAID-5 se puede usar sobre 3 o más discos, con cero o más discos de reserva.
El tamaño del dispositivo RAID-5 resultante será (N-1)*S, justo como en
RAID-4. La gran diferencia entre RAID-5 y RAID-4 es que la información de
paridad se distribuye uniformemente entre los discos participantes, evitando
el problema del cuello de botella del RAID-4.
<ITEM>
Si uno de los discos falla, todos los datos todavía permanecerán intactos,
gracias a la información de paridad. Si existen discos de reserva
disponibles, la reconstrucción comenzará inmediatamente después del fallo
del dispositivo. Si dos discos fallan simultáneamente, todos los dato se
perderán. RAID-5 puede sobrevivir a un fallo de disco, pero no a dos o más.
<ITEM>Normalmente, el rendimiento de las lecturas y las escrituras se
incrementará, pero es difícil predecir en qué medida.
</ITEMIZE>
</ITEMIZE>

<SECT2>Discos de reserva
<P>
Los discos de reserva son discos que no forman parte del grupo RAID hasta
que uno de los discos activos falla. Cuando se detecta un fallo de disco, el
dispositivo se marca como ``defectuoso'' y la reconstrucción se inicia
inmediatamente sobre el primer disco de reserva disponible.  
<P>
De esta manera, los discos de reservan proporcionan una buena seguridad
extra, especialmente, a sistemas RAID-5 que, tal vez, son difíciles de
lograr (físicamente). Se puede permitir que el sistema funcione durante
algún tiempo con un dispositivo defectuoso, ya que se conserva toda la
redundancia mediante los discos de reserva.
<P>
No puede estar seguro de que su sistema sobrevivirá a una caída de disco. La
capa RAID puede que maneje los fallos de dispositivos verdaderamente bien,
pero las controladoras SCSI se podrían romper durante el manejo del error o
el chipset IDE podría bloquearse, o podrían ocurrir muchas otras cosas.
<P>


<SECT1>Intercambio sobre RAID
<P>
No hay ninguna razón para usar RAID por razones de rendimiento del intercambio. El
propio núcleo puede repartir el intercambio entre varios dispositivos si
simplemente les da la misma prioridad en el fichero fstab.
<P>
Un buen fstab se parece a éste:
<VERB>
/dev/sda2       swap           swap    defaults,pri=1   0 0
/dev/sdb2       swap           swap    defaults,pri=1   0 0
/dev/sdc2       swap           swap    defaults,pri=1   0 0
/dev/sdd2       swap           swap    defaults,pri=1   0 0
/dev/sde2       swap           swap    defaults,pri=1   0 0
/dev/sdf2       swap           swap    defaults,pri=1   0 0
/dev/sdg2       swap           swap    defaults,pri=1   0 0
</VERB>
Esta configuración permite a la máquina intercambiar en paralelo sobre siete
dispositivos SCSI. No necesita RAID, ya que esa ha sido una característica
del núcleo desde hace mucho tiempo.
<P>
Otra razón para usar RAID para intercambio es la alta disponibilidad. Si
configura un sistema para arrancar desde, por ejemplo, un dispositivo
RAID-1, el sistema podría ser capaz de sobrevivir a un fallo de disco. Pero si
el sistema ha estado intercambiando sobre el ahora dispositivo defectuoso,
puede estar seguro de que se vendrá abajo. El intercambio sobre un
dispositivo RAID-1 solucionaría este problema.
<P>
Sin embargo, el intercambio sobre RAID-{1,4,5} <BF>NO</BF> está soportado.
Puede configurarlo, pero fracasará. La razón es que la capa RAID algunas
veces reserva memoria antes de realizar una escritura. Esto produce un
bloqueo mortal ya que el núcleo tendrá que reservar memoria antes de que
pueda intercambiar, e intercambiar antes de que pueda reservar memoria.
<P>
Es triste pero cierto, al menos por ahora.
<P>

<SECT>Asuntos hardware
<P>
Esta sección mencionará algunos de los asuntos hardware involucrados en el
funcionamiento de un RAID software.
<P>
<SECT1>Configuración IDE
<P>
En efecto, es posible hacer funcionar un RAID sobre discos IDE. También se
puede obtener un rendimiento excelente. De hecho, el precio actual de los
discos y las controladoras IDE hacen de IDE algo a tener en cuenta cuando se
montan nuevos sistemas RAID.
<ITEMIZE>
<ITEM><BF>Estabilidad física:</BF> tradicionalmente, los discos IDE han sido
de peor calidad mecánica que los discos SCSI. Incluso hoy en día, la
garantía de los discos IDE es típicamente de un año, mientras que,
a menudo, es de 3 a 5 años en los discos SCSI. Aunque no es justo decir que
los discos IDE están, por definición, malamente hechos, uno debería ser
consciente de que los discos IDE de <EM>algunas</EM> marcas <EM>pueden</EM>
fallar con más frecuencia que los discos SCSI similares. Sin embargo, otras
marcas usan exactamente la misma estructura mecánica tanto para los discos
SCSI como para los discos IDE. Todo se reduce a: todos los discos fallan,
tarde o temprano, y uno debería estar preparado para ello.
</ITEM>
<ITEM><BF>Integridad de los datos:</BF> al principio, IDE no tenía forma de
asegurar que los datos enviados a través del bus IDE eran los mismos que los
datos escritos realmente en el disco. Esto se debió a la falta total de
paridad, sumas de verificación (``checksums''), etc. Ahora, con el estándar
UltraDMA, los dispositivos IDE realizan una suma de verificación sobre los
datos que reciben y por eso es altamente improbable que los datos se
corrompan.
</ITEM>
<ITEM><BF>Rendimiento:</BF> no voy a escribir aquí sobre el rendimiento de
IDE de forma detallada. La historia realmente corta es:
<ITEMIZE>
<ITEM>Los dispositivos IDE son rápidos (12 MB/s y más)</ITEM>
<ITEM>IDE tiene una mayor sobrecarga de CPU que SCSI (pero, ¿a quién le
preocupa?)</ITEM>
<ITEM>Sólo usa <BF>un</BF> disco IDE por bus IDE, los discos esclavos
deterioran el rendimiento</ITEM>
</ITEMIZE>
</ITEM>
<ITEM><BF>Supervivencia a los fallos:</BF> la controladora IDE normalmente
sobrevive a un dispositivo IDE que ha fallado. La capa RAID marcará el disco
como defectuoso y, si está trabajando con un RAID de nivel 1 o superior, la
máquina debería trabajar igual de bien hasta que la desconecte para su
mantenimiento.</ITEM>
</ITEMIZE>
<P>
Es <BF>muy</BF> importante que sólo use <BF>un</BF> disco IDE por bus IDE.
Dos discos no sólo arruinarían el rendimiento sino que, también, el fallo de
un disco a menudo garantiza el fallo del bus y, por tanto, el fallo de todos
los discos de ese bus. En una configuración RAID tolerante a fallos (RAID de
niveles 1, 4, 5) el fallo de un disco se puede manejar pero el fallo de dos
discos (los dos discos del bus que ha fallado debido al fallo de uno de los
discos) dejará el array inutilizable. También, el dispositivo esclavo o la
controladora IDE de un bus pueden confundirse de manera horrible cuando el
dispositivo maestro del bus falla. Un bus, un disco, esa es la regla.
<P>
Existen por ahí controladoras IDE PCI baratas. A menudo puede obtener 2 o 4
buses por unos 80 dólares. Considerando el precio mucho más bajo de los
discos IDE respecto a los discos SCSI, diría que un array de discos IDE
podría ser una solución realmente buena si uno puede vivir con los
relativamente pocos discos (unos 8 probablemente) que se pueden conectar a
un sistema típico (a menos que, naturalmente, tenga muchas ranuras PCI para
esas controladoras IDE).
<P>
 
<SECT1>Intercambio de discos en caliente
<P>
Éste ha sido un tema de actualidad en la lista linux-kernel durante algún
tiempo. Aunque el intercambio en caliente de los dispositivos está soportado
hasta cierto punto, todavía no es algo que se pueda hacer fácilmente.
<P>
<SECT2>Intercambio en caliente de dispositivos IDE
<P>
<BF>¡No lo haga!</BF> IDE no maneja para nada el intercambio en caliente.
Seguro, puede funcionar para usted si su manejador IDE se compila como
módulo (sólo posible en la serie 2.2 del núcleo) y lo vuelve a cargar
después de que haya reemplazado el dispositivo. Pero también puede terminar
perfectamente con una controladora IDE frita y observará un tiempo de
inactividad mucho mayor que el que simplemente habría tomado el reemplazar
el dispositivo en un sistema apagado.
<P>
El principal problema, a parte de los temas eléctricos que pueden destruir
su hardware, es que se debe reexplorar el bus IDE después de que se hayan
intercambiado los discos. El manejador IDE actual no puede hacer eso. Si el
nuevo disco es 100% idéntico al antiguo (geometría, etc.) <EM>puede</EM> que
funcione incluso sin volver a explorar el bus pero, créame, aquí está
caminando por un borde afilado.
<P>
<SECT2>Intercambio en caliente de dispositivos SCSI
<P>
El hardware SCSI normal tampoco es capaz de realizar un intercambio en
caliente. Sin embargo, <EM>puede</EM> que funcione. Si su manejador SCSI
soporta la reexploración del bus y la conexión y desconexión de
dispositivos, puede ser capaz de intercambiar dispositivos en caliente. Sin
embargo, en un bus SCSI normal probablemente no debería desenchufar
dispositivos mientras su sistema esté todavía encendido. Pero, le repito,
puede que funcione simplemente (y también puede terminar con su hardware
frito).
<P>
La capa SCSI <BF>debería</BF> sobrevivir si un disco muere, pero no todos
los manejadores SCSI tratan esto todavía. Si su manejador SCSI muere cuando
un disco cae, su sistema se caerá con él y la conexión en caliente no será
verdaderamente interesante entonces.
<P>
<SECT2>Intercambio en caliente con SCA
<P>
Con SCA debería ser posible conectar dispositivos en caliente. Sin embargo,
no poseo el hardware para probar esto y no he oído de nadie que lo haya
probado, por lo que verdaderamente no puedo dar ninguna receta de cómo hacer
esto.
<P>
De todos modos, si quiere jugar con esto, debería conocer los aspectos
internos de SCSI y de RAID. Por tanto, no voy a escribir aquí nada que no
pueda comprobar que funciona. En cambio, sí puedo dar una pocas pistas: 
<ITEMIZE>
<ITEM>Busque la cadena <BF>remove-single-device</BF> en
<BF>linux/drivers/scsi/scsi.c</BF></ITEM>
<ITEM>Eche un vistazo a <BF>raidhotremove</BF> y <BF>raidhotadd</BF>
</ITEMIZE>
<P>
No todos los manejadores SCSI soportan la conexión y desconexión de
dispositivos. En la serie 2.2 del núcleo, al menos los manejadores de la
controladoras Adaptec 2940 y Symbios NCR53c8xx parecen soportarlo, mientras
que otras puede que sí o puede que no. Agradecería que alguien me pasara más
información sobre esto...
<P>


<SECT>Configuración de RAID
<P>
<SECT1>Configuración general
<P>
Esto es lo que necesita para cualquiera de los niveles RAID:
<ITEMIZE>
<ITEM>Un núcleo.  Obtenga la versión 2.0.36 o un núcleo 2.2.x reciente.
<ITEM>Los parches RAID.  Normalmente existe un parche disponible para los
núcleos recientes.
<ITEM>El paquete de herramientas RAID (raidtools).
<ITEM>Paciencia, una pizza y su bebida con cafeína favorita.
</ITEMIZE>
<P>
Todo este software se puede encontrar en &ftpkernel;. Las herramientas RAID y
los parches están en el subdirectorio <TT>daemons/raid/alpha</TT>. Los núcleos
se encuentran en el subdirectorio <TT>kernel</TT>.
<P>
Parchee el núcleo, configúrelo para incluir el soporte del nivel RAID que
quiera usar. Compílelo e instálelo.
<P>
A continuación desempaquete, configure, compile e instale las herramientas
RAID.
<P>
Vale, hasta ahora todo va bien. Si rearranca ahora, debería tener un fichero
llamado &mdstat;. Recuérdelo, ese fichero es su amigo. Vea lo que contiene
haciendo un <TT>cat </TT>&mdstat;. Le debe decir que tiene registrada la
personalidad RAID (es decir, el modo RAID) correcta y que actualmente no hay
dispositivos RAID activos.
<P>
Cree las particiones que quiere incluir en su grupo RAID.
<P>
Ahora, vayamos a un modo específico.
<P>

<SECT1>Modo lineal
<P>
De acuerdo, así que tiene dos o más particiones que no son necesariamente
del mismo tamaño (pero que, naturalmente, pueden serlo) que quiere adjuntar
unas con otras.
<P>
Prepare el fichero &raidtab; para describir su configuración. He preparado
un raditab para dos discos en modo lineal y el fichero se parece a esto:
<P>
<VERB>
raiddev /dev/md0
        raid-level      linear
        nr-raid-disks   2
        persistent-superblock 1
        device          /dev/sdb6
        raid-disk       0
        device          /dev/sdc5
        raid-disk       1
</VERB>
Aquí no se soportan discos de reserva. Si un disco muere, el array muere con
él. No hay información que poner en un disco de reserva.
<P>
Vale, creemos el array. Ejecute la orden
<VERB>
  mkraid /dev/md0
</VERB>
<P>
Esto inicializará su array, escribirá los superbloques persistentes y
arrancará el array.
<P>
Échele un vistazo a &mdstat;. Debe ver que el array está funcionando.
<P>
Ahora, puede crear un sistema de ficheros, justo como haría con
cualquier otro dispositivo, montarlo, incluirlo en su fstab, etc. 
<P>

<SECT1>RAID-0
<P>
Tiene dos o más dispositivos, de aproximadamente el mismo tamaño, y quiere
combinar sus capacidades de almacenamiento y también combinar sus
rendimientos accediéndolos en paralelo.
<P>
Prepare el fichero &raidtab; para describir su configuración. Un raidtab de
ejemplo se parece a esto:
<VERB>
raiddev /dev/md0
        raid-level      0
        nr-raid-disks   2
        persistent-superblock 1
        chunk-size     4
        device          /dev/sdb6
        raid-disk       0
        device          /dev/sdc5
        raid-disk       1
</VERB>
Como en el modo lineal, los discos de reserva tampoco se soportan aquí.
Un RAID-0 no tiene redundancia, por lo que cuando un disco muere, el array
le acompaña.
<P>
Una vez más, ejecute simplemente
<VERB>
  mkraid /dev/md0
</VERB>
para inicializar el array. Esto debe inicializar los superbloques y poner en
funcionamiento el dispositivo RAID. Éche un vistazo a &mdstat; para ver qué
sucede. Debería ver que su dispositivo ahora está en funcionamiento.
<P>
Ahora, /dev/md0 está listo para ser formateado, montado, usado y maltratado.
<P>

<SECT1>RAID-1
<P>
Tiene dos dispositivos de aproximadamente el mismo tamaño y quiere que cada
uno de los dos sea un duplicado del otro. Finalmente, tiene más dispositivos
que quiere guardar como discos de reserva preparados, que automáticamente
formarán parte del duplicado si uno de los dispositivos activos se rompe.
<P>
Configure el fichero &raidtab; así:
<VERB>
raiddev /dev/md0
        raid-level      1
        nr-raid-disks   2
        nr-spare-disks  0
        chunk-size      4
        persistent-superblock 1
        device          /dev/sdb6
        raid-disk       0
        device          /dev/sdc5
        raid-disk       1
</VERB>
Si tiene discos de reserva, puede añadirlos al final de la especificación de
dispositivos como
<VERB>
        device          /dev/sdd5
        spare-disk      0
</VERB>
Recuerde configurar la entrada <TT>nr-spare-disks</TT> adecuadamente.
<P>
De acuerdo, ahora estamos listos para comenzar la inicialización del RAID.
Se debe construir el duplicado, es decir, los contenidos (de todos modos,
sin importancia ahora, ya que el dispositivo todavía está sin formatear) de
los dos dispositivos se deben sincronizar.
<P>
Dé la orden
<VERB>
  mkraid /dev/md0
</VERB>
para comenzar la inicialización del duplicado.
<P>
Compruebe el fichero &mdstat;. Debe decirle que se ha puesto en
funcionamiento el dispositivo /dev/md0, que está siendo reconstruido el
duplicado y una cuenta del tiempo estimado para la terminación de la
reconstrucción.
<P>
La reconstrucción se realiza usando el ancho de banda ocioso de E/S. De esta
manera, su sistema todavía debería ser capaz de responder en gran medida,
aunque los LEDs de sus discos deben estar bonitamente resplandecientes.
<P>
El proceso de reconstrucción es transparente, por lo que realmente puede
usar el dispositivo aunque la duplicación esté actualmente en
reconstrucción.
<P>
Intente formatear el dispositivo mientras la reconstrucción se esté
realizando. Funcionará. También puede montarlo y usarlo mientras la
reconstrucción se esté realizando. Naturalmente, si el disco equivocado
se rompe mientras se está realizando la reconstrucción, no hay solución.
<P>

<SECT1>RAID-4
<P>-
<BF>¡Nota!</BF> No he comprobado esta configuración por mí mismo. La
configuración de más abajo es mi mejor suposición, no algo que realmente haya
tenido funcionando.
<P>
Tiene tres o más dispositivos de aproximadamente el mismo tamaño, un
dispositivo es significativamente más rápido que los otros dispositivos y
quiere combinarlos todos en un único dispositivo más grande, conservando
todavía alguna información de redundancia. Finalmente, tiene varios
dispositivos que desea usar como discos de reserva.
<P>
Configure el fichero /etc/raidtab así:
<VERB>
raiddev /dev/md0
        raid-level      4
        nr-raid-disks   4
        nr-spare-disks  0
	persistent-superblock 1
        chunk-size      32
        device          /dev/sdb1
        raid-disk       0
        device          /dev/sdc1
        raid-disk       1
        device          /dev/sdd1
        raid-disk       2
        device          /dev/sde1
        raid-disk       3
</VERB>
Si tuviéramos discos de reserva, se insertarían de forma parecida, siguiendo
las especificaciones de discos RAID;
<VERB>
        device         /dev/sdf1
        spare-disk     0
</VERB>
como de costumbre.
<P>
Su array se puede inicializar con la orden
<VERB>
   mkraid /dev/md0
</VERB>
como es habitual.
<P>
Debería ver la sección de opciones especiales de mke2fs antes de formatear
el dispositivo.
<P>


<SECT1>RAID-5
<P>
Tiene tres o más dispositivos de aproximadamente el mismo tamaño, quiere
combinarlos en un dispositivo mayor, pero conservando todavía cierto grado
de redundancia para la seguridad de datos. Finalmente, tiene varios
dispositivos para usar como discos de reserva, que no tomarán parte en el
array antes de que otro dispositivo falle.
<P>
Si usa N dispositivos donde el tamaño del más pequeño es S, el tamaño de todo
el array será (N-1)*S. El espacio ``faltante'' se usa para información
de paridad (redundancia). De esta manera, si cualquier disco falla, todos
los datos permanecerán intactos. Pero si dos discos fallan, todos los datos
se perderán.
<P>
Configure el fichero /etc/raidtab así:
<VERB>
raiddev /dev/md0
        raid-level      5
        nr-raid-disks   7
        nr-spare-disks  0
	persistent-superblock 1
        parity-algorithm        left-symmetric
        chunk-size      32
        device          /dev/sda3
        raid-disk       0
        device          /dev/sdb1
        raid-disk       1
        device          /dev/sdc1
        raid-disk       2
        device          /dev/sdd1
        raid-disk       3
        device          /dev/sde1
        raid-disk       4
        device          /dev/sdf1
        raid-disk       5
        device          /dev/sdg1
        raid-disk       6
</VERB>
Si tuviéramos discos de reserva, se insertarían de forma parecida, siguiendo
las especificaciones de discos RAID;
<VERB>
        device         /dev/sdh1
        spare-disk     0
</VERB>
Y así sucesivamente.
<P>
Un tamaño de porción de 32KB es un buen valor por defecto para muchos
sistemas de ficheros de propósito general de este tamaño. El array sobre el
que se utiliza el raidtab anterior es un dispositivo de 7 por 6 GB = 36 GB
(recuerde que (N-1)*S = (7-1)*6 = 36). Contiene un sistema de ficheros ext2
con un tamaño de bloque de 4KB. Podría incrementar tanto el tamaño de
porción del array como el tamaño de bloque del sistema de ficheros si
su sistema de ficheros fuera o bien mucho mayor o bien si simplemente
contuviera ficheros muy grandes.
<P>
Vale, ya hemos hablado bastante. Configure el fichero raidtab y
veamos si funciona. Ejecute la orden
<VERB>
  mkraid /dev/md0
</VERB>
y observe qué ocurre. Es de esperar que sus discos comiencen a trabajar como
locos debido a que empiezan la reconstrucción de su array. Échele un
vistazo  a &mdstat; para ver qué está sucediendo.
<P>
Si el dispositivo se ha creado correctamente, el proceso de reconstrucción
comenzará ahora. Su array no será consistente hasta que esta fase de
reconstrucción haya terminado. No obstante, el array es totalmente funcional
(excepto, por supuesto, para el manejo de fallos de dispositivos) y puede
formatearlo y usarlo incluso mientras se esté reconstruyendo.
<P>
Consulte la sección de opciones especiales de mke2fs antes de formatear el
array.
<P>
Bueno, ahora que ya tiene su dispositivo RAID funcionando, siempre puede
pararlo o rearrancarlo usando las órdenes
<VERB>
  raidstop /dev/md0
</VERB>
y
<VERB>
  raidstart /dev/md0,
</VERB>
respectivamente.
<P>
En lugar de colocar éstos en ficheros de inicio y rearrancar un número
astronómico de veces hasta hacer que funcione, siga leyendo y haga funcionar
la autodetección.
<P>

<SECT1>El superbloque persistente
<P>
Volviendo a los ``buenos viejos tiempos'' (``The Good Old Days'' (TM)),
las herramientas RAID (raidtools) leerían su fichero &raidtab; y a
continuación inicializarían el array. Sin embargo, esto requeriría que el
sistema de ficheros sobre el que reside &raidtab; estuviera montado. Esto
es desafortunado si quiere arrancar a partir de un RAID.
<P>
También, la anterior aproximación producía complicaciones al montar
sistemas de ficheros sobre dispositivos RAID. Éstos no se podían colocar en
el fichero &fstab; como era usual, sino que tenían que ser montados en los
guiones de inicio.
<P>
Los superbloques persistentes solucionan estos problemas. Cuando un array se
inicializa con la opción <TT>persistent-superblock</TT> en el fichero
&raidtab;, se escribe un superbloque especial al principio de todos los
discos participantes en el array. Esto permite al núcleo leer la
configuración de los dispositivos RAID directamente de los discos
involucrados, en lugar de leerla de algún fichero de configuración que puede
no estar disponible en todo momento.
<P>
Sin embargo, todavía debería mantener un fichero &raidtab; consistente, ya que
puede necesitar este fichero para una reconstrucción posterior del array.
<P>
Los superbloques persistentes son obligatorios si desea la autodetección de
sus dispositivos RAID durante el arranque del sistema. Esto se describe en
la sección <BF>Autodetección</BF>.
<P>

<SECT1>Tamaños de porción
<P>
El tamaño de porción merece una explicación. Nunca puede escribir de forma
totalmente paralela a un grupo de discos. Si tuviera dos discos y quisiera
escribir un byte, tendría que escribir cuatro bits en cada disco; realmente,
todos los segundos bits irían al disco 0 y los otros al disco 1.
Sencillamente, el hardware no soporta eso. En su lugar, elegimos algún
tamaño de porción que definimos como la masa ``atómica'' más pequeña de datos
que puede ser escrita en los dispositivos. Una escritura de 16 KB con un
tamaño de porción de 4 KB provocaría que la primera y tercera porción de 4KB
se escribieran en el primer disco y la segunda y el cuarta porción en el
segundo, en el caso de un RAID-0 de dos discos. De esta manera, para grandes
escrituras, podría observar una sobrecarga más pequeña teniendo porciones lo
bastante grandes, mientras que los arrays que contuvieran principalmente
ficheros pequeños se podrían beneficiar más de un tamaño de porción más
pequeño.
<P>
Los tamaños de porción se pueden especificar para todos los niveles de RAID
excepto para el modo lineal.
<P>
Para un rendimiento óptimo, debería experimentar con el valor, así como con
el tamaño de bloque del sistema de ficheros que pusiera en el array.
<P>
El argumento de la opción <TT>chunk-size</TT> en &raidtab; especifica el
tamaño de porción en kilobytes. Por tanto, ``4'' significa ``4 KB''.
<P>
<SECT2>RAID-0
<P>
Los datos se escriben ``casi'' en paralelo en todos los discos del array.
Realmente, se escriben <TT>chunk-size</TT> bytes en cada disco, de forma
consecutiva.
<P>
Si especifica un tamaño de porción de 4 KB y escribe 16 KB a un array de 3
discos, el sistema RAID escribirá 4 KB a los discos 0, 1 y 2, en paralelo, y
a continuación los 4 KB restantes al disco 0.
<P>
Un tamaño de porción de 32 KB es un punto de inicio razonable para la mayoría
de los arrays. Pero el valor óptimo depende muchísimo del número de discos
implicados, del contenido del sistema de ficheros que coloca y de muchos
otros factores. Experimente con él para obtener el mejor rendimiento.
<P>
<SECT2>RAID-1
<P>
Para las escrituras, el tamaño de porción no afecta al array, ya que se deben
escribir todos los datos a todos los discos sin importar qué. Para las
lecturas, sin embargo, el tamaño de porción indica cuántos datos leer
consecutivamente de los discos participantes. Ya que todos los discos activos
del array contienen la misma información, las lecturas se pueden hacer en
paralelo al estilo de un RAID-0.
<P>
<SECT2>RAID-4
<P>
Cuando se realiza una escritura en un array RAID-4, también se debe
actualizar la información de paridad en el disco de paridad. El tamaño de
porción es el tamaño de los bloques de paridad. Si se escribe un byte a un
array RAID-4, entonces se leerán <TT>chunk-size</TT> bytes de los N-1
discos, se calculará la información de paridad y se escribirán
<TT>chunk-size</TT> bytes al disco de paridad.
<P>
El tamaño de porción afecta al rendimiento de las lecturas de la misma manera
que en un RAID-0, ya que las lecturas de un RAID-4 se realizan de la misma
forma.
<P>
<SECT2>RAID-5
<P>
En RAID-5 el tamaño de porción tiene exactamente el mimo significado que en un
RAID-4.
<P>
Un tamaño de porción razonable para un RAID-5 es 128 KB pero, como siempre,
puede desear experimentar con éste.
<P>
También consulte la sección de opciones especiales de mke2fs. Esto afecta al
rendimiento de un RAID-5.
<P>

<SECT1>Opciones de mke2fs
<P>
Hay disponible una opción especial cuando se formatean dispositivos RAID-4 y
RAID-5 con mke2fs. La opción <TT>-R stride=nn</TT> permitirá a mke2fs
situar mejor diferentes estructuras de datos específicas de ext2 en un
dispositivo RAID de forma inteligente.
<P>
Si el tamaño de porción es 32 KB significa que 32 KB de datos consecutivos
residirán en un único disco. Si queremos construir un sistema de ficheros
ext2 con un tamaño de bloque de 4KB, nos damos cuenta de que habrá 8 bloques
del sistema de ficheros en una porción del array. Podemos pasar esta
información a la utilidad mke2fs cuando se cree el sistema de ficheros:
<VERB>
  mke2fs -b 4096 -R stride=8 /dev/md0
</VERB>
<P>
El rendimiento de un RAID-{4,5} se ve fuertemente influido por esta opción. No
estoy seguro de cómo la opción <TT>stride</TT> afectará a otros niveles
RAID. Si alguien tiene información sobre esto, por favor, que la envíe a mi
dirección.
<P>

<SECT1>Autodetección
<P>
La autodetección permite a los dispositivos RAID ser automáticamente
reconocidos por el núcleo durante el arranque, justo después de que se
realice la detección ordinaria de particiones.
<P>
Esto requiere varias cosas:
<ENUM>
<ITEM>Necesita soporte para autodetección en el núcleo. Compruebe esto
<ITEM>Debe haber creado los dispositivos RAID usando superbloques
persistentes
<ITEM>El tipo de partición de los dispositivos usados en el RAID se debe
establecer a <BF>0xFD</BF>  (use fdisk y establezca el tipo a ``fd'')
</ENUM>
<P>
NOTA: asegúrese de que su RAID NO ESTÁ FUNCIONANDO antes de cambiar los
tipos de las particiones. Use <TT>raidstop /dev/md0</TT> para parar el
dispositivo.
<P>
Si sigue los pasos 1, 2 y 3 de arriba, la autodetección debería activarse.
Pruebe rearrancar. Cuando el sistema se levante, vea el contenido de
&mdstat;; debería decirle que su RAID está funcionando.
<P>
Durante el arranque, podría ver mensajes similares a estos:
<VERB>
 Oct 22 00:51:59 malthe kernel: SCSI device sdg: hdwr sector= 512
  bytes. Sectors= 12657717 [6180 MB] [6.2 GB]
 Oct 22 00:51:59 malthe kernel: Partition check:
 Oct 22 00:51:59 malthe kernel:  sda: sda1 sda2 sda3 sda4
 Oct 22 00:51:59 malthe kernel:  sdb: sdb1 sdb2
 Oct 22 00:51:59 malthe kernel:  sdc: sdc1 sdc2
 Oct 22 00:51:59 malthe kernel:  sdd: sdd1 sdd2
 Oct 22 00:51:59 malthe kernel:  sde: sde1 sde2
 Oct 22 00:51:59 malthe kernel:  sdf: sdf1 sdf2
 Oct 22 00:51:59 malthe kernel:  sdg: sdg1 sdg2
 Oct 22 00:51:59 malthe kernel: autodetecting RAID arrays
 Oct 22 00:51:59 malthe kernel: (read) sdb1's sb offset: 6199872
 Oct 22 00:51:59 malthe kernel: bind<sdb1,1>
 Oct 22 00:51:59 malthe kernel: (read) sdc1's sb offset: 6199872
 Oct 22 00:51:59 malthe kernel: bind<sdc1,2>
 Oct 22 00:51:59 malthe kernel: (read) sdd1's sb offset: 6199872
 Oct 22 00:51:59 malthe kernel: bind<sdd1,3>
 Oct 22 00:51:59 malthe kernel: (read) sde1's sb offset: 6199872
 Oct 22 00:51:59 malthe kernel: bind<sde1,4>
 Oct 22 00:51:59 malthe kernel: (read) sdf1's sb offset: 6205376
 Oct 22 00:51:59 malthe kernel: bind<sdf1,5>
 Oct 22 00:51:59 malthe kernel: (read) sdg1's sb offset: 6205376
 Oct 22 00:51:59 malthe kernel: bind<sdg1,6>
 Oct 22 00:51:59 malthe kernel: autorunning md0
 Oct 22 00:51:59 malthe kernel: running: <sdg1><sdf1><sde1><sdd1><sdc1><sdb1>
 Oct 22 00:51:59 malthe kernel: now!
 Oct 22 00:51:59 malthe kernel: md: md0: raid array is not clean --
  starting background reconstruction 
</VERB>
Esta es la salida de la autodetección de un array RAID-5 que no fue
limpiamente desactivado (es decir, la máquina se cayó). La reconstrucción se
inicia automáticamente. Montar este dispositivo es perfectamente seguro, ya
que la reconstrucción es transparente y todos los datos son consistentes
(sólo es la información de paridad la que es inconsistente - aunque la misma
no se necesita hasta que un dispositivo falle).
<P>
Los dispositivos autoarrancados también son automáticamente parados durante
el cierre del sistema. No se preocupe de los guiones de inicio. Simplemente,
use los dispositivos /dev/md como cualquier otro dispositivo /dev/sd o
/dev/hd.
<P>
Sí, verdaderamente es así de fácil.
<P>
Quizás desee buscar cualquier orden raidstart/raidstop en sus guiones de
inicio. Éstas suelen encontrarse en los guiones de inicio estándares de
RedHat. Se usan para el antiguo estilo de RAID y no tienen utilidad en el nuevo
estilo de RAID con autodetección. Simplemente, elimine las líneas y todo
irá perfectamente bien.
<P>

<SECT1>Arrancar desde un RAID
<P>
Existen varias formas de configurar un sistema que monta su sistema de
ficheros raíz sobre un dispositivo RAID. Desafortunadamente, ninguna de las
distribuciones de Linux con las que yo he probado (RedHat y Debian) soportan 
un dispositivo RAID como dispositivo del sistema de ficheros raíz durante el
proceso de instalación. Por tanto, le va a doler un poco si quiere
esto pero, de hecho, es posible.
<P>
Actualmente, LILO no maneja dispositivos RAID y ,por ello, no se puede
cargar el núcleo desde un dispositivo RAID en el instante del arranque. Su
sistema de ficheros <TT>/boot</TT> tendrá que residir en un dispositivo que
no sea RAID. Un modo de asegurar que su sistema arranca, pase lo que pase, es
crear particiones <TT>/boot</TT> similares en todas las unidades de su RAID,
de esa forma la BIOS siempre puede cargar datos desde, por ejemplo, la
primera unidad disponible. Esto necesita que no arranque con un disco
defectuoso en su sistema.
<P>
Otra forma de asegurar que su sistema siempre arranca es crear un disquete de
arranque cuando toda la configuración se haya terminado. Si muere el disco en
el que reside el sistema de ficheros <TT>/boot</TT>, siempre puede arrancar
desde el disquete.
<P>
<SECT2>Método 1
<P>
Este método asume que posee un disco de reserva en el que puede instalar el
sistema y que no es parte del RAID que configurará más adelante.
<P>
<ITEMIZE>
<ITEM>Primero, instale un sistema normal en su disco extra.</ITEM>
<ITEM>Obtenga el núcleo que piensa ejecutar, obtenga los parches y las
herramientas RAID y haga que su sistema arranque con el nuevo núcleo con
soporte RAID. Asegúrese de que el soporte RAID está <BF>dentro</BF> del
núcleo y que no se carga como módulo.
<ITEM>Vale, ahora debe configurar y crear el RAID que tiene pensado usar
para el sistema de ficheros raíz. Éste es un procedimiento estándar como ya
se describió en otra parte de este documento.</ITEM>
<ITEM>Simplemente para asegurarse de que todo está bien, trate de rearrancar
el sistema para ver si el nuevo RAID aparece durante el arranque. Debería
aparecer.</ITEM>
<ITEM> Coloque un sistema de ficheros sobre el nuevo array (usando
<TT>mke2fs</TT>), y móntelo en /mnt/newroot.</ITEM>
<ITEM>Ahora, copie el contenido de su sistema de ficheros raíz actual (el
disco extra) al nuevo sistema de ficheros raíz (el array). Hay muchas formas
de hacer esto. Una de ellas es
<VERB>
 cd /
 find . -xdev | cpio -pm /mnt/newroot
</VERB></ITEM>
<ITEM>Debe modificar el fichero <TT>/mnt/newroot/etc/fstab</TT> para usar el
dispositivo correcto (el dispositivo raíz <TT>/dev/md?</TT>) para el sistema
de ficheros raíz.</ITEM>
<ITEM>Ahora, desmonte el sistema de ficheros <TT>/boot</TT> actual y móntelo
en su lugar en <TT>/mnt/newroot/boot</TT>. Esto es necesario para que LILO
funcione correctamente en el siguiente paso.</ITEM>
<ITEM>Actualice <TT>/mnt/newroot/etc/lilo.conf</TT> para que apunte a los
dispositivos correctos. El dispositivo de arranque debe ser todavía un disco
normal (no un dispositivo RAID) pero el dispositivo raíz debe apuntar a su
nuevo RAID. Cuando esté hecho, ejecute <VERB> lilo -r /mnt/newroot</VERB>
Esta ejecución de LILO debería terminar sin errores.</ITEM>
<ITEM>Rearranque el sistema y observe que todo aparece como se esperaba :)
</ITEM>
</ITEMIZE>
<P>
Si está haciendo esto con discos IDE, asegúrese de indicarle a su BIOS que
todos los discos son del tipo ``auto-detect'', así la BIOS permitirá a su
máquina arrancar incluso cuando un disco haya fallado.
<P>
<SECT2>Método 2
<P>
Este método necesita que parchee su paquete raidtools para poder incluir la
directiva <TT>failed-disk</TT> en &raidtab;. Busque en los archivos de la
lista de correo Linux-raid los mensajes enviados por Martin Bene, alrededor
del 23 de abril de 1999, donde se envió el parche <TT>failed-disk</TT>.
Se espera que esta funcionalidad esté pronto en el paquete raidtools
(para cuando esté leyendo esto puede que incluso no necesite parchear las
raidtools).
<P>
<BF>Sólo</BF> puede utilizar este método en RAIDs de niveles 1 o superiores.
La idea es instalar un sistema sobre un disco que es adrede marcado como
estropeado en el RAID, copiar a continuación el sistema en el RAID que
estará funcionando en modo degrado y finalmente hacer que el RAID use el ya
no necesario ``disco de instalación'', aniquilando la anterior instalación
pero haciendo que el RAID funcione en modo no degradado.
<P>
<ITEMIZE>
<ITEM>Primero, instale un sistema normal sobre un disco (que más tarde
formará parte de su RAID). ¡Es importante que este disco (o partición) no sea
el más pequeño. Si lo es, no será posible añadirlo al RAID más tarde!
<ITEM>A continuación, obtenga el núcleo, los parches, las herramientas,
etc., etc. Ya conoce el ejercicio. Haga que su sistema arranque con un nuevo
núcleo que tenga el soporte RAID que necesita compilado dentro del núcleo.
<ITEM>Ahora, configure el RAID con su dispositivo raíz actual como el
<TT>failed-disk</TT> (disco estropeado) en el fichero <TT>raidtab</TT>. No
coloque el <TT>failed-disk</TT> como el primer disco en el fichero
<TT>raidtab</TT>, eso le dará problemas para poner en marcha el RAID. Cree el
RAID y coloque un sistema de ficheros en él.</ITEM>
<ITEM>Pruebe a rearrancar y vea si el RAID aparece como debería
hacerlo.</ITEM>
<ITEM>Copie los ficheros del sistema y reconfigure el sistema para usar el
RAID como dispositivo raíz, como se ha descrito en la sección anterior.</ITEM>
<ITEM>Cuando su sistema arranque con éxito desde el RAID, puede modificar el
fichero <TT>raidtab</TT> para incluir el <TT>failed-disk</TT> anterior como
un disco <TT>raid-disk</TT> normal. Ahora, ejecute <TT>raidhotadd</TT> para
añadir el disco a su sistema RAID.</ITEM>
<ITEM>Ahora debería tener un sistema capaz de arrancar desde un RAID no
degradado.</ITEM> 
</ITEMIZE>
<P>


<SECT1>Dificultades
<P>
Nunca NUNCA <BF>nunca</BF> reparticione discos que son parte de un RAID que
está funcionando. Si debe alterar la tabla de particiones de un disco que es
parte de un RAID, pare primero el array y después reparticione.
<P>
Es fácil poner demasiados discos en un bus. Un bus Fast-Wide SCSI normal
puede sostener 10 MB/s que es menos de lo que muchos discos pueden obtener por
sí solos hoy en día. Por supuesto, colocar seis de tales discos en un bus no
le proporcionará el aumento de rendimiento esperado.
<P>
La mayoría de los controladores SCSI sólo le proporcionarán un rendimiento
extra si los buses SCSI son llevados prácticamente al máximo por los discos
conectados a ellos. No observará una mejora de rendimiento por usar dos
controladores 2940 con dos viejos discos SCSI en lugar de simplemente hacer
funcionar los dos discos sobre un único controlador.
<P>
Si olvida la opción <TT>persistent-superblock</TT> puede que su array no
arranque por las buenas después de que haya sido parado. Simplemente, recree
el array con la opción colocada correctamente en el fichero raidtab.
<P>
Si un RAID-5 no logra reconstruirse después de que un disco haya sido
eliminado y reinsertado, puede deberse al orden de los dispositivos en el
fichero raidtab. Intente mover el primer par ``device''--``raid-disk''
al final de la descripción del array en el fichero raidtab.
<P>

<SECT>Comprobación
<P>
Si piensa usar un RAID para obtener tolerancia a fallos, también puede que
quiera comprobar su configuración para ver si realmente funciona. Ahora
bien, ¿cómo se simula un fallo de disco?.
<P>
El resumen es que no puede, salvo quizás atravesando mediante un hacha de
fuego la unidad sobre la que quiere ``simular'' el fallo.
Nunca puede saber qué ocurrirá si un disco muere. Puede que se apodere
eléctricamente del bus al que está conectado, haciendo que todas las
unidades en ese bus sean inaccesibles, aunque yo nunca he oído que eso haya
ocurrido. La unidad también puede simplemente informar de un fallo de
lectura/escritura a la capa SCSI/IDE que a su vez hará que la capa RAID
maneje esta situación de forma elegante. Afortunadamente, esta es la forma
en la que normalmente ocurren las cosas.
<P>
<SECT1>Simulación de un fallo de disco
<P>
Si quiere simular un fallo de disco entonces desconecte la unidad. Debe
hacer esto con el <BF>sistema apagado</BF>. Si está interesado en comprobar
si sus datos pueden sobrevivir con un disco menos de los habituales, no hay
motivo para ser un vaquero de las conexiones en caliente aquí. Apague el
sistema, desconecte el disco y enciéndalo de nuevo.
<P>
Mire en el registro del sistema (``syslog'') y en &mdstat; para ver qué es
lo que está haciendo el RAID. ¿Ha funcionado?.
<P>
Recuerde que <BF>debe</BF> utilizar un RAID-{1,4,5} para que su array sea
capaz de sobrevivir a un fallo de disco. Un modo lineal o un RAID-0 fallarán
totalmente cuando se pierda un dispositivo.
<P>
Cuando haya reconectado el disco de nuevo (recuerde, con el sistema apagado,
naturalmente) podrá añadir el ``nuevo'' dispositivo al RAID otra vez, con la
orden <TT>raidhotadd</TT>.
<P>
<SECT1>Simulación de una corrupción de datos
<P>
Un RAID (ya sea hardware o software) asume que si una escritura en un disco
no devuelve un error, entonces la escritura ha tenido éxito. Por tanto. so
si disco corrompe datos sin devolver un error, sus datos se
<EM>corromperán</EM>. Naturalmente, esto es muy improbable que ocurra, pero
es posible, y produciría un sistema de ficheros corrupto.
<P>
Un RAID no puede y no está pensado para proteger contra la corrupción de
datos en un medio. Por tanto, tampoco tiene ningún sentido corromper
a propósito los datos de un disco (usando <TT>dd</TT>, por ejemplo) para ver
cómo manejará el sistema RAID esa situación. Es más probable (a menos que
corrompa el superbloque del RAID) que la capa RAID no descubra nunca la
corrupción, sino que su sistema de ficheros en el dispositivo RAID se
corrompa.
<P>
Así es como se supone que funcionan las cosas. Un RAID no es una garantía
para la integridad de datos, simplemente le permite conservar sus datos si
un disco muere (naturalmente, con RAIDs de niveles iguales o superiores a
1).
<P>

<SECT>Rendimiento
<P>
Esta sección contiene varias pruebas de evaluación de prestaciones
(``benchmarks'') de un sistema del mundo real usando un RAID software.
<P>
Las evaluaciones se han realizado con el programa <TT>bonnie</TT> y todas
las veces con ficheros dos o más veces más grandes que el tamaño de la
RAM física de la máquina.
<P>
Estas evaluaciones <EM>sólo</EM> miden el ancho de banda de entrada y de
salida sobre un único gran fichero. Esto es algo interesante de saber si uno
está interesado en el máximo rendimiento de E/S para grandes
lecturas/escrituras. Sin embargo, tales números nos dicen poco sobre cuál
sería el rendimiento si el array se usara para un almacén temporal de
noticias, un servidor web, etc. etc. Tenga siempre en cuenta que los números
de las evaluaciones son el resultado de ejecutar un programa ``sintético''.
Pocos programas del mundo real hacen lo que <TT>bonnie</TT> hace y, aunque
es interesante mirar estos números de E/S, no son indicadores en última
instancia del rendimiento de los dispositivos del mundo real.
<P>
Por ahora, sólo poseo resultados de mi propia máquina. La configuración es:
<ITEMIZE>
<ITEM>Dual Pentium Pro 150 MHz</ITEM>
<ITEM>256 MB RAM (60 MHz EDO)</ITEM>
<ITEM>Tres IBM UltraStar 9ES 4.5 GB U2W SCSI</ITEM>
<ITEM>Adaptec 2940U2W</ITEM>
<ITEM>Un IBM UltraStar 9ES 4.5 GB UW SCSI</ITEM>
<ITEM>Adaptec 2940 UW</ITEM>
<ITEM>Núcleo 2.2.7 con los parches RAID</ITEM>
</ITEMIZE>
<P>
Los tres discos U2W cuelgan de la controladora U2W y el disco UW cuelga de
la controladora UW.
<P>
Parece imposible sacar mucho más de 30 MB/s a través de los buses SCSI de
este sistema, usando un RAID o no. Mi suposición es que, debido a que el
sistema es bastante antiguo, el ancho de banda de la memoria lo fastidia y,
por tanto, limita lo que se puede enviar a través de las controladoras SCSI.
<P>
<SECT1>RAID-0
<P>
<BF>Lectura</BF> significa <BF>entrada de bloques secuencial</BF> y
<BF>Escritura</BF> significa <BF>salida de bloques secuencial</BF>. El
tamaño de fichero fue de 1GB en todas las pruebas. Las pruebas se realizaron
en modo monousuario. Se configuró el manejador SCSI para que no utilizara el
encolamiento de órdenes etiquetadas (``tagged command queuing'', TCQ).
<P>
<TABLE>
<TABULAR CA="|l|l|l|l|">
 Tamaño de porción | Tamaño de bloque | Lectura KB/s | Escritura  KB/s @@
 4k  | 1k | 19712 | 18035 @
 4k  | 4k | 34048 | 27061 @
 8k  | 1k | 19301 | 18091 @
 8k  | 4k | 33920 | 27118 @
 16k | 1k | 19330 | 18179 @
 16k | 2k | 28161 | 23682 @
 16k | 4k | 33990 | 27229 @
 32k | 1k | 19251 | 18194 @
 32k | 4k | 34071 | 26976
</TABULAR>
</TABLE>
<P>
A partir de esto vemos que el tamaño de porción del RAID no importa mucho.
Sin embargo, el tamaño de bloque del sistema de ficheros ext2 debería ser
tan grande como fuera posible, lo cual significa 4KB (es decir, el tamaño de
página) en una IA-32 (N.T.: arquitectura Intel de 32 bits).
<P>
<SECT1>RAID-0 con TCQ
<P>
Esta vez, el manejador SCSI se configuró para usar TCQ, con una longitud de
cola de 8. Por lo demás, todo es lo mismo de antes.
<P>
<TABLE>
<TABULAR CA="|l|l|l|l|">
 Tamaño de porción | Tamaño de bloque | Lectura KB/s | Escritura  KB/s @@
 32k | 4k | 33617 | 27215 
</TABULAR>
</TABLE>
<P>
No se realizaron más pruebas. TCQ pareció incrementar ligeramente el
rendimiento de las escrituras, pero verdaderamente no hubo mucha diferencia
en absoluto.
<P>
<SECT1>RAID-5
<P>
El array se configuró para funcionar en el modo RAID-5 y se hicieron pruebas
similares.
<P>
<TABLE>
<TABULAR CA="|l|l|l|l|">
 Tamaño de porción | Tamaño de bloque | Lectura KB/s | Escritura  KB/s @@
 8k | 1k | 11090 | 6874 @
 8k | 4k | 13474 | 12229 @
 32k | 1k | 11442 | 8291 @
 32k | 2k | 16089 | 10926 @
 32k | 4k | 18724 | 12627
</TABULAR>
</TABLE>
<P>
Ahora, tanto el tamaño de porción como el tamaño de bloque parecen realmente
significativos.
<P>
<SECT1>RAID-10
<P>
Un RAID-10 significa ``bandas duplicadas'' o un array RAID-1 de dos arrays
RAID-0. El tamaño de porción es tanto el tamaño de las porciones del array
RAID-1 como del array RAID-0. No realicé pruebas en las que esos tamaños de
porción fueran diferentes, aunque esa debería ser una configuración
perfectamente válida.
<P>
<TABLE>
<TABULAR CA="|l|l|l|l|">
 Tamaño de porción | Tamaño de bloque | Lectura KB/s | Escritura  KB/s @@
 32k | 1k | 13753 | 11580 @ 
 32k | 4k | 23432 | 22249
</TABULAR>
</TABLE>
<P>
No se realizaron más pruebas. El tamaño de fichero fue de 900MB debido a que
las cuatro particiones involucradas eran de 500 MB cada una, lo cual no deja
espacio para un fichero de 1GB en esta configuración (RAID-1 sobre dos
arrays de 1000MB).
<P>

<SECT>Agradecimientos
<P>
Las siguientes personas han contribuido a la creación de este documento:
<ITEMIZE>
<ITEM>Ingo Molnar
<ITEM>Jim Warren
<ITEM>Louis Mandelstam
<ITEM>Allan Noah
<ITEM>Yasunori Taniike
<ITEM>La gente de la lista de correo Linux-RAID
<ITEM>El que se me olvida, lo siento :)
</ITEMIZE>
<P>
Por favor, envíe correcciones, sugerencias, etc. al autor. Es la única forma
en que este CÓMO puede mejorar.
<P>
Envíe correcciones, sugerencias, etc. sobre esta traducción al español a
Juan Piernas Cánovas (<htmlurl url="mailto:piernas@ditec.um.es"
name="piernas@ditec.um.es">).

</ARTICLE>
